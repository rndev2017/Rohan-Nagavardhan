"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf, __hasOwnProp = Object.prototype.hasOwnProperty;
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from == "object" || typeof from == "function")
    for (let key of __getOwnPropNames(from))
      !__hasOwnProp.call(to, key) && key !== except && __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: !0 }) : target,
  mod
));
Object.defineProperty(exports, "__esModule", { value: !0 });
var paths = require("@sanity/util/paths"), arrify = require("arrify"), mutiny = require("@bjoerge/mutiny"), promises = require("node:fs/promises"), createDebug = require("debug"), FIFO = require("fast-fifo"), createSafeJsonParser = require("@sanity/util/createSafeJsonParser"), groqJs = require("groq-js"), client$1 = require("@sanity/client"), client = require("@sanity/util/client"), node_os = require("node:os"), path = require("node:path");
function _interopDefaultCompat(e) {
  return e && typeof e == "object" && "default" in e ? e : { default: e };
}
var arrify__default = /* @__PURE__ */ _interopDefaultCompat(arrify), createDebug__default = /* @__PURE__ */ _interopDefaultCompat(createDebug), FIFO__default = /* @__PURE__ */ _interopDefaultCompat(FIFO), path__default = /* @__PURE__ */ _interopDefaultCompat(path);
function defineMigration(migration) {
  return migration;
}
async function* decodeText(it) {
  const decoder = new TextDecoder();
  for await (const chunk of it)
    yield decoder.decode(chunk);
}
function sleep(ms) {
  return new Promise((resolve) => setTimeout(resolve, ms));
}
async function* delay(it, ms) {
  for await (const chunk of it)
    await sleep(ms), yield chunk;
}
async function* filter(it, predicate) {
  for await (const chunk of it)
    await predicate(chunk) && (yield chunk);
}
async function* parseJSON(it, { parse: parse2 = JSON.parse } = {}) {
  for await (const chunk of it)
    yield parse2(chunk);
}
async function* stringifyJSON(it) {
  for await (const chunk of it)
    yield JSON.stringify(chunk);
}
async function* map(it, project) {
  for await (const chunk of it)
    yield project(chunk);
}
async function* split(it, delimiter) {
  let buf = "";
  for await (const chunk of it)
    if (buf += chunk, buf.includes(delimiter)) {
      const lastIndex = buf.lastIndexOf(delimiter), parts = buf.slice(0, Math.max(0, lastIndex)).split(delimiter);
      for (const part of parts)
        yield part;
      buf = buf.slice(Math.max(0, lastIndex + delimiter.length));
    }
  yield buf;
}
function parse(it, options) {
  return parseJSON(
    filter(split(it, `
`), (line) => !!(line && line.trim())),
    options
  );
}
async function* stringify(iterable) {
  for await (const doc of iterable)
    yield `${JSON.stringify(doc)}
`;
}
async function* take(it, count) {
  let i = 0;
  for await (const chunk of it) {
    if (i++ >= count) return;
    yield chunk;
  }
}
async function toArray(it) {
  const result = [];
  for await (const chunk of it)
    result.push(chunk);
  return result;
}
function create(document) {
  return { type: "create", document };
}
function patch(id, patches, options) {
  return {
    type: "patch",
    id,
    patches: arrify__default.default(patches),
    ...options ? { options } : {}
  };
}
function at(path2, operation) {
  return {
    path: typeof path2 == "string" ? paths.fromString(path2) : path2,
    op: operation
  };
}
function createIfNotExists(document) {
  return { type: "createIfNotExists", document };
}
function createOrReplace(document) {
  return { type: "createOrReplace", document };
}
function delete_(id) {
  return { type: "delete", id };
}
const del = delete_, set = (value) => ({ type: "set", value }), setIfMissing = (value) => ({
  type: "setIfMissing",
  value
}), unset = () => ({ type: "unset" }), inc = (amount = 1) => ({
  type: "inc",
  amount
}), dec = (amount = 1) => ({
  type: "dec",
  amount
}), diffMatchPatch = (value) => ({
  type: "diffMatchPatch",
  value
});
function insert(items, position, indexOrReferenceItem) {
  return {
    type: "insert",
    referenceItem: indexOrReferenceItem,
    position,
    items: arrify__default.default(items)
  };
}
function append(items) {
  return insert(items, "after", -1);
}
function prepend(items) {
  return insert(items, "before", 0);
}
function insertBefore(items, indexOrReferenceItem) {
  return insert(items, "before", indexOrReferenceItem);
}
const insertAfter = (items, indexOrReferenceItem) => insert(items, "after", indexOrReferenceItem);
function truncate(startIndex, endIndex) {
  return {
    type: "truncate",
    startIndex,
    endIndex
  };
}
function replace(items, referenceItem) {
  return {
    type: "replace",
    referenceItem,
    items: arrify__default.default(items)
  };
}
function transaction(idOrMutations, _mutations) {
  const [id, mutations] = typeof idOrMutations == "string" ? [idOrMutations, _mutations] : [void 0, idOrMutations];
  return { type: "transaction", id, mutations };
}
function isMutation(mutation) {
  return mutation !== null && typeof mutation == "object" && "type" in mutation && (mutation.type === "create" || mutation.type === "createIfNotExists" || mutation.type === "createOrReplace" || mutation.type === "patch" || mutation.type === "delete");
}
function isTransaction(mutation) {
  return mutation !== null && typeof mutation == "object" && "type" in mutation && mutation.type === "transaction";
}
function isOperation(value) {
  return value !== null && typeof value == "object" && "type" in value && (value.type === "set" || value.type === "unset" || value.type === "insert" || value.type === "diffMatchPatch" || value.type === "dec" || value.type === "inc" || value.type === "upsert" || value.type === "unassign" || value.type === "truncate" || value.type === "setIfMissing");
}
function isNodePatch(change) {
  return change !== null && typeof change == "object" && "path" in change && Array.isArray(change.path) && "op" in change && isOperation(change.op);
}
function getValueType(value) {
  return Array.isArray(value) ? "array" : value === null ? "null" : typeof value;
}
function callMap(mapFn, value, path2) {
  const res = mapFn(value, path2);
  return Array.isArray(res) ? res : [res];
}
function getPathWithKey(item, index, container) {
  return item && Array.isArray(container) && typeof item == "object" && "_key" in item && typeof item._key == "string" ? { _key: item._key } : index;
}
function mapObject(reducerFn, object, path2) {
  return [
    ...callMap(reducerFn, object, path2),
    ...Object.keys(object).flatMap(
      (key) => flatMapAny(reducerFn, object[key], path2.concat(getPathWithKey(object[key], key, object)))
    )
  ];
}
function mapArray(mapFn, array, path2) {
  return [
    ...callMap(mapFn, array, path2),
    ...array.flatMap(
      (item, index) => flatMapAny(mapFn, item, path2.concat(getPathWithKey(item, index, array)))
    )
  ];
}
function flatMapAny(mapFn, val, path2) {
  const type = getValueType(val);
  return type === "object" ? mapObject(mapFn, val, path2) : type === "array" ? mapArray(mapFn, val, path2) : callMap(mapFn, val, path2);
}
function flatMapDeep(value, mapFn) {
  return flatMapAny(mapFn, value, []);
}
function normalizeMigrateDefinition(migration) {
  return typeof migration.migrate == "function" ? normalizeIteratorValues(migration.migrate) : createAsyncIterableMutation(migration.migrate, {
    filter: migration.filter,
    documentTypes: migration.documentTypes
  });
}
function normalizeIteratorValues(asyncIterable) {
  return async function* (docs, context) {
    for await (const documentMutations of asyncIterable(docs, context))
      yield normalizeMutation(documentMutations);
  };
}
function normalizeMutation(change) {
  return Array.isArray(change) ? change.flatMap((ch) => normalizeMutation(ch)) : isRawMutation(change) ? mutiny.SanityEncoder.decode([change]) : [change];
}
function isRawMutation(mutation) {
  return "createIfNotExists" in mutation || "createOrReplace" in mutation || "create" in mutation || "patch" in mutation || "delete" in mutation;
}
function createAsyncIterableMutation(migration, opts) {
  const documentTypesSet = new Set(opts.documentTypes);
  return async function* (docs, context) {
    for await (const doc of docs()) {
      if (opts.documentTypes && !documentTypesSet.has(doc._type)) continue;
      const documentMutations = await collectDocumentMutations(migration, doc, context);
      documentMutations.length > 0 && (yield documentMutations);
    }
  };
}
async function collectDocumentMutations(migration, doc, context) {
  var _a;
  const documentMutations = Promise.resolve((_a = migration.document) == null ? void 0 : _a.call(migration, doc, context)), nodeMigrations = flatMapDeep(doc, async (value, path2) => {
    var _a2;
    const [nodeReturnValues, nodeTypeReturnValues] = await Promise.all([
      Promise.resolve((_a2 = migration.node) == null ? void 0 : _a2.call(migration, value, path2, context)),
      Promise.resolve(migrateNodeType(migration, value, path2, context))
    ]);
    return [...arrify__default.default(nodeReturnValues), ...arrify__default.default(nodeTypeReturnValues)].map(
      (change) => change && normalizeNodeMutation(path2, change)
    );
  });
  return (await Promise.all([...arrify__default.default(await documentMutations), ...nodeMigrations])).flat().flatMap((change) => change ? normalizeDocumentMutation(doc._id, change) : []);
}
function normalizeDocumentMutation(documentId, change) {
  return Array.isArray(change) ? change.flatMap((ch) => normalizeDocumentMutation(documentId, ch)) : isRawMutation(change) ? mutiny.SanityEncoder.decode([change])[0] : isTransaction(change) || isMutation(change) ? change : patch(documentId, change);
}
function normalizeNodeMutation(path2, change) {
  return Array.isArray(change) ? change.flatMap((ch) => normalizeNodeMutation(path2, ch)) : isRawMutation(change) ? mutiny.SanityEncoder.decode([change])[0] : isNodePatch(change) ? at(path2.concat(change.path), change.op) : isOperation(change) ? at(path2, change) : change;
}
function migrateNodeType(migration, value, path2, context) {
  var _a, _b, _c, _d, _e, _f;
  switch (getValueType(value)) {
    case "string":
      return (_a = migration.string) == null ? void 0 : _a.call(migration, value, path2, context);
    case "number":
      return (_b = migration.number) == null ? void 0 : _b.call(migration, value, path2, context);
    case "boolean":
      return (_c = migration.boolean) == null ? void 0 : _c.call(migration, value, path2, context);
    case "object":
      return (_d = migration.object) == null ? void 0 : _d.call(migration, value, path2, context);
    case "array":
      return (_e = migration.array) == null ? void 0 : _e.call(migration, value, path2, context);
    case "null":
      return (_f = migration.null) == null ? void 0 : _f.call(migration, value, path2, context);
    default:
      throw new Error("Unknown value type");
  }
}
function wrapDocumentsIteratorProducer(factory) {
  function documents() {
    return factory();
  }
  return documents[Symbol.asyncIterator] = () => {
    throw new Error(
      `The migration is attempting to iterate over the "documents" function, please call the function instead:

      // BAD:
      for await (const document of documents) {
        // ...
      }

      // GOOD:                        \u{1F447} This is a function and has to be called
      for await (const document of documents()) {
        // ...
      }
      `
    );
  }, documents;
}
function collectMigrationMutations(migration, documents, context) {
  return normalizeMigrateDefinition(migration)(wrapDocumentsIteratorProducer(documents), context);
}
const MUTATION_ENDPOINT_MAX_BODY_SIZE = 262144, DEFAULT_MUTATION_CONCURRENCY = 6, MAX_MUTATION_CONCURRENCY = 10;
var baseDebug = createDebug__default.default("sanity:migrate");
const debug$1 = baseDebug.extend("bufferThroughFile"), CHUNK_SIZE$1 = 1024;
function bufferThroughFile(source, filename, options) {
  const signal = options == null ? void 0 : options.signal;
  let writeHandle, readHandle, bufferDone = !1;
  signal == null || signal.addEventListener("abort", async () => {
    debug$1("Aborting bufferThroughFile"), await Promise.all([
      writeHandle && writeHandle.close(),
      readHandle && (await readHandle).close()
    ]);
  });
  let readerCount = 0, ready;
  async function pump(reader) {
    try {
      for (; ; ) {
        const { done, value } = await reader.read();
        if (done || signal != null && signal.aborted)
          return;
        await writeHandle.write(value);
      }
    } finally {
      await writeHandle.close(), bufferDone = !0, reader.releaseLock();
    }
  }
  function createBufferedReader() {
    let totalBytesRead = 0;
    return async function tryReadFromBuffer(handle) {
      const { bytesRead, buffer } = await handle.read(
        new Uint8Array(CHUNK_SIZE$1),
        0,
        CHUNK_SIZE$1,
        totalBytesRead
      );
      return bytesRead === 0 && !bufferDone && !(signal != null && signal.aborted) ? (debug$1("Not enough data in buffer file, waiting for more data to be written"), tryReadFromBuffer(handle)) : (totalBytesRead += bytesRead, { bytesRead, buffer });
    };
  }
  function init() {
    return ready || (ready = (async () => {
      debug$1("Initializing bufferThroughFile"), writeHandle = await promises.open(filename, "w"), debug$1("Start buffering source stream to file"), pump(source.getReader()).then(() => {
        debug$1("Buffering source stream to buffer file");
      });
    })()), ready;
  }
  function getReadHandle() {
    return readHandle || (debug$1("Opening read handle on %s", filename), readHandle = promises.open(filename, "r")), readHandle;
  }
  function onReaderStart() {
    readerCount++;
  }
  async function onReaderEnd() {
    if (readerCount--, readerCount === 0 && readHandle) {
      const handle = readHandle;
      readHandle = null, debug$1("Closing read handle on %s", filename), await (await handle).close(), (options == null ? void 0 : options.keepFile) !== !0 && (debug$1("Removing buffer file", filename), await promises.unlink(filename));
    }
  }
  return () => {
    const readChunk = createBufferedReader();
    let didEnd = !1;
    function onEnd() {
      didEnd || (didEnd = !0, onReaderEnd());
    }
    return new ReadableStream({
      async start() {
        if (signal != null && signal.aborted)
          throw new Error("Cannot create new buffered readers on aborted stream");
        debug$1("Reader started reading from file handle"), onReaderStart(), await init(), await getReadHandle();
      },
      async pull(controller) {
        if (!readHandle)
          throw new Error("Cannot read from closed handle");
        const { bytesRead, buffer } = await readChunk(await readHandle);
        bytesRead === 0 && bufferDone ? (debug$1("Reader done reading from file handle"), await onEnd(), controller.close()) : controller.enqueue(buffer.subarray(0, bytesRead));
      },
      cancel() {
        onEnd();
      }
    });
  };
}
const objectToString = Object.prototype.toString, uint8ArrayStringified = "[object Uint8Array]";
function isUint8Array(value) {
  return value ? value.constructor === Uint8Array ? !0 : objectToString.call(value) === uint8ArrayStringified : !1;
}
function assertUint8Array(value) {
  if (!isUint8Array(value))
    throw new TypeError(`Expected \`Uint8Array\`, got \`${typeof value}\``);
}
function concatUint8Arrays(arrays, totalLength) {
  if (arrays.length === 0)
    return new Uint8Array(0);
  totalLength != null || (totalLength = arrays.reduce((accumulator, currentValue) => accumulator + currentValue.length, 0));
  const returnValue = new Uint8Array(totalLength);
  let offset = 0;
  for (const array of arrays)
    assertUint8Array(array), returnValue.set(array, offset), offset += array.length;
  return returnValue;
}
function areUint8ArraysEqual(a, b) {
  if (assertUint8Array(a), assertUint8Array(b), a === b)
    return !0;
  if (a.length !== b.length)
    return !1;
  for (let index = 0; index < a.length; index++)
    if (a[index] !== b[index])
      return !1;
  return !0;
}
function peekInto(readable, options) {
  const { size } = options;
  return new Promise((resolve, reject) => {
    let totalBytesRead = 0, streamCompleted = !1;
    const chunks = [], reader = readable.getReader();
    function settled() {
      const head = concatUint8Arrays(chunks);
      resolve([
        head,
        new ReadableStream({
          start(controller) {
            controller.enqueue(head), streamCompleted && controller.close();
          },
          async pull(controller) {
            const { done, value } = await reader.read();
            done ? controller.close() : controller.enqueue(value);
          }
        })
      ]);
    }
    (async () => {
      for (; ; ) {
        const { done, value: chunk } = await reader.read();
        if (done) {
          streamCompleted = !0;
          break;
        } else if (totalBytesRead += chunk.byteLength, chunks.push(chunk), totalBytesRead >= size)
          break;
      }
    })().then(settled, reject);
  });
}
function isGzip(buffer) {
  return buffer.length > 3 && buffer[0] === 31 && buffer[1] === 139 && buffer[2] === 8;
}
function isDeflate(buf) {
  return buf.length > 2 && buf[0] === 120 && (buf[1] === 1 || buf[1] === 156 || buf[1] === 218);
}
async function maybeDecompress(readable) {
  const [head, stream] = await peekInto(readable, { size: 10 });
  return isGzip(head) ? stream.pipeThrough(new DecompressionStream("gzip")) : isDeflate(head) ? stream.pipeThrough(new DecompressionStream("deflate-raw")) : stream;
}
const debug = baseDebug.extend("readFileAsWebStream"), CHUNK_SIZE = 1024 * 16;
function readFileAsWebStream(filename) {
  let fileHandle, position = 0;
  return new ReadableStream({
    async start() {
      debug("Starting readable stream from", filename), fileHandle = await promises.open(filename, "r");
    },
    async pull(controller) {
      const { bytesRead, buffer } = await fileHandle.read(
        new Uint8Array(CHUNK_SIZE),
        0,
        CHUNK_SIZE,
        position
      );
      bytesRead === 0 ? (await fileHandle.close(), debug("Closing readable stream from", filename), controller.close()) : (position += bytesRead, controller.enqueue(buffer.subarray(0, bytesRead)));
    },
    cancel() {
      return debug("Cancelling readable stream from", filename), fileHandle.close();
    }
  });
}
async function drain(stream) {
  const reader = stream.getReader();
  for (; ; ) {
    const { done } = await reader.read();
    if (done)
      return;
  }
}
var __defProp$1 = Object.defineProperty, __defNormalProp$1 = (obj, key, value) => key in obj ? __defProp$1(obj, key, { enumerable: !0, configurable: !0, writable: !0, value }) : obj[key] = value, __publicField$1 = (obj, key, value) => __defNormalProp$1(obj, typeof key != "symbol" ? key + "" : key, value);
const EMPTY = new Uint8Array();
class BufferList {
  constructor() {
    __publicField$1(this, "buffered"), __publicField$1(this, "shifted"), __publicField$1(this, "queue"), __publicField$1(this, "_offset"), this.buffered = 0, this.shifted = 0, this.queue = new FIFO__default.default(), this._offset = 0;
  }
  push(buffer) {
    this.buffered += buffer.byteLength, this.queue.push(buffer);
  }
  shiftFirst(size) {
    return this.buffered === 0 ? null : this._next(size);
  }
  shift(size) {
    if (size > this.buffered) return null;
    if (size === 0) return EMPTY;
    let chunk = this._next(size);
    if (size === chunk.byteLength) return chunk;
    const chunks = [chunk];
    for (; (size -= chunk.byteLength) > 0; )
      chunk = this._next(size), chunks.push(chunk);
    return concatUint8Arrays(chunks);
  }
  _next(size) {
    const buf = this.queue.peek(), rem = buf.byteLength - this._offset;
    if (size >= rem) {
      const sub = this._offset ? buf.subarray(this._offset, buf.byteLength) : buf;
      return this.queue.shift(), this._offset = 0, this.buffered -= rem, this.shifted += rem, sub;
    }
    return this.buffered -= size, this.shifted += size, buf.subarray(this._offset, this._offset += size);
  }
}
const ZERO_OFFSET = 48, USTAR_MAGIC = new Uint8Array([117, 115, 116, 97, 114, 0]), GNU_MAGIC = new Uint8Array([117, 115, 116, 97, 114, 32]), GNU_VER = new Uint8Array([32, 0]), MAGIC_OFFSET = 257, VERSION_OFFSET = 263;
function decode(buf, filenameEncoding, allowUnknownFormat) {
  let typeflag = buf[156] === 0 ? 0 : buf[156] - ZERO_OFFSET, name = decodeStr(buf, 0, 100, filenameEncoding);
  const mode = decodeOct(buf, 100, 8), uid = decodeOct(buf, 108, 8), gid = decodeOct(buf, 116, 8), size = decodeOct(buf, 124, 12), mtime = decodeOct(buf, 136, 12), type = toType(typeflag), linkname = buf[157] === 0 ? null : decodeStr(buf, 157, 100, filenameEncoding), uname = decodeStr(buf, 265, 32), gname = decodeStr(buf, 297, 32), devmajor = decodeOct(buf, 329, 8), devminor = decodeOct(buf, 337, 8), c = cksum(buf);
  if (c === 8 * 32) return null;
  if (c !== decodeOct(buf, 148, 8))
    throw new Error("Invalid tar header. Maybe the tar is corrupted or it needs to be gunzipped?");
  if (isUSTAR(buf))
    buf[345] && (name = `${decodeStr(buf, 345, 155, filenameEncoding)}/${name}`);
  else if (!isGNU(buf) && !allowUnknownFormat)
    throw new Error("Invalid tar header: unknown format.");
  return typeflag === 0 && name && name[name.length - 1] === "/" && (typeflag = 5), {
    type,
    name,
    mode,
    uid,
    gid,
    size,
    mtime: mtime ? new Date(1e3 * mtime) : null,
    linkname,
    uname,
    gname,
    devmajor,
    devminor
  };
}
function isUSTAR(buf) {
  return areUint8ArraysEqual(USTAR_MAGIC, buf.subarray(MAGIC_OFFSET, MAGIC_OFFSET + 6));
}
function isGNU(buf) {
  return areUint8ArraysEqual(GNU_MAGIC, buf.subarray(MAGIC_OFFSET, MAGIC_OFFSET + 6)) && areUint8ArraysEqual(GNU_VER, buf.subarray(VERSION_OFFSET, VERSION_OFFSET + 2));
}
function clamp(index, len, defaultValue) {
  return typeof index != "number" ? defaultValue : (index = ~~index, index >= len ? len : index >= 0 || (index += len, index >= 0) ? index : 0);
}
function toType(flag) {
  switch (flag) {
    case 0:
      return "file";
    case 1:
      return "link";
    case 2:
      return "symlink";
    case 3:
      return "character-device";
    case 4:
      return "block-device";
    case 5:
      return "directory";
    case 6:
      return "fifo";
    case 7:
      return "contiguous-file";
    case 72:
      return "pax-header";
    case 55:
      return "pax-global-header";
    case 27:
      return "gnu-long-link-path";
    case 28:
    case 30:
      return "gnu-long-path";
    default:
      return null;
  }
}
function indexOf(block, num, offset, end) {
  for (; offset < end; offset++)
    if (block[offset] === num) return offset;
  return end;
}
function cksum(block) {
  let sum = 256;
  for (let i = 0; i < 148; i++) sum += block[i];
  for (let j = 156; j < 512; j++) sum += block[j];
  return sum;
}
function parse256(buf) {
  let positive;
  if (buf[0] === 128) positive = !0;
  else if (buf[0] === 255) positive = !1;
  else return null;
  const tuple = [];
  let i;
  for (i = buf.length - 1; i > 0; i--) {
    const byte = buf[i];
    positive ? tuple.push(byte) : tuple.push(255 - byte);
  }
  let sum = 0;
  const l = tuple.length;
  for (i = 0; i < l; i++)
    sum += tuple[i] * Math.pow(256, i);
  return positive ? sum : -1 * sum;
}
const decoders = {}, getCachedDecoder = (encoding) => (encoding in decoders || (decoders[encoding] = new TextDecoder(encoding)), decoders[encoding]);
function toString(uint8, encoding = "utf-8") {
  return getCachedDecoder(encoding).decode(uint8);
}
function decodeOct(val, offset, length) {
  if (val = val.subarray(offset, offset + length), offset = 0, val[offset] & 128)
    return parse256(val);
  for (; offset < val.length && val[offset] === 32; ) offset++;
  const end = clamp(indexOf(val, 32, offset, val.length), val.length, val.length);
  for (; offset < end && val[offset] === 0; ) offset++;
  return end === offset ? 0 : parseInt(toString(val.subarray(offset, end)), 8);
}
function decodeStr(val, offset, length, encoding) {
  return toString(val.subarray(offset, indexOf(val, 0, offset, offset + length)), encoding);
}
const emptyReadableStream = () => new ReadableStream({
  pull(controller) {
    controller.close();
  }
});
function untar(stream, options = {}) {
  const buffer = new BufferList(), reader = stream.getReader();
  let readingChunk = !1;
  return new ReadableStream({
    async pull(controller) {
      var _a, _b;
      if (readingChunk)
        return;
      const { done, value } = await reader.read();
      done || buffer.push(value);
      const headerChunk = buffer.shift(512);
      if (!headerChunk)
        throw new Error("Unexpected end of tar file. Expected 512 bytes of headers.");
      const header = decode(
        headerChunk,
        (_a = options.filenameEncoding) != null ? _a : "utf-8",
        (_b = options.allowUnknownFormat) != null ? _b : !1
      );
      header ? header.size === null || header.size === 0 || header.type === "directory" ? controller.enqueue([header, emptyReadableStream()]) : (readingChunk = !0, controller.enqueue([
        header,
        entryStream(reader, header.size, buffer, () => {
          readingChunk = !1;
        })
      ])) : done && controller.close();
    }
  });
}
function entryStream(reader, expectedBytes, buffer, next) {
  let totalBytesRead = 0;
  return new ReadableStream({
    async pull(controller) {
      const { done, value } = await reader.read(), remaining = expectedBytes - totalBytesRead;
      done || buffer.push(value);
      const chunk = buffer.shiftFirst(remaining);
      if (!chunk)
        throw new Error("Premature end of tar stream");
      controller.enqueue(chunk), totalBytesRead += chunk.byteLength, (chunk == null ? void 0 : chunk.byteLength) === remaining && (discardPadding(buffer, expectedBytes), controller.close(), next());
    }
  });
}
function getPadding(size) {
  return size &= 511, size === 0 ? 0 : 512 - size;
}
function discardPadding(bl, size) {
  const overflow = getPadding(size);
  overflow > 0 && bl.shift(overflow);
}
async function* streamToAsyncIterator(stream) {
  const reader = stream.getReader();
  try {
    for (; ; ) {
      const { done, value } = await reader.read();
      if (done) return;
      yield value;
    }
  } finally {
    reader.releaseLock();
  }
}
async function* fromExportArchive(path2) {
  for await (const [header, entry] of streamToAsyncIterator(
    untar(await maybeDecompress(readFileAsWebStream(path2)))
  ))
    if (header.type === "file" && header.name.endsWith(".ndjson"))
      for await (const chunk of streamToAsyncIterator(entry))
        yield chunk;
    else
      await drain(entry);
}
const endpoints = {
  users: {
    me: () => ({
      global: !0,
      path: "/users/me",
      method: "GET",
      searchParams: []
    })
  },
  data: {
    query: (dataset) => ({
      global: !1,
      method: "GET",
      path: `/query/${dataset}`,
      searchParams: []
    }),
    export: (dataset, documentTypes) => ({
      global: !1,
      method: "GET",
      path: `/data/export/${dataset}`,
      searchParams: documentTypes && (documentTypes == null ? void 0 : documentTypes.length) > 0 ? [["types", documentTypes.join(",")]] : []
    }),
    mutate: (dataset, options) => {
      const params = [
        (options == null ? void 0 : options.tag) && ["tag", options.tag],
        (options == null ? void 0 : options.returnIds) && ["returnIds", "true"],
        (options == null ? void 0 : options.returnDocuments) && ["returnDocuments", "true"],
        (options == null ? void 0 : options.autoGenerateArrayKeys) && ["autoGenerateArrayKeys", "true"],
        (options == null ? void 0 : options.visibility) && ["visibility", options.visibility],
        (options == null ? void 0 : options.dryRun) && ["dryRun", "true"]
      ].filter(Boolean);
      return {
        global: !1,
        method: "POST",
        path: `/data/mutate/${dataset}`,
        searchParams: params
      };
    }
  }
};
var __defProp2 = Object.defineProperty, __defNormalProp = (obj, key, value) => key in obj ? __defProp2(obj, key, { enumerable: !0, configurable: !0, writable: !0, value }) : obj[key] = value, __publicField = (obj, key, value) => __defNormalProp(obj, key + "", value);
class HTTPError extends Error {
  constructor(statusCode, message) {
    super(message), __publicField(this, "statusCode"), this.name = "HTTPError", this.statusCode = statusCode;
  }
}
async function assert2xx(res) {
  if (res.status < 200 || res.status > 299) {
    const jsonResponse = await res.json().catch(() => null), message = jsonResponse != null && jsonResponse.error ? `${jsonResponse.error}: ${jsonResponse.message}` : `HTTP Error ${res.status}: ${res.statusText}`;
    throw new HTTPError(res.status, message);
  }
}
async function fetchStream({ url, init }) {
  const response = await fetch(url, init);
  if (await assert2xx(response), response.body === null) throw new Error("No response received");
  return response.body;
}
async function fetchAsyncIterator(options) {
  return streamToAsyncIterator(await fetchStream(options));
}
function getUserAgent() {
  if (typeof window > "u")
    try {
      const pkg = require("../../package.json");
      return `${pkg.name}@${pkg.version}`;
    } catch {
    }
  return null;
}
function normalizeApiHost(apiHost) {
  return apiHost.replace(/^https?:\/\//, "");
}
function toFetchOptions(req) {
  const { endpoint, apiVersion, tag, projectId, apiHost, token, body } = req, requestInit = {
    method: endpoint.method || "GET",
    headers: {
      "Content-Type": "application/json"
    },
    body
  }, ua = getUserAgent();
  ua && (requestInit.headers = {
    ...requestInit.headers,
    "User-Agent": ua
  }), token && (requestInit.headers = {
    ...requestInit.headers,
    Authorization: `bearer ${token}`
  });
  const normalizedApiHost = normalizeApiHost(apiHost), path2 = `/${apiVersion}${endpoint.path}`, host = endpoint.global ? normalizedApiHost : `${projectId}.${normalizedApiHost}`, searchParams = new URLSearchParams([
    ...endpoint.searchParams,
    ...tag ? [["tag", tag]] : []
  ]).toString();
  return {
    url: `https://${host}/${path2}${searchParams ? `?${searchParams}` : ""}`,
    init: requestInit
  };
}
function fromExportEndpoint(options) {
  var _a;
  return fetchStream(
    toFetchOptions({
      projectId: options.projectId,
      apiVersion: options.apiVersion,
      token: options.token,
      apiHost: (_a = options.apiHost) != null ? _a : "api.sanity.io",
      tag: "sanity.migration.export",
      endpoint: endpoints.data.export(options.dataset, options.documentTypes)
    })
  );
}
const safeJsonParser = createSafeJsonParser.createSafeJsonParser({
  errorLabel: "Error streaming dataset"
});
function asyncIterableToStream(it) {
  return new ReadableStream({
    async pull(controller) {
      const { value, done } = await it.next();
      done ? controller.close() : controller.enqueue(value);
    }
  });
}
function isSystemDocumentId(id) {
  return id.startsWith("_.");
}
function parseGroqFilter(filter2) {
  try {
    return groqJs.parse(`*[${filter2}]`);
  } catch (err) {
    throw err.message = `Failed to parse GROQ filter "${filter2}": ${err.message}`, err;
  }
}
async function matchesFilter(parsedFilter, document) {
  return (await (await groqJs.evaluate(parsedFilter, { dataset: [document] })).get()).length === 1;
}
async function* applyFilters(migration, documents) {
  const documentTypes = migration.documentTypes, parsedFilter = migration.filter ? parseGroqFilter(migration.filter) : void 0;
  for await (const doc of documents)
    isSystemDocumentId(doc._id) || documentTypes && documentTypes.length > 0 && !documentTypes.includes(doc._type) || parsedFilter && !await matchesFilter(parsedFilter, doc) || (yield doc);
}
const MAX_FETCH_CONCURRENCY = 10, limitClientConcurrency = client.createClientConcurrencyLimiter(MAX_FETCH_CONCURRENCY);
function createContextClient(config) {
  return restrictClient(
    limitClientConcurrency(
      client$1.createClient({ ...config, useCdn: !1, requestTagPrefix: "sanity.migration" })
    )
  );
}
const ALLOWED_PROPERTIES = [
  "fetch",
  "clone",
  "config",
  "withConfig",
  "getDocument",
  "getDocuments",
  "users",
  "projects"
];
function restrictClient(client2) {
  return new Proxy(client2, {
    get: (target, property) => {
      switch (property) {
        case "clone":
          return (...args) => restrictClient(target.clone(...args));
        case "config":
          return (...args) => {
            const result = target.config(...args);
            return args[0] ? restrictClient(result) : result;
          };
        case "withConfig":
          return (...args) => restrictClient(target.withConfig(...args));
        default: {
          if (ALLOWED_PROPERTIES.includes(property))
            return target[property];
          throw new Error(
            `Client method "${String(
              property
            )}" can not be called during a migration. Only ${ALLOWED_PROPERTIES.join(
              ", "
            )} are allowed.`
          );
        }
      }
    }
  });
}
function createFilteredDocumentsClient(getFilteredDocumentsReadableStream) {
  function getAllDocumentsFromBuffer() {
    return parse(decodeText(streamToAsyncIterator(getFilteredDocumentsReadableStream())), {
      parse: safeJsonParser
    });
  }
  async function getDocumentsFromBuffer(ids) {
    const found = {};
    let remaining = ids.length;
    for await (const doc of getAllDocumentsFromBuffer())
      if (ids.includes(doc._id) && (remaining--, found[doc._id] = doc), remaining === 0) break;
    return ids.map((id) => found[id]);
  }
  async function getDocumentFromBuffer(id) {
    return (await getDocumentsFromBuffer([id]))[0];
  }
  return {
    getDocument: getDocumentFromBuffer,
    getDocuments: getDocumentsFromBuffer
  };
}
async function createBufferFile() {
  const bufferDir = path__default.default.join(
    node_os.tmpdir(),
    "sanity-migrate",
    `${Date.now()}-${Math.random().toString(36).slice(2)}`
  );
  return await promises.mkdir(bufferDir, { recursive: !0 }), path__default.default.join(bufferDir, "snapshot.ndjson");
}
async function* dryRun(config, migration) {
  const source = config.exportPath ? fromExportArchive(config.exportPath) : streamToAsyncIterator(
    await fromExportEndpoint({ ...config.api, documentTypes: migration.documentTypes })
  ), filteredDocuments = applyFilters(
    migration,
    parse(decodeText(source), { parse: safeJsonParser })
  ), abortController = new AbortController(), createReader = bufferThroughFile(
    asyncIterableToStream(stringify(filteredDocuments)),
    await createBufferFile(),
    { signal: abortController.signal }
  ), client2 = createContextClient({ ...config.api, useCdn: !1 }), filteredDocumentsClient = createFilteredDocumentsClient(createReader);
  yield* collectMigrationMutations(
    migration,
    () => parse(decodeText(streamToAsyncIterator(createReader())), { parse: safeJsonParser }),
    {
      client: client2,
      filtered: filteredDocumentsClient
    }
  ), abortController.abort();
}
async function* concatStr(it) {
  let buf = "";
  for await (const chunk of it)
    buf += chunk;
  yield buf;
}
async function lastValueFrom(it, options) {
  const defaultGiven = "defaultValue" in {};
  let latestValue, didYield = !1;
  for await (const value of it)
    didYield = !0, latestValue = value;
  if (!didYield) {
    if (defaultGiven)
      return options.defaultValue;
    throw new Error(
      "No value yielded from async iterable. If this iterable is empty, provide a default value."
    );
  }
  return latestValue;
}
async function mapAsync(it, project, concurrency) {
  const { pMapIterable } = await import("p-map");
  return pMapIterable(it, (v) => project(v), {
    concurrency
  });
}
async function* tap(it, interceptor) {
  for await (const chunk of it)
    interceptor(chunk), yield chunk;
}
const PADDING_SIZE = 16;
function isTransactionPayload(payload) {
  return payload && payload.mutations && Array.isArray(payload.mutations);
}
async function* batchMutations(mutations, maxBatchSize) {
  let currentBatch = [], currentBatchSize = 0;
  for await (const mutation of mutations) {
    if (isTransactionPayload(mutation)) {
      yield { mutations: currentBatch }, yield mutation, currentBatch = [], currentBatchSize = 0;
      continue;
    }
    const mutationSize = JSON.stringify(mutation).length;
    if (mutationSize >= maxBatchSize + PADDING_SIZE) {
      currentBatch.length && (yield { mutations: currentBatch }), yield { mutations: [...arrify__default.default(mutation)] }, currentBatch = [], currentBatchSize = 0;
      continue;
    }
    currentBatchSize += mutationSize, currentBatchSize >= maxBatchSize + PADDING_SIZE && (yield { mutations: currentBatch }, currentBatch = [], currentBatchSize = 0), currentBatch.push(...arrify__default.default(mutation));
  }
  currentBatch.length > 0 && (yield { mutations: currentBatch });
}
async function* toSanityMutations(it) {
  for await (const mutation of it)
    for (const mut of arrify__default.default(mutation)) {
      if (isTransaction(mut)) {
        yield {
          transactionId: mut.id,
          mutations: mutiny.SanityEncoder.encode(mut.mutations)
        };
        continue;
      }
      yield mutiny.SanityEncoder.encode(arrify__default.default(mut));
    }
}
async function* toFetchOptionsIterable(apiConfig, mutations) {
  var _a;
  for await (const transaction2 of mutations)
    yield toFetchOptions({
      projectId: apiConfig.projectId,
      apiVersion: apiConfig.apiVersion,
      token: apiConfig.token,
      tag: "sanity.migration.mutate",
      apiHost: (_a = apiConfig.apiHost) != null ? _a : "api.sanity.io",
      endpoint: endpoints.data.mutate(apiConfig.dataset, {
        returnIds: !0,
        visibility: "async",
        autoGenerateArrayKeys: !0
      }),
      body: JSON.stringify(transaction2)
    });
}
async function run(config, migration) {
  var _a, _b, _c;
  const stats = {
    documents: 0,
    mutations: 0,
    pending: 0,
    queuedBatches: 0,
    completedTransactions: [],
    currentTransactions: []
  }, filteredDocuments = applyFilters(
    migration,
    parse(
      decodeText(
        streamToAsyncIterator(
          await fromExportEndpoint({ ...config.api, documentTypes: migration.documentTypes })
        )
      ),
      { parse: safeJsonParser }
    )
  ), abortController = new AbortController(), createReader = bufferThroughFile(
    asyncIterableToStream(stringify(filteredDocuments)),
    await createBufferFile(),
    { signal: abortController.signal }
  ), client2 = createContextClient({
    ...config.api,
    useCdn: !1,
    requestTagPrefix: "sanity.migration"
  }), filteredDocumentsClient = createFilteredDocumentsClient(createReader), mutations = tap(collectMigrationMutations(migration, () => tap(
    parse(decodeText(streamToAsyncIterator(createReader())), {
      parse: safeJsonParser
    }),
    () => {
      var _a2;
      (_a2 = config.onProgress) == null || _a2.call(config, { ...stats, documents: ++stats.documents });
    }
  ), {
    client: client2,
    filtered: filteredDocumentsClient
  }), (muts) => {
    var _a2;
    stats.currentTransactions = arrify__default.default(muts), (_a2 = config.onProgress) == null || _a2.call(config, {
      ...stats,
      mutations: ++stats.mutations
    });
  }), concurrency = (_a = config == null ? void 0 : config.concurrency) != null ? _a : DEFAULT_MUTATION_CONCURRENCY;
  if (concurrency > MAX_MUTATION_CONCURRENCY)
    throw new Error(`Concurrency exceeds maximum allowed value (${MAX_MUTATION_CONCURRENCY})`);
  const batches = tap(
    batchMutations(toSanityMutations(mutations), MUTATION_ENDPOINT_MAX_BODY_SIZE),
    () => {
      var _a2;
      (_a2 = config.onProgress) == null || _a2.call(config, { ...stats, queuedBatches: ++stats.queuedBatches });
    }
  ), submit = async (opts) => lastValueFrom(parseJSON(concatStr(decodeText(await fetchAsyncIterator(opts))))), commits = await mapAsync(
    toFetchOptionsIterable(config.api, batches),
    (opts) => {
      var _a2;
      return (_a2 = config.onProgress) == null || _a2.call(config, { ...stats, pending: ++stats.pending }), submit(opts);
    },
    concurrency
  );
  for await (const result of commits)
    stats.completedTransactions.push(result), (_b = config.onProgress) == null || _b.call(config, {
      ...stats
    });
  (_c = config.onProgress) == null || _c.call(config, {
    ...stats,
    done: !0
  }), abortController.abort();
}
function* fromDocuments(documents) {
  for (const document of documents)
    yield document;
}
exports.DEFAULT_MUTATION_CONCURRENCY = DEFAULT_MUTATION_CONCURRENCY;
exports.MAX_MUTATION_CONCURRENCY = MAX_MUTATION_CONCURRENCY;
exports.append = append;
exports.at = at;
exports.collectMigrationMutations = collectMigrationMutations;
exports.create = create;
exports.createIfNotExists = createIfNotExists;
exports.createOrReplace = createOrReplace;
exports.dec = dec;
exports.decodeText = decodeText;
exports.defineMigration = defineMigration;
exports.del = del;
exports.delay = delay;
exports.delete_ = delete_;
exports.diffMatchPatch = diffMatchPatch;
exports.dryRun = dryRun;
exports.filter = filter;
exports.fromDocuments = fromDocuments;
exports.fromExportArchive = fromExportArchive;
exports.fromExportEndpoint = fromExportEndpoint;
exports.inc = inc;
exports.insert = insert;
exports.insertAfter = insertAfter;
exports.insertBefore = insertBefore;
exports.map = map;
exports.parse = parse;
exports.parseJSON = parseJSON;
exports.patch = patch;
exports.prepend = prepend;
exports.replace = replace;
exports.run = run;
exports.safeJsonParser = safeJsonParser;
exports.set = set;
exports.setIfMissing = setIfMissing;
exports.split = split;
exports.stringify = stringify;
exports.stringifyJSON = stringifyJSON;
exports.take = take;
exports.toArray = toArray;
exports.toFetchOptionsIterable = toFetchOptionsIterable;
exports.transaction = transaction;
exports.truncate = truncate;
exports.unset = unset;
//# sourceMappingURL=index.js.map
