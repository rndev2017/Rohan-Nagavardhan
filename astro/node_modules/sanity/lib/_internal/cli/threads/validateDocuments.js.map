{"version":3,"file":"validateDocuments.js","sources":["../../../../src/_internal/cli/util/extractDocumentsFromNdjsonOrTarball.ts","../../../../src/_internal/cli/util/workerChannels.ts","../../../../src/_internal/cli/threads/validateDocuments.ts"],"sourcesContent":["import path from 'node:path'\nimport readline from 'node:readline'\nimport {Readable, type Writable} from 'node:stream'\nimport zlib from 'node:zlib'\n\nimport {type SanityDocument} from '@sanity/types'\nimport tar from 'tar-stream'\n\nconst HEADER_SIZE = 300\n\n// https://github.com/kevva/is-gzip/blob/13dab7c877787bd5cff9de5482b1736f00df99c6/index.js\nconst isGzip = (buf: Buffer) =>\n  buf.length >= 3 && buf[0] === 0x1f && buf[1] === 0x8b && buf[2] === 0x08\n\n// https://github.com/watson/is-deflate/blob/f9e8f0c7814eed715e13e29e97c69acee319686a/index.js\nconst isDeflate = (buf: Buffer) =>\n  buf.length >= 2 && buf[0] === 0x78 && (buf[1] === 1 || buf[1] === 0x9c || buf[1] === 0xda)\n\n// https://github.com/kevva/is-tar/blob/d295ffa2002a5d415946fc3d49f024ace8c28bd3/index.js\nconst isTar = (buf: Buffer) =>\n  buf.length >= 262 &&\n  buf[257] === 0x75 &&\n  buf[258] === 0x73 &&\n  buf[259] === 0x74 &&\n  buf[260] === 0x61 &&\n  buf[261] === 0x72\n\nasync function* extract<TReturn>(\n  stream: AsyncIterable<Buffer>,\n  extractor: Writable & AsyncIterable<TReturn>,\n) {\n  // set up a task to drain the input iterable into the extractor asynchronously\n  // before this function delegates to the extractor's iterable (containing the\n  // result of the extraction)\n  const drained = new Promise<void>((resolve, reject) => {\n    // setTimeout is used here to ensure draining occurs after delegation\n    setTimeout(async () => {\n      try {\n        for await (const chunk of stream) extractor.write(chunk)\n        extractor.end()\n        resolve()\n      } catch (err) {\n        reject(err)\n      }\n    })\n  })\n\n  // have this function delegate the results of the extractor\n  yield* extractor\n  await drained\n  extractor.destroy()\n}\n\n/**\n * Given a async iterable of buffers, looks at the header of the file in the\n * first few bytes to see the file type then extracts the contents tries again.\n * If the given iterable of buffers is a tarball then it looks for an ndjson\n * files and returns another iterable of buffers with the contents of the\n * ndjson file\n */\nasync function* maybeExtractNdjson(stream: AsyncIterable<Buffer>): AsyncIterable<Buffer> {\n  let buffer = Buffer.alloc(0)\n\n  for await (const chunk of stream) {\n    buffer = Buffer.concat([buffer, chunk])\n    if (buffer.length < HEADER_SIZE) continue\n\n    const fileHeader = buffer\n    const restOfStream = async function* restOfStream() {\n      yield fileHeader\n      yield* stream\n    }\n\n    if (isGzip(fileHeader)) {\n      yield* maybeExtractNdjson(extract(restOfStream(), zlib.createGunzip()))\n      return\n    }\n\n    if (isDeflate(fileHeader)) {\n      yield* maybeExtractNdjson(extract(restOfStream(), zlib.createDeflate()))\n      return\n    }\n\n    if (isTar(fileHeader)) {\n      for await (const entry of extract(restOfStream(), tar.extract())) {\n        const filename = path.basename(entry.header.name)\n        const extname = path.extname(filename).toLowerCase()\n        // ignore hidden and non-ndjson files\n        if (extname !== '.ndjson' || filename.startsWith('.')) continue\n\n        for await (const ndjsonChunk of entry) yield ndjsonChunk\n        return\n      }\n    }\n\n    yield* restOfStream()\n  }\n}\n\n/**\n * Takes in an async iterable of buffers from an ndjson file or tarball and\n * returns an async iterable of sanity documents.\n */\nexport async function* extractDocumentsFromNdjsonOrTarball(\n  file: AsyncIterable<Buffer>,\n): AsyncIterable<SanityDocument> {\n  const lines = readline.createInterface({\n    input: Readable.from(maybeExtractNdjson(file)),\n  })\n\n  for await (const line of lines) {\n    const trimmed = line.trim()\n    if (trimmed) yield JSON.parse(trimmed) as SanityDocument\n  }\n  lines.close()\n}\n","import {type MessagePort, type Worker} from 'node:worker_threads'\n\ntype StreamReporter<TPayload = unknown> = {emit: (payload: TPayload) => void; end: () => void}\ntype EventReporter<TPayload = unknown> = (payload: TPayload) => void\ntype EventReceiver<TPayload = unknown> = () => Promise<TPayload>\ntype StreamReceiver<TPayload = unknown> = () => AsyncIterable<TPayload>\n\ntype EventKeys<TWorkerChannel extends WorkerChannel> = {\n  [K in keyof TWorkerChannel]: TWorkerChannel[K] extends WorkerChannelEvent<any> ? K : never\n}[keyof TWorkerChannel]\ntype StreamKeys<TWorkerChannel extends WorkerChannel> = {\n  [K in keyof TWorkerChannel]: TWorkerChannel[K] extends WorkerChannelStream<any> ? K : never\n}[keyof TWorkerChannel]\n\ntype EventMessage<TPayload = unknown> = {type: 'event'; name: string; payload: TPayload}\ntype StreamEmissionMessage<TPayload = unknown> = {type: 'emission'; name: string; payload: TPayload}\ntype StreamEndMessage = {type: 'end'; name: string}\ntype WorkerChannelMessage = EventMessage | StreamEmissionMessage | StreamEndMessage\n\n/**\n * Represents the definition of a \"worker channel\" to report progress from the\n * worker to the parent. Worker channels can define named events or streams and\n * the worker will report events and streams while the parent will await them.\n * This allows the control flow of the parent to follow the control flow of the\n * worker 1-to-1.\n */\nexport type WorkerChannel<\n  TWorkerChannel extends Record<\n    string,\n    WorkerChannelEvent<unknown> | WorkerChannelStream<unknown>\n  > = Record<string, WorkerChannelEvent<unknown> | WorkerChannelStream<unknown>>,\n> = TWorkerChannel\n\nexport type WorkerChannelEvent<TPayload = void> = {type: 'event'; payload: TPayload}\nexport type WorkerChannelStream<TPayload = void> = {type: 'stream'; payload: TPayload}\n\nexport interface WorkerChannelReporter<TWorkerChannel extends WorkerChannel> {\n  event: {\n    [K in EventKeys<TWorkerChannel>]: TWorkerChannel[K] extends WorkerChannelEvent<infer TPayload>\n      ? EventReporter<TPayload>\n      : void\n  }\n  stream: {\n    [K in StreamKeys<TWorkerChannel>]: TWorkerChannel[K] extends WorkerChannelStream<infer TPayload>\n      ? StreamReporter<TPayload>\n      : void\n  }\n}\n\nexport interface WorkerChannelReceiver<TWorkerChannel extends WorkerChannel> {\n  event: {\n    [K in EventKeys<TWorkerChannel>]: TWorkerChannel[K] extends WorkerChannelEvent<infer TPayload>\n      ? EventReceiver<TPayload>\n      : void\n  }\n  stream: {\n    [K in StreamKeys<TWorkerChannel>]: TWorkerChannel[K] extends WorkerChannelStream<infer TPayload>\n      ? StreamReceiver<TPayload>\n      : void\n  }\n  // TODO: good candidate for [Symbol.asyncDispose] when our tooling better supports it\n  dispose: () => Promise<number>\n}\n\n/**\n * A simple queue that has two primary methods: `push(message)` and\n * `await next()`. This message queue is used by the \"receiver\" of the worker\n * channel and this class handles buffering incoming messages if the worker is\n * producing faster than the parent as well as returning a promise if there is\n * no message yet in the queue when the parent awaits `next()`.\n */\nclass MessageQueue<T> {\n  resolver: ((result: IteratorResult<T>) => void) | null = null\n  queue: T[] = []\n\n  push(message: T) {\n    if (this.resolver) {\n      this.resolver({value: message, done: false})\n      this.resolver = null\n    } else {\n      this.queue.push(message)\n    }\n  }\n\n  next(): Promise<IteratorResult<T>> {\n    if (this.queue.length) {\n      return Promise.resolve({value: this.queue.shift()!, done: false})\n    }\n\n    return new Promise((resolve) => (this.resolver = resolve))\n  }\n\n  end() {\n    if (this.resolver) {\n      this.resolver({value: undefined, done: true})\n    }\n  }\n}\n\nfunction isWorkerChannelMessage(message: unknown): message is WorkerChannelMessage {\n  if (typeof message !== 'object') return false\n  if (!message) return false\n  if (!('type' in message)) return false\n  if (typeof message.type !== 'string') return false\n  const types: string[] = ['event', 'emission', 'end'] satisfies WorkerChannelMessage['type'][]\n  return types.includes(message.type)\n}\n\n/**\n * Creates a \"worker channel receiver\" that subscribes to incoming messages\n * from the given worker and returns promises for worker channel events and\n * async iterators for worker channel streams.\n */\nexport function createReceiver<TWorkerChannel extends WorkerChannel>(\n  worker: Worker,\n): WorkerChannelReceiver<TWorkerChannel> {\n  const _events = new Map<string, MessageQueue<EventMessage>>()\n  const _streams = new Map<string, MessageQueue<StreamEmissionMessage>>()\n  const errors = new MessageQueue<{type: 'error'; error: unknown}>()\n\n  const eventQueue = (name: string) => {\n    const queue = _events.get(name) ?? new MessageQueue()\n    if (!_events.has(name)) _events.set(name, queue)\n    return queue\n  }\n\n  const streamQueue = (name: string) => {\n    const queue = _streams.get(name) ?? new MessageQueue()\n    if (!_streams.has(name)) _streams.set(name, queue)\n    return queue\n  }\n\n  const handleMessage = (message: unknown) => {\n    if (!isWorkerChannelMessage(message)) return\n    if (message.type === 'event') eventQueue(message.name).push(message)\n    if (message.type === 'emission') streamQueue(message.name).push(message)\n    if (message.type === 'end') streamQueue(message.name).end()\n  }\n\n  const handleError = (error: unknown) => {\n    errors.push({type: 'error', error})\n  }\n\n  worker.addListener('message', handleMessage)\n  worker.addListener('error', handleError)\n\n  return {\n    event: new Proxy({} as WorkerChannelReceiver<TWorkerChannel>['event'], {\n      get: (target, name) => {\n        if (typeof name !== 'string') return target[name as keyof typeof target]\n\n        const eventReceiver: EventReceiver = async () => {\n          const {value} = await Promise.race([eventQueue(name).next(), errors.next()])\n          if (value.type === 'error') throw value.error\n          return value.payload\n        }\n\n        return eventReceiver\n      },\n    }),\n    stream: new Proxy({} as WorkerChannelReceiver<TWorkerChannel>['stream'], {\n      get: (target, prop) => {\n        if (typeof prop !== 'string') return target[prop as keyof typeof target]\n        const name = prop // alias for better typescript narrowing\n\n        async function* streamReceiver() {\n          while (true) {\n            const {value, done} = await Promise.race([streamQueue(name).next(), errors.next()])\n            if (done) return\n            if (value.type === 'error') throw value.error\n            yield value.payload\n          }\n        }\n\n        return streamReceiver satisfies StreamReceiver\n      },\n    }),\n    dispose: () => {\n      worker.removeListener('message', handleMessage)\n      worker.removeListener('error', handleError)\n      return worker.terminate()\n    },\n  }\n}\n\n/**\n * Creates a \"worker channel reporter\" that sends messages to the given\n * `parentPort` to be received by a worker channel receiver.\n */\nexport function createReporter<TWorkerChannel extends WorkerChannel>(\n  parentPort: MessagePort | null,\n): WorkerChannelReporter<TWorkerChannel> {\n  if (!parentPort) {\n    throw new Error('parentPart was falsy')\n  }\n\n  return {\n    event: new Proxy({} as WorkerChannelReporter<TWorkerChannel>['event'], {\n      get: (target, name) => {\n        if (typeof name !== 'string') return target[name as keyof typeof target]\n\n        const eventReporter: EventReporter = (payload) => {\n          const message: EventMessage = {type: 'event', name, payload}\n          parentPort.postMessage(message)\n        }\n\n        return eventReporter\n      },\n    }),\n    stream: new Proxy({} as WorkerChannelReporter<TWorkerChannel>['stream'], {\n      get: (target, name) => {\n        if (typeof name !== 'string') return target[name as keyof typeof target]\n\n        const streamReporter: StreamReporter = {\n          emit: (payload) => {\n            const message: StreamEmissionMessage = {type: 'emission', name, payload}\n            parentPort.postMessage(message)\n          },\n          end: () => {\n            const message: StreamEndMessage = {type: 'end', name}\n            parentPort.postMessage(message)\n          },\n        }\n\n        return streamReporter\n      },\n    }),\n  }\n}\n","import fs from 'node:fs'\nimport os from 'node:os'\nimport path from 'node:path'\nimport readline from 'node:readline'\nimport {Readable} from 'node:stream'\nimport {isMainThread, parentPort, workerData as _workerData} from 'node:worker_threads'\n\nimport {\n  type ClientConfig,\n  createClient,\n  type SanityClient,\n  type SanityDocument,\n} from '@sanity/client'\nimport {isReference, type ValidationContext, type ValidationMarker} from '@sanity/types'\nimport {isRecord, validateDocument} from 'sanity'\n\nimport {extractDocumentsFromNdjsonOrTarball} from '../util/extractDocumentsFromNdjsonOrTarball'\nimport {getStudioWorkspaces} from '../util/getStudioWorkspaces'\nimport {mockBrowserEnvironment} from '../util/mockBrowserEnvironment'\nimport {\n  createReporter,\n  type WorkerChannel,\n  type WorkerChannelEvent,\n  type WorkerChannelStream,\n} from '../util/workerChannels'\n\nconst MAX_VALIDATION_CONCURRENCY = 100\nconst DOCUMENT_VALIDATION_TIMEOUT = 30000\nconst REFERENCE_INTEGRITY_BATCH_SIZE = 100\n\ninterface AvailabilityResponse {\n  omitted: {id: string; reason: 'existence' | 'permission'}[]\n}\n\n/** @internal */\nexport interface ValidateDocumentsWorkerData {\n  workDir: string\n  configPath?: string\n  workspace?: string\n  clientConfig?: Partial<ClientConfig>\n  projectId?: string\n  dataset?: string\n  ndjsonFilePath?: string\n  level?: ValidationMarker['level']\n  maxCustomValidationConcurrency?: number\n}\n\n/** @internal */\nexport type ValidationWorkerChannel = WorkerChannel<{\n  loadedWorkspace: WorkerChannelEvent<{\n    name: string\n    projectId: string\n    dataset: string\n    studioHost: string | null\n    basePath: string\n  }>\n  loadedDocumentCount: WorkerChannelEvent<{documentCount: number}>\n  exportProgress: WorkerChannelStream<{downloadedCount: number; documentCount: number}>\n  exportFinished: WorkerChannelEvent<{totalDocumentsToValidate: number}>\n  loadedReferenceIntegrity: WorkerChannelEvent\n  validation: WorkerChannelStream<{\n    validatedCount: number\n    documentId: string\n    documentType: string\n    intentUrl?: string\n    revision: string\n    level: ValidationMarker['level']\n    markers: ValidationMarker[]\n  }>\n}>\n\nconst {\n  clientConfig,\n  workDir,\n  workspace: workspaceName,\n  configPath,\n  dataset,\n  ndjsonFilePath,\n  projectId,\n  level,\n  maxCustomValidationConcurrency,\n} = _workerData as ValidateDocumentsWorkerData\n\nif (isMainThread || !parentPort) {\n  throw new Error('This module must be run as a worker thread')\n}\n\nconst levelValues = {error: 0, warning: 1, info: 2} as const\n\nconst report = createReporter<ValidationWorkerChannel>(parentPort)\n\nconst getReferenceIds = (value: unknown) => {\n  const ids = new Set<string>()\n\n  function traverse(node: unknown) {\n    if (isReference(node)) {\n      ids.add(node._ref)\n      return\n    }\n\n    if (typeof node === 'object' && node) {\n      // Note: this works for arrays too\n      for (const item of Object.values(node)) traverse(item)\n    }\n  }\n\n  traverse(value)\n\n  return ids\n}\n\nconst idRegex = /^[^-][A-Z0-9._-]*$/i\n\n// during testing, the `doc` endpoint 502'ed if given an invalid ID\nconst isValidId = (id: unknown) => typeof id === 'string' && idRegex.test(id)\nconst shouldIncludeDocument = (document: SanityDocument) => {\n  // Filter out system documents\n  return !document._type.startsWith('system.')\n}\n\nasync function* readerToGenerator(reader: ReadableStreamDefaultReader<Uint8Array>) {\n  while (true) {\n    const {value, done} = await reader.read()\n    if (value) yield value\n    if (done) return\n  }\n}\n\nvalidateDocuments()\n\nasync function loadWorkspace() {\n  const workspaces = await getStudioWorkspaces({basePath: workDir, configPath})\n\n  if (!workspaces.length) {\n    throw new Error(`Configuration did not return any workspaces.`)\n  }\n\n  let _workspace\n  if (workspaceName) {\n    _workspace = workspaces.find((w) => w.name === workspaceName)\n    if (!_workspace) {\n      throw new Error(`Could not find any workspaces with name \\`${workspaceName}\\``)\n    }\n  } else {\n    if (workspaces.length !== 1) {\n      throw new Error(\n        \"Multiple workspaces found. Please specify which workspace to use with '--workspace'.\",\n      )\n    }\n    _workspace = workspaces[0]\n  }\n  const workspace = _workspace\n\n  const client = createClient({\n    ...clientConfig,\n    dataset: dataset || workspace.dataset,\n    projectId: projectId || workspace.projectId,\n    requestTagPrefix: 'sanity.cli.validate',\n  }).config({apiVersion: 'v2021-03-25'})\n\n  let studioHost\n  try {\n    const project = await client.projects.getById(projectId || workspace.projectId)\n    studioHost = project.metadata.externalStudioHost || project.studioHost\n  } catch {\n    // no big deal if we fail to get the studio host\n    studioHost = null\n  }\n\n  report.event.loadedWorkspace({\n    projectId: workspace.projectId,\n    dataset: workspace.dataset,\n    name: workspace.name,\n    studioHost,\n    basePath: workspace.basePath,\n  })\n\n  return {workspace, client, studioHost}\n}\n\nasync function downloadFromExport(client: SanityClient) {\n  const exportUrl = new URL(client.getUrl(`/data/export/${client.config().dataset}`, false))\n\n  const documentCount = await client.fetch('length(*)')\n  report.event.loadedDocumentCount({documentCount})\n\n  const {token} = client.config()\n  const response = await fetch(exportUrl, {\n    headers: new Headers({...(token && {Authorization: `Bearer ${token}`})}),\n  })\n\n  const reader = response.body?.getReader()\n  if (!reader) throw new Error('Could not get reader from response body.')\n\n  let downloadedCount = 0\n  const referencedIds = new Set<string>()\n  const documentIds = new Set<string>()\n  const lines = readline.createInterface({input: Readable.from(readerToGenerator(reader))})\n\n  // Note: we stream the export to a file and then re-read from that file to\n  // make this less memory intensive.\n  // this is a similar pattern to the import/export CLI commands\n  const slugDate = new Date()\n    .toISOString()\n    .replace(/[^a-z0-9]/gi, '-')\n    .toLowerCase()\n  const tempOutputFile = path.join(os.tmpdir(), `sanity-validate-${slugDate}.ndjson`)\n  const outputStream = fs.createWriteStream(tempOutputFile)\n\n  for await (const line of lines) {\n    const document = JSON.parse(line) as SanityDocument\n\n    if (shouldIncludeDocument(document)) {\n      documentIds.add(document._id)\n      for (const referenceId of getReferenceIds(document)) {\n        referencedIds.add(referenceId)\n      }\n\n      outputStream.write(`${line}\\n`)\n    }\n\n    downloadedCount++\n    report.stream.exportProgress.emit({downloadedCount, documentCount})\n  }\n\n  await new Promise<void>((resolve, reject) =>\n    outputStream.close((err) => (err ? reject(err) : resolve())),\n  )\n\n  report.stream.exportProgress.end()\n  report.event.exportFinished({totalDocumentsToValidate: documentIds.size})\n\n  const getDocuments = () =>\n    extractDocumentsFromNdjsonOrTarball(fs.createReadStream(tempOutputFile))\n\n  return {documentIds, referencedIds, getDocuments, cleanup: () => fs.promises.rm(tempOutputFile)}\n}\n\nasync function downloadFromFile(filePath: string) {\n  const referencedIds = new Set<string>()\n  const documentIds = new Set<string>()\n  const getDocuments = () => extractDocumentsFromNdjsonOrTarball(fs.createReadStream(filePath))\n\n  for await (const document of getDocuments()) {\n    if (shouldIncludeDocument(document)) {\n      documentIds.add(document._id)\n      for (const referenceId of getReferenceIds(document)) {\n        referencedIds.add(referenceId)\n      }\n    }\n  }\n\n  report.event.exportFinished({totalDocumentsToValidate: documentIds.size})\n\n  return {documentIds, referencedIds, getDocuments, cleanup: undefined}\n}\n\ninterface CheckReferenceExistenceOptions {\n  client: SanityClient\n  referencedIds: Set<string>\n  documentIds: Set<string>\n}\n\nasync function checkReferenceExistence({\n  client,\n  documentIds,\n  referencedIds: _referencedIds,\n}: CheckReferenceExistenceOptions) {\n  const existingIds = new Set(documentIds)\n  const idsToCheck = Array.from(_referencedIds)\n    .filter((id) => !existingIds.has(id) && isValidId(id))\n    .sort()\n\n  const batches = idsToCheck.reduce<string[][]>(\n    (acc, next, index) => {\n      const batchIndex = Math.floor(index / REFERENCE_INTEGRITY_BATCH_SIZE)\n      const batch = acc[batchIndex]\n      batch.push(next)\n      return acc\n    },\n    Array.from<string[]>({\n      length: Math.ceil(idsToCheck.length / REFERENCE_INTEGRITY_BATCH_SIZE),\n    }).map(() => []),\n  )\n\n  for (const batch of batches) {\n    const {omitted} = await client.request<AvailabilityResponse>({\n      uri: client.getDataUrl('doc', batch.join(',')),\n      json: true,\n      query: {excludeContent: 'true'},\n      tag: 'documents-availability',\n    })\n\n    const omittedIds = omitted.reduce<Record<string, 'existence' | 'permission'>>((acc, next) => {\n      acc[next.id] = next.reason\n      return acc\n    }, {})\n\n    for (const id of batch) {\n      // unless the document ID is in the `omitted` object explictly due to\n      // the reason `'existence'`, then it should exist\n      if (omittedIds[id] !== 'existence') {\n        existingIds.add(id)\n      }\n    }\n  }\n  report.event.loadedReferenceIntegrity()\n\n  return {existingIds}\n}\n\nasync function validateDocuments() {\n  // note: this is dynamically imported because this module is ESM only and this\n  // file gets compiled to CJS at this time\n  const {default: pMap} = await import('p-map')\n\n  const cleanupBrowserEnvironment = mockBrowserEnvironment(workDir)\n\n  let cleanupDownloadedDocuments: (() => Promise<void>) | undefined\n\n  try {\n    const {client, workspace, studioHost} = await loadWorkspace()\n    const {documentIds, referencedIds, getDocuments, cleanup} = ndjsonFilePath\n      ? await downloadFromFile(ndjsonFilePath)\n      : await downloadFromExport(client)\n    cleanupDownloadedDocuments = cleanup\n    const {existingIds} = await checkReferenceExistence({client, referencedIds, documentIds})\n\n    const getClient = <TOptions extends Partial<ClientConfig>>(options: TOptions) =>\n      client.withConfig(options)\n\n    const getDocumentExists: ValidationContext['getDocumentExists'] = ({id}) =>\n      Promise.resolve(existingIds.has(id))\n\n    const getLevel = (markers: ValidationMarker[]) => {\n      let foundWarning = false\n      for (const marker of markers) {\n        if (marker.level === 'error') return 'error'\n        if (marker.level === 'warning') foundWarning = true\n      }\n\n      if (foundWarning) return 'warning'\n      return 'info'\n    }\n\n    let validatedCount = 0\n\n    const validate = async (document: SanityDocument) => {\n      let markers: ValidationMarker[]\n\n      try {\n        const timeout = Symbol('timeout')\n\n        const result = await Promise.race([\n          validateDocument({\n            document,\n            workspace,\n            getClient,\n            getDocumentExists,\n            environment: 'cli',\n            maxCustomValidationConcurrency,\n          }),\n          new Promise<typeof timeout>((resolve) =>\n            setTimeout(() => resolve(timeout), DOCUMENT_VALIDATION_TIMEOUT),\n          ),\n        ])\n\n        if (result === timeout) {\n          throw new Error(\n            `Document '${document._id}' failed to validate within ${DOCUMENT_VALIDATION_TIMEOUT}ms.`,\n          )\n        }\n\n        markers = result\n          // remove deprecated `item` from the marker\n          .map(({item, ...marker}) => marker)\n          // filter out unwanted levels\n          .filter((marker) => {\n            const markerValue = levelValues[marker.level]\n            const flagLevelValue =\n              levelValues[level as keyof typeof levelValues] ?? levelValues.info\n            return markerValue <= flagLevelValue\n          })\n      } catch (err) {\n        const errorMessage =\n          isRecord(err) && typeof err.message === 'string' ? err.message : 'Unknown error'\n\n        const message = `Exception occurred while validating value: ${errorMessage}`\n\n        markers = [\n          {\n            message,\n            level: 'error',\n            path: [],\n          },\n        ]\n      }\n\n      validatedCount++\n\n      const intentUrl =\n        studioHost &&\n        `${studioHost}${path.resolve(\n          workspace.basePath,\n          `/intent/edit/id=${encodeURIComponent(document._id)};type=${encodeURIComponent(\n            document._type,\n          )}`,\n        )}`\n\n      report.stream.validation.emit({\n        documentId: document._id,\n        documentType: document._type,\n        revision: document._rev,\n        ...(intentUrl && {intentUrl}),\n        markers,\n        validatedCount,\n        level: getLevel(markers),\n      })\n    }\n\n    await pMap(getDocuments(), validate, {concurrency: MAX_VALIDATION_CONCURRENCY})\n\n    report.stream.validation.end()\n  } finally {\n    await cleanupDownloadedDocuments?.()\n    cleanupBrowserEnvironment()\n  }\n}\n"],"names":["zlib","tar","path","readline","Readable","_workerData","isMainThread","parentPort","isReference","getStudioWorkspaces","client","createClient","os","fs","mockBrowserEnvironment","validateDocument","isRecord"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;AAQA,MAAM,cAAc,KAGd,SAAS,CAAC,QACd,IAAI,UAAU,KAAK,IAAI,CAAC,MAAM,MAAQ,IAAI,CAAC,MAAM,OAAQ,IAAI,CAAC,MAAM,GAGhE,YAAY,CAAC,QACjB,IAAI,UAAU,KAAK,IAAI,CAAC,MAAM,QAAS,IAAI,CAAC,MAAM,KAAK,IAAI,CAAC,MAAM,OAAQ,IAAI,CAAC,MAAM,MAGjF,QAAQ,CAAC,QACb,IAAI,UAAU,OACd,IAAI,GAAG,MAAM,OACb,IAAI,GAAG,MAAM,OACb,IAAI,GAAG,MAAM,OACb,IAAI,GAAG,MAAM,MACb,IAAI,GAAG,MAAM;AAEf,gBAAgB,QACd,QACA,WACA;AAIA,QAAM,UAAU,IAAI,QAAc,CAAC,SAAS,WAAW;AAErD,eAAW,YAAY;AACjB,UAAA;AACF,yBAAiB,SAAS,OAAkB,WAAA,MAAM,KAAK;AAC7C,kBAAA,OACV;eACO,KAAK;AACZ,eAAO,GAAG;AAAA,MACZ;AAAA,IAAA,CACD;AAAA,EAAA,CACF;AAGD,SAAO,WACP,MAAM,SACN,UAAU,QAAQ;AACpB;AASA,gBAAgB,mBAAmB,QAAsD;AACnF,MAAA,SAAS,OAAO,MAAM,CAAC;AAE3B,mBAAiB,SAAS,QAAQ;AAChC,QAAA,SAAS,OAAO,OAAO,CAAC,QAAQ,KAAK,CAAC,GAClC,OAAO,SAAS,YAAa;AAE3B,UAAA,aAAa,QACb,eAAe,mBAA+B;AAClD,YAAM,YACN,OAAO;AAAA,IAAA;AAGL,QAAA,OAAO,UAAU,GAAG;AACtB,aAAO,mBAAmB,QAAQ,aAAA,GAAgBA,cAAAA,QAAK,aAAc,CAAA,CAAC;AACtE;AAAA,IACF;AAEI,QAAA,UAAU,UAAU,GAAG;AACzB,aAAO,mBAAmB,QAAQ,aAAA,GAAgBA,cAAAA,QAAK,cAAe,CAAA,CAAC;AACvE;AAAA,IACF;AAEA,QAAI,MAAM,UAAU;AAClB,uBAAiB,SAAS,QAAQ,gBAAgBC,aAAI,QAAA,QAAA,CAAS,GAAG;AAChE,cAAM,WAAWC,cAAAA,QAAK,SAAS,MAAM,OAAO,IAAI;AAG5C,YAAA,EAFYA,cAAK,QAAA,QAAQ,QAAQ,EAAE,kBAEvB,aAAa,SAAS,WAAW,GAAG,IAEpD;AAAiB,2BAAA,eAAe,MAAa,OAAA;AAC7C;AAAA,QAAA;AAAA,MACF;AAGF,WAAO,aAAa;AAAA,EACtB;AACF;AAMA,gBAAuB,oCACrB,MAC+B;AACzB,QAAA,QAAQC,0BAAS,gBAAgB;AAAA,IACrC,OAAOC,YAAAA,SAAS,KAAK,mBAAmB,IAAI,CAAC;AAAA,EAAA,CAC9C;AAED,mBAAiB,QAAQ,OAAO;AACxB,UAAA,UAAU,KAAK;AACjB,gBAAS,MAAM,KAAK,MAAM,OAAO;AAAA,EACvC;AACA,QAAM,MAAM;AACd;AC0EO,SAAS,eACd,YACuC;AACvC,MAAI,CAAC;AACG,UAAA,IAAI,MAAM,sBAAsB;AAGjC,SAAA;AAAA,IACL,OAAO,IAAI,MAAM,IAAsD;AAAA,MACrE,KAAK,CAAC,QAAQ,SACR,OAAO,QAAS,WAAiB,OAAO,IAA2B,IAElC,CAAC,YAAY;AAChD,cAAM,UAAwB,EAAC,MAAM,SAAS,MAAM,QAAO;AAC3D,mBAAW,YAAY,OAAO;AAAA,MAChC;AAAA,IAAA,CAIH;AAAA,IACD,QAAQ,IAAI,MAAM,IAAuD;AAAA,MACvE,KAAK,CAAC,QAAQ,SACR,OAAO,QAAS,WAAiB,OAAO,IAA2B,IAEhC;AAAA,QACrC,MAAM,CAAC,YAAY;AACjB,gBAAM,UAAiC,EAAC,MAAM,YAAY,MAAM,QAAO;AACvE,qBAAW,YAAY,OAAO;AAAA,QAChC;AAAA,QACA,KAAK,MAAM;AACT,gBAAM,UAA4B,EAAC,MAAM,OAAO,KAAI;AACpD,qBAAW,YAAY,OAAO;AAAA,QAChC;AAAA,MACF;AAAA,IAAA,CAIH;AAAA,EAAA;AAEL;AC1MA,MAAM,6BAA6B,KAC7B,8BAA8B,KAC9B,iCAAiC,KA2CjC;AAAA,EACJ;AAAA,EACA;AAAA,EACA,WAAW;AAAA,EACX;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF,IAAIC;AAEJ,IAAIC,oBAAAA,gBAAgB,CAACC,oBAAA;AACb,QAAA,IAAI,MAAM,4CAA4C;AAG9D,MAAM,cAAc,EAAC,OAAO,GAAG,SAAS,GAAG,MAAM,EAAC,GAE5C,SAAS,eAAwCA,oBAAAA,UAAU,GAE3D,kBAAkB,CAAC,UAAmB;AACpC,QAAA,0BAAU;AAEhB,WAAS,SAAS,MAAe;AAC3B,QAAAC,MAAAA,YAAY,IAAI,GAAG;AACjB,UAAA,IAAI,KAAK,IAAI;AACjB;AAAA,IACF;AAEI,QAAA,OAAO,QAAS,YAAY;AAE9B,iBAAW,QAAQ,OAAO,OAAO,IAAI,YAAY,IAAI;AAAA,EAEzD;AAEA,SAAA,SAAS,KAAK,GAEP;AACT,GAEM,UAAU,uBAGV,YAAY,CAAC,OAAgB,OAAO,MAAO,YAAY,QAAQ,KAAK,EAAE,GACtE,wBAAwB,CAAC,aAEtB,CAAC,SAAS,MAAM,WAAW,SAAS;AAG7C,gBAAgB,kBAAkB,QAAiD;AACpE,aAAA;AACX,UAAM,EAAC,OAAO,KAAA,IAAQ,MAAM,OAAO,KAAK;AACpC,QAAA,UAAO,MAAM,QACb,KAAM;AAAA,EACZ;AACF;AAEA;AAEA,eAAe,gBAAgB;AAC7B,QAAM,aAAa,MAAMC,wCAAoB,EAAC,UAAU,SAAS,YAAW;AAE5E,MAAI,CAAC,WAAW;AACR,UAAA,IAAI,MAAM,8CAA8C;AAG5D,MAAA;AACA,MAAA;AACF,QAAA,aAAa,WAAW,KAAK,CAAC,MAAM,EAAE,SAAS,aAAa,GACxD,CAAC;AACH,YAAM,IAAI,MAAM,6CAA6C,aAAa,IAAI;AAAA,SAE3E;AACL,QAAI,WAAW,WAAW;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MAAA;AAGJ,iBAAa,WAAW,CAAC;AAAA,EAC3B;AACM,QAAA,YAAY,YAEZC,WAASC,oBAAa;AAAA,IAC1B,GAAG;AAAA,IACH,SAAS,WAAW,UAAU;AAAA,IAC9B,WAAW,aAAa,UAAU;AAAA,IAClC,kBAAkB;AAAA,EACnB,CAAA,EAAE,OAAO,EAAC,YAAY,cAAc,CAAA;AAEjC,MAAA;AACA,MAAA;AACF,UAAM,UAAU,MAAMD,SAAO,SAAS,QAAQ,aAAa,UAAU,SAAS;AACjE,iBAAA,QAAQ,SAAS,sBAAsB,QAAQ;AAAA,EAAA,QACtD;AAEO,iBAAA;AAAA,EACf;AAEA,SAAA,OAAO,MAAM,gBAAgB;AAAA,IAC3B,WAAW,UAAU;AAAA,IACrB,SAAS,UAAU;AAAA,IACnB,MAAM,UAAU;AAAA,IAChB;AAAA,IACA,UAAU,UAAU;AAAA,EACrB,CAAA,GAEM,EAAC,mBAAWA,UAAQ,WAAU;AACvC;AAEA,eAAe,mBAAmBA,SAAsB;AACtD,QAAM,YAAY,IAAI,IAAIA,QAAO,OAAO,gBAAgBA,QAAO,OAAO,EAAE,OAAO,IAAI,EAAK,CAAC,GAEnF,gBAAgB,MAAMA,QAAO,MAAM,WAAW;AACpD,SAAO,MAAM,oBAAoB,EAAC,cAAc,CAAA;AAE1C,QAAA,EAAC,MAAS,IAAAA,QAAO,OAKjB,GAAA,UAJW,MAAM,MAAM,WAAW;AAAA,IACtC,SAAS,IAAI,QAAQ,EAAC,GAAI,SAAS,EAAC,eAAe,UAAU,KAAK,GAAE,GAAG;AAAA,EAAA,CACxE,GAEuB,MAAM;AAC9B,MAAI,CAAC,OAAc,OAAA,IAAI,MAAM,0CAA0C;AAEvE,MAAI,kBAAkB;AAChB,QAAA,oCAAoB,OACpB,cAAkB,oBAAA,OAClB,QAAQP,kBAAAA,QAAS,gBAAgB,EAAC,OAAOC,qBAAS,KAAK,kBAAkB,MAAM,CAAC,EAAA,CAAE,GAKlF,YAAW,oBAAI,KAClB,GAAA,YAAA,EACA,QAAQ,eAAe,GAAG,EAC1B,YAAA,GACG,iBAAiBF,sBAAK,KAAKU,YAAG,QAAA,OAAA,GAAU,mBAAmB,QAAQ,SAAS,GAC5E,eAAeC,YAAG,QAAA,kBAAkB,cAAc;AAExD,mBAAiB,QAAQ,OAAO;AACxB,UAAA,WAAW,KAAK,MAAM,IAAI;AAE5B,QAAA,sBAAsB,QAAQ,GAAG;AACvB,kBAAA,IAAI,SAAS,GAAG;AACjB,iBAAA,eAAe,gBAAgB,QAAQ;AAChD,sBAAc,IAAI,WAAW;AAGlB,mBAAA,MAAM,GAAG,IAAI;AAAA,CAAI;AAAA,IAChC;AAEA,uBACA,OAAO,OAAO,eAAe,KAAK,EAAC,iBAAiB,eAAc;AAAA,EACpE;AAEA,SAAA,MAAM,IAAI;AAAA,IAAc,CAAC,SAAS,WAChC,aAAa,MAAM,CAAC,QAAS,MAAM,OAAO,GAAG,IAAI,QAAA,CAAU;AAAA,EAG7D,GAAA,OAAO,OAAO,eAAe,IAC7B,GAAA,OAAO,MAAM,eAAe,EAAC,0BAA0B,YAAY,KAAA,CAAK,GAKjE,EAAC,aAAa,eAAe,cAHf,MACnB,oCAAoCA,YAAAA,QAAG,iBAAiB,cAAc,CAAC,GAEvB,SAAS,MAAMA,YAAAA,QAAG,SAAS,GAAG,cAAc,EAAC;AACjG;AAEA,eAAe,iBAAiB,UAAkB;AAChD,QAAM,gBAAgB,oBAAI,OACpB,cAAkB,oBAAA,IAAA,GAClB,eAAe,MAAM,oCAAoCA,YAAAA,QAAG,iBAAiB,QAAQ,CAAC;AAE5F,mBAAiB,YAAY,aAAa;AACpC,QAAA,sBAAsB,QAAQ,GAAG;AACvB,kBAAA,IAAI,SAAS,GAAG;AACjB,iBAAA,eAAe,gBAAgB,QAAQ;AAChD,sBAAc,IAAI,WAAW;AAAA,IAEjC;AAGF,SAAA,OAAO,MAAM,eAAe,EAAC,0BAA0B,YAAY,KAAA,CAAK,GAEjE,EAAC,aAAa,eAAe,cAAc,SAAS,OAAS;AACtE;AAQA,eAAe,wBAAwB;AAAA,EACrC,QAAAH;AAAA,EACA;AAAA,EACA,eAAe;AACjB,GAAmC;AAC3B,QAAA,cAAc,IAAI,IAAI,WAAW,GACjC,aAAa,MAAM,KAAK,cAAc,EACzC,OAAO,CAAC,OAAO,CAAC,YAAY,IAAI,EAAE,KAAK,UAAU,EAAE,CAAC,EACpD,KAEG,GAAA,UAAU,WAAW;AAAA,IACzB,CAAC,KAAK,MAAM,UAAU;AACpB,YAAM,aAAa,KAAK,MAAM,QAAQ,8BAA8B;AAEpE,aADc,IAAI,UAAU,EACtB,KAAK,IAAI,GACR;AAAA,IACT;AAAA,IACA,MAAM,KAAe;AAAA,MACnB,QAAQ,KAAK,KAAK,WAAW,SAAS,8BAA8B;AAAA,IAAA,CACrE,EAAE,IAAI,MAAM,EAAE;AAAA,EAAA;AAGjB,aAAW,SAAS,SAAS;AAC3B,UAAM,EAAC,QAAA,IAAW,MAAMA,QAAO,QAA8B;AAAA,MAC3D,KAAKA,QAAO,WAAW,OAAO,MAAM,KAAK,GAAG,CAAC;AAAA,MAC7C,MAAM;AAAA,MACN,OAAO,EAAC,gBAAgB,OAAM;AAAA,MAC9B,KAAK;AAAA,IAAA,CACN,GAEK,aAAa,QAAQ,OAAmD,CAAC,KAAK,UAClF,IAAI,KAAK,EAAE,IAAI,KAAK,QACb,MACN,CAAA,CAAE;AAEL,eAAW,MAAM;AAGX,iBAAW,EAAE,MAAM,eACrB,YAAY,IAAI,EAAE;AAAA,EAGxB;AACA,SAAA,OAAO,MAAM,yBAAA,GAEN,EAAC,YAAW;AACrB;AAEA,eAAe,oBAAoB;AAG3B,QAAA,EAAC,SAAS,KAAA,IAAQ,MAAM,OAAO,OAAO,GAEtC,4BAA4BI,8CAAuB,OAAO;AAE5D,MAAA;AAEA,MAAA;AACI,UAAA,EAAC,QAAAJ,SAAQ,WAAW,WAAA,IAAc,MAAM,iBACxC,EAAC,aAAa,eAAe,cAAc,YAAW,iBACxD,MAAM,iBAAiB,cAAc,IACrC,MAAM,mBAAmBA,OAAM;AACN,iCAAA;AAC7B,UAAM,EAAC,YAAe,IAAA,MAAM,wBAAwB,EAAC,QAAAA,SAAQ,eAAe,YAAA,CAAY,GAElF,YAAY,CAAyC,YACzDA,QAAO,WAAW,OAAO,GAErB,oBAA4D,CAAC,EAAC,SAClE,QAAQ,QAAQ,YAAY,IAAI,EAAE,CAAC,GAE/B,WAAW,CAAC,YAAgC;AAChD,UAAI,eAAe;AACnB,iBAAW,UAAU,SAAS;AACxB,YAAA,OAAO,UAAU,QAAgB,QAAA;AACjC,eAAO,UAAU,cAAW,eAAe;AAAA,MACjD;AAEA,aAAI,eAAqB,YAClB;AAAA,IAAA;AAGT,QAAI,iBAAiB;AAEf,UAAA,WAAW,OAAO,aAA6B;AAC/C,UAAA;AAEA,UAAA;AACF,cAAM,UAAU,OAAO,SAAS,GAE1B,SAAS,MAAM,QAAQ,KAAK;AAAA,UAChCK,wBAAiB;AAAA,YACf;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA,aAAa;AAAA,YACb;AAAA,UAAA,CACD;AAAA,UACD,IAAI;AAAA,YAAwB,CAAC,YAC3B,WAAW,MAAM,QAAQ,OAAO,GAAG,2BAA2B;AAAA,UAChE;AAAA,QAAA,CACD;AAED,YAAI,WAAW;AACb,gBAAM,IAAI;AAAA,YACR,aAAa,SAAS,GAAG,+BAA+B,2BAA2B;AAAA,UAAA;AAIvF,kBAAU,OAEP,IAAI,CAAC,EAAC,MAAM,GAAG,OAAM,MAAM,MAAM,EAEjC,OAAO,CAAC,WAAW;AACZ,gBAAA,cAAc,YAAY,OAAO,KAAK,GACtC,iBACJ,YAAY,KAAiC,KAAK,YAAY;AAChE,iBAAO,eAAe;AAAA,QAAA,CACvB;AAAA,eACI,KAAK;AAMF,kBAAA;AAAA,UACR;AAAA,YACE,SAJY,8CAFdC,OAAA,SAAS,GAAG,KAAK,OAAO,IAAI,WAAY,WAAW,IAAI,UAAU,eAEO;AAAA,YAKtE,OAAO;AAAA,YACP,MAAM,CAAC;AAAA,UACT;AAAA,QAAA;AAAA,MAEJ;AAEA;AAEA,YAAM,YACJ,cACA,GAAG,UAAU,GAAGd,cAAK,QAAA;AAAA,QACnB,UAAU;AAAA,QACV,mBAAmB,mBAAmB,SAAS,GAAG,CAAC,SAAS;AAAA,UAC1D,SAAS;AAAA,QAAA,CACV;AAAA,MACF,CAAA;AAEI,aAAA,OAAO,WAAW,KAAK;AAAA,QAC5B,YAAY,SAAS;AAAA,QACrB,cAAc,SAAS;AAAA,QACvB,UAAU,SAAS;AAAA,QACnB,GAAI,aAAa,EAAC,UAAS;AAAA,QAC3B;AAAA,QACA;AAAA,QACA,OAAO,SAAS,OAAO;AAAA,MAAA,CACxB;AAAA,IAAA;AAGH,UAAM,KAAK,gBAAgB,UAAU,EAAC,aAAa,2BAA2B,CAAA,GAE9E,OAAO,OAAO,WAAW,IAAI;AAAA,EAAA,UAC7B;AACM,UAAA,6BAAA,GACN;EACF;AACF;"}