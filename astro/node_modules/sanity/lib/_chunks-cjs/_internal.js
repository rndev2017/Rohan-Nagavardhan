"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf, __hasOwnProp = Object.prototype.hasOwnProperty;
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from == "object" || typeof from == "function")
    for (let key of __getOwnPropNames(from))
      !__hasOwnProp.call(to, key) && key !== except && __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: !0 }) : target,
  mod
));
var debug$3 = require("debug"), fs = require("node:fs"), fs$1 = require("node:fs/promises"), os = require("node:os"), path = require("node:path"), promises = require("node:stream/promises"), fs$2 = require("@sanity/util/fs"), asyncMutex = require("async-mutex"), isString = require("lodash/isString.js"), prettyMs = require("pretty-ms"), helpers = require("yargs/helpers"), yargs = require("yargs/yargs"), zlib = require("node:zlib"), rimraf = require("rimraf"), getIt = require("get-it"), middleware = require("get-it/middleware"), node_stream = require("node:stream"), consoleTablePrinter = require("console-table-printer"), dateFns = require("date-fns"), url = require("node:url"), logSymbols = require("log-symbols"), oneline = require("oneline"), EventSource = require("@sanity/eventsource"), rxjs = require("rxjs"), exportDataset = require("@sanity/export"), sanityImport = require("@sanity/import"), padStart = require("lodash/padStart.js"), uuid = require("@sanity/uuid"), chokidar = require("chokidar"), execa = require("execa"), json5 = require("json5"), isEqual = require("lodash/isEqual.js"), isPlainObject = require("lodash/isPlainObject.js"), noop$1 = require("lodash/noop.js"), pluralize = require("pluralize-esm"), tokenize = require("json-lexer"), open = require("open"), node_util = require("node:util"), groupBy = require("lodash/groupBy.js"), deburr = require("lodash/deburr.js"), node = require("esbuild-register/dist/node"), migrate = require("@sanity/migrate"), node_tty = require("node:tty"), types = require("@sanity/types"), size = require("lodash/size.js"), sortBy = require("lodash/sortBy.js");
function _interopDefaultCompat(e) {
  return e && typeof e == "object" && "default" in e ? e : { default: e };
}
var debug__default = /* @__PURE__ */ _interopDefaultCompat(debug$3), fs__default = /* @__PURE__ */ _interopDefaultCompat(fs), fs__default$1 = /* @__PURE__ */ _interopDefaultCompat(fs$1), os__default = /* @__PURE__ */ _interopDefaultCompat(os), path__default = /* @__PURE__ */ _interopDefaultCompat(path), isString__default = /* @__PURE__ */ _interopDefaultCompat(isString), prettyMs__default = /* @__PURE__ */ _interopDefaultCompat(prettyMs), yargs__default = /* @__PURE__ */ _interopDefaultCompat(yargs), zlib__default = /* @__PURE__ */ _interopDefaultCompat(zlib), rimraf__default = /* @__PURE__ */ _interopDefaultCompat(rimraf), url__default = /* @__PURE__ */ _interopDefaultCompat(url), logSymbols__default = /* @__PURE__ */ _interopDefaultCompat(logSymbols), oneline__default = /* @__PURE__ */ _interopDefaultCompat(oneline), EventSource__default = /* @__PURE__ */ _interopDefaultCompat(EventSource), exportDataset__default = /* @__PURE__ */ _interopDefaultCompat(exportDataset), sanityImport__default = /* @__PURE__ */ _interopDefaultCompat(sanityImport), padStart__default = /* @__PURE__ */ _interopDefaultCompat(padStart), chokidar__default = /* @__PURE__ */ _interopDefaultCompat(chokidar), execa__default = /* @__PURE__ */ _interopDefaultCompat(execa), json5__default = /* @__PURE__ */ _interopDefaultCompat(json5), isEqual__default = /* @__PURE__ */ _interopDefaultCompat(isEqual), isPlainObject__default = /* @__PURE__ */ _interopDefaultCompat(isPlainObject), noop__default = /* @__PURE__ */ _interopDefaultCompat(noop$1), pluralize__default = /* @__PURE__ */ _interopDefaultCompat(pluralize), tokenize__default = /* @__PURE__ */ _interopDefaultCompat(tokenize), open__default = /* @__PURE__ */ _interopDefaultCompat(open), groupBy__default = /* @__PURE__ */ _interopDefaultCompat(groupBy), deburr__default = /* @__PURE__ */ _interopDefaultCompat(deburr), size__default = /* @__PURE__ */ _interopDefaultCompat(size), sortBy__default = /* @__PURE__ */ _interopDefaultCompat(sortBy);
const defaultApiVersion$1 = "v2024-02-21", datasetBackupGroup = {
  name: "backup",
  signature: "[COMMAND]",
  description: "Manage backups.",
  isGroupRoot: !0
};
function parseApiErr(err) {
  var _a, _b, _c, _d;
  const apiErr = {};
  return err.code ? apiErr.statusCode = err.code : err.statusCode && (apiErr.statusCode = err.statusCode), err.message ? apiErr.message = err.message : err.statusMessage ? apiErr.message = err.statusMessage : (_b = (_a = err == null ? void 0 : err.response) == null ? void 0 : _a.body) != null && _b.message ? apiErr.message = err.response.body.message : (_d = (_c = err == null ? void 0 : err.response) == null ? void 0 : _c.data) != null && _d.message ? apiErr.message = err.response.data.message : apiErr.message = JSON.stringify(err), apiErr;
}
const debug$2 = debug__default.default("sanity:core");
function validateDatasetName(datasetName) {
  if (!datasetName)
    return "Dataset name is missing";
  const name = `${datasetName}`;
  return name.toLowerCase() !== name ? "Dataset name must be all lowercase characters" : name.length < 2 ? "Dataset name must be at least two characters long" : name.length > 64 ? "Dataset name must be at most 64 characters" : /^[a-z0-9]/.test(name) ? /^[a-z0-9][-_a-z0-9]+$/.test(name) ? /[-_]$/.test(name) ? "Dataset name must not end with a dash or an underscore" : !1 : "Dataset name must only contain letters, numbers, dashes and underscores" : "Dataset name must start with a letter or a number";
}
function promptForDatasetName(prompt, options = {}) {
  return prompt.single({
    type: "input",
    message: "Dataset name:",
    validate: (name) => validateDatasetName(name) || !0,
    ...options
  });
}
async function chooseDatasetPrompt(context, options = {}) {
  const { apiClient, prompt } = context, { message, allowCreation } = options, client = apiClient(), datasets = await client.datasets.list(), hasProduction = datasets.find((dataset) => dataset.name === "production"), datasetChoices = datasets.map((dataset) => ({ value: dataset.name })), selected = await prompt.single({
    message: message || "Select dataset to use",
    type: "list",
    choices: allowCreation ? [{ value: "new", name: "Create new dataset" }, new prompt.Separator(), ...datasetChoices] : datasetChoices
  });
  if (selected === "new") {
    debug$2("User wants to create a new dataset, prompting for name");
    const newDatasetName = await promptForDatasetName(prompt, {
      message: "Name your dataset:",
      default: hasProduction ? void 0 : "production"
    });
    return await client.datasets.create(newDatasetName), newDatasetName;
  }
  return selected;
}
async function resolveApiClient(context, datasetName, apiVersion) {
  const { apiClient } = context;
  let client = apiClient();
  const { projectId, token } = client.config();
  if (!projectId)
    throw new Error("Project ID not defined");
  let selectedDataset = datasetName;
  return selectedDataset || (selectedDataset = await chooseDatasetPrompt(context, {
    message: "Select the dataset name:"
  })), client = client.withConfig({ dataset: datasetName, apiVersion }), {
    projectId,
    datasetName: selectedDataset,
    token,
    client
  };
}
const helpText$y = `
Examples
  sanity backup disable DATASET_NAME
`, disableDatasetBackupCommand = {
  name: "disable",
  group: "backup",
  signature: "[DATASET_NAME]",
  description: "Disable backup for a dataset.",
  helpText: helpText$y,
  action: async (args, context) => {
    const { output, chalk } = context, [dataset] = args.argsWithoutOptions, { projectId, datasetName, token, client } = await resolveApiClient(
      context,
      dataset,
      defaultApiVersion$1
    );
    try {
      await client.request({
        method: "PUT",
        headers: { Authorization: `Bearer ${token}` },
        uri: `/projects/${projectId}/datasets/${datasetName}/settings/backups`,
        body: {
          enabled: !1
        }
      }), output.print(`${chalk.green(`Disabled daily backups for dataset ${datasetName}
`)}`);
    } catch (error) {
      const { message } = parseApiErr(error);
      output.print(`${chalk.red(`Disabling dataset backup failed: ${message}`)}
`);
    }
  }
};
var debug$1 = require("debug")("sanity:backup");
const archiver = require("archiver");
function archiveDir(tmpOutDir, outFilePath, progressCb) {
  return new Promise((resolve, reject) => {
    const archiveDestination = fs.createWriteStream(outFilePath);
    archiveDestination.on("error", (err) => {
      reject(err);
    }), archiveDestination.on("close", () => {
      resolve();
    });
    const archive = archiver("tar", {
      gzip: !0,
      gzipOptions: { level: zlib__default.default.constants.Z_DEFAULT_COMPRESSION }
    });
    archive.on("error", (err) => {
      debug$1(`Archiving errored!
%s`, err.stack), reject(err);
    }), archive.on("warning", (err) => {
      debug$1("Archive warning: %s", err.message);
    }), archive.on("progress", (progress2) => {
      progressCb(progress2.fs.processedBytes);
    }), archive.pipe(archiveDestination), archive.directory(tmpOutDir, !1), archive.finalize();
  });
}
const maxBackupIdsShown = 100;
async function chooseBackupIdPrompt(context, datasetName) {
  var _a;
  const { prompt } = context, { projectId, token, client } = await resolveApiClient(context, datasetName, defaultApiVersion$1);
  try {
    const response = await client.request({
      headers: { Authorization: `Bearer ${token}` },
      uri: `/projects/${projectId}/datasets/${datasetName}/backups`,
      query: { limit: maxBackupIdsShown.toString() }
    });
    if (((_a = response == null ? void 0 : response.backups) == null ? void 0 : _a.length) > 0) {
      const backupIdChoices = response.backups.map((backup) => ({
        value: backup.id
      }));
      return await prompt.single({
        message: `Select backup ID to use (only last ${maxBackupIdsShown} shown)`,
        type: "list",
        choices: backupIdChoices
      });
    }
  } catch (err) {
    throw new Error(`Failed to fetch backups for dataset ${datasetName}: ${err.message}`);
  }
  throw new Error("No backups found");
}
function cleanupTmpDir(tmpDir) {
  rimraf__default.default(tmpDir, (err) => {
    err && debug$1(`Error cleaning up temporary files: ${err.message}`);
  });
}
const MAX_RETRIES = 5, BACKOFF_DELAY_BASE = 200, exponentialBackoff = (retryCount) => Math.pow(2, retryCount) * BACKOFF_DELAY_BASE;
async function withRetry(operation, maxRetries = MAX_RETRIES) {
  for (let retryCount = 0; retryCount < maxRetries; retryCount++)
    try {
      return await operation();
    } catch (err) {
      if (err.response && err.response.statusCode && err.response.statusCode < 500)
        throw err;
      const retryDelay = exponentialBackoff(retryCount);
      debug$1(`Error encountered, retrying after ${retryDelay}ms: %s`, err.message), await new Promise((resolve) => setTimeout(resolve, retryDelay));
    }
  throw new Error("Operation failed after all retries");
}
const CONNECTION_TIMEOUT$1 = 15 * 1e3, READ_TIMEOUT$1 = 3 * 60 * 1e3, request$1 = getIt.getIt([middleware.keepAlive(), middleware.promise()]);
async function downloadAsset(url2, fileName, fileType, outDir) {
  const normalizedFileName = path__default.default.basename(fileName), assetFilePath = getAssetFilePath(normalizedFileName, fileType, outDir);
  await withRetry(async () => {
    const response = await request$1({
      url: url2,
      maxRedirects: 5,
      timeout: { connect: CONNECTION_TIMEOUT$1, socket: READ_TIMEOUT$1 },
      stream: !0
    });
    debug$1("Received asset %s with status code %d", normalizedFileName, response == null ? void 0 : response.statusCode), await promises.pipeline(response.body, fs.createWriteStream(assetFilePath));
  });
}
function getAssetFilePath(fileName, fileType, outDir) {
  let assetFilePath = "";
  return fileType === "image" ? assetFilePath = path__default.default.join(outDir, "images", fileName) : fileType === "file" && (assetFilePath = path__default.default.join(outDir, "files", fileName)), assetFilePath;
}
const CONNECTION_TIMEOUT = 15 * 1e3, READ_TIMEOUT = 3 * 60 * 1e3, request = getIt.getIt([middleware.keepAlive(), middleware.promise()]);
async function downloadDocument(url2) {
  const response = await withRetry(
    () => request({
      url: url2,
      maxRedirects: 5,
      timeout: { connect: CONNECTION_TIMEOUT, socket: READ_TIMEOUT }
    })
  );
  return debug$1("Received document from %s with status code %d", url2, response == null ? void 0 : response.statusCode), response.body;
}
var __defProp2 = Object.defineProperty, __defNormalProp = (obj, key, value) => key in obj ? __defProp2(obj, key, { enumerable: !0, configurable: !0, writable: !0, value }) : obj[key] = value, __publicField = (obj, key, value) => __defNormalProp(obj, typeof key != "symbol" ? key + "" : key, value);
class PaginatedGetBackupStream extends node_stream.Readable {
  constructor(client, projectId, datasetName, backupId, token) {
    super({ objectMode: !0 }), __publicField(this, "cursor", ""), __publicField(this, "client"), __publicField(this, "projectId"), __publicField(this, "datasetName"), __publicField(this, "backupId"), __publicField(this, "token"), __publicField(this, "totalFiles", 0), this.client = client, this.projectId = projectId, this.datasetName = datasetName, this.backupId = backupId, this.token = token;
  }
  async _read() {
    try {
      const data = await this.fetchNextBackupPage();
      this.totalFiles === 0 && (this.totalFiles = data.totalFiles), data.files.forEach((file) => this.push(file)), typeof data.nextCursor == "string" && data.nextCursor !== "" ? this.cursor = data.nextCursor : this.push(null);
    } catch (err) {
      this.destroy(err);
    }
  }
  // fetchNextBackupPage fetches the next page of backed up files from the backup API.
  async fetchNextBackupPage() {
    const query = this.cursor === "" ? {} : { nextCursor: this.cursor };
    try {
      return await this.client.request({
        headers: { Authorization: `Bearer ${this.token}` },
        uri: `/projects/${this.projectId}/datasets/${this.datasetName}/backups/${this.backupId}`,
        query
      });
    } catch (error) {
      let msg = error.statusCode ? error.response.body.message : error.message;
      throw msg === void 0 && (msg = String(error)), new Error(`Downloading dataset backup failed: ${msg}`);
    }
  }
}
const newProgress = (output, startStep) => {
  let spinner = output.spinner(startStep).start(), lastProgress = { step: startStep }, start = Date.now();
  const print = (progress2) => {
    const elapsed = prettyMs__default.default(Date.now() - start);
    progress2.current && progress2.current > 0 && progress2.total && progress2.total > 0 ? spinner.text = `${progress2.step} (${progress2.current}/${progress2.total}) [${elapsed}]` : spinner.text = `${progress2.step} [${elapsed}]`;
  };
  return {
    set: (progress2) => {
      progress2.step !== lastProgress.step ? (print(lastProgress), spinner.succeed(), spinner = output.spinner(progress2.step).start(), start = Date.now()) : progress2.step === lastProgress.step && progress2.update && print(progress2), lastProgress = progress2;
    },
    update: (progress2) => {
      print(progress2), lastProgress = progress2;
    },
    succeed: () => {
      spinner.succeed(), start = Date.now();
    },
    fail: () => {
      spinner.fail(), start = Date.now();
    }
  };
};
function humanFileSize(size2) {
  const i = size2 == 0 ? 0 : Math.floor(Math.log(size2) / Math.log(1024));
  return `${(size2 / Math.pow(1024, i)).toFixed(2)} ${["B", "kB", "MB", "GB", "TB"][i]}`;
}
function isPathDirName(filepath) {
  return !/\.\w+$/.test(filepath);
}
const debug = debug__default.default("sanity:backup"), DEFAULT_DOWNLOAD_CONCURRENCY = 10, MAX_DOWNLOAD_CONCURRENCY = 24, helpText$x = `
Options
  --backup-id <string> The backup ID to download. (required)
  --out <string>       The file or directory path the backup should download to.
  --overwrite          Allows overwriting of existing backup file.
  --concurrency <num>  Concurrent number of backup item downloads. (max: 24)

Examples
  sanity backup download DATASET_NAME --backup-id 2024-01-01-backup-1
  sanity backup download DATASET_NAME --backup-id 2024-01-01-backup-2 --out /path/to/file
  sanity backup download DATASET_NAME --backup-id 2024-01-01-backup-3 --out /path/to/file --overwrite
`;
function parseCliFlags$7(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).options("backup-id", { type: "string" }).options("out", { type: "string" }).options("concurrency", { type: "number", default: DEFAULT_DOWNLOAD_CONCURRENCY }).options("overwrite", { type: "boolean", default: !1 }).argv;
}
const downloadBackupCommand = {
  name: "download",
  group: "backup",
  signature: "[DATASET_NAME]",
  description: "Download a dataset backup to a local file.",
  helpText: helpText$x,
  // eslint-disable-next-line max-statements
  action: async (args, context) => {
    const { output, chalk } = context, [client, opts] = await prepareBackupOptions(context, args), { projectId, datasetName, backupId, outDir, outFileName } = opts;
    if (outDir === "" || outFileName === "") {
      output.print("Operation cancelled.");
      return;
    }
    const outFilePath = path__default.default.join(outDir, outFileName);
    output.print("\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E"), output.print("\u2502                                                           \u2502"), output.print("\u2502 Downloading backup for:                                   \u2502"), output.print(`\u2502 ${chalk.bold("projectId")}: ${chalk.cyan(projectId).padEnd(56)} \u2502`), output.print(`\u2502 ${chalk.bold("dataset")}: ${chalk.cyan(datasetName).padEnd(58)} \u2502`), output.print(`\u2502 ${chalk.bold("backupId")}: ${chalk.cyan(backupId).padEnd(56)} \u2502`), output.print("\u2502                                                           \u2502"), output.print("\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F"), output.print(""), output.print(`Downloading backup to "${chalk.cyan(outFilePath)}"`);
    const start = Date.now(), progressSpinner = newProgress(output, "Setting up backup environment..."), tmpOutDir = await fs$1.mkdtemp(path__default.default.join(os.tmpdir(), "sanity-backup-"));
    for (const dir of [outDir, path__default.default.join(tmpOutDir, "images"), path__default.default.join(tmpOutDir, "files")])
      fs.mkdirSync(dir, { recursive: !0 });
    debug("Writing to temporary directory %s", tmpOutDir);
    const tmpOutDocumentsFile = path__default.default.join(tmpOutDir, "data.ndjson"), docOutStream = fs.createWriteStream(tmpOutDocumentsFile), docWriteMutex = new asyncMutex.Mutex();
    try {
      const backupFileStream = new PaginatedGetBackupStream(
        client,
        opts.projectId,
        opts.datasetName,
        opts.backupId,
        opts.token
      ), files = [];
      let i = 0;
      for await (const file of backupFileStream)
        files.push(file), i++, progressSpinner.set({
          step: "Reading backup files...",
          update: !0,
          current: i,
          total: backupFileStream.totalFiles
        });
      let totalItemsDownloaded = 0;
      const { default: pMap } = await import("p-map");
      await pMap(
        files,
        async (file) => {
          if (file.type === "file" || file.type === "image")
            await downloadAsset(file.url, file.name, file.type, tmpOutDir);
          else {
            const doc = await downloadDocument(file.url);
            await docWriteMutex.runExclusive(() => {
              docOutStream.write(`${doc}
`);
            });
          }
          totalItemsDownloaded += 1, progressSpinner.set({
            step: "Downloading documents and assets...",
            update: !0,
            current: totalItemsDownloaded,
            total: backupFileStream.totalFiles
          });
        },
        { concurrency: opts.concurrency }
      );
    } catch (error) {
      progressSpinner.fail();
      const { message } = parseApiErr(error);
      throw new Error(`Downloading dataset backup failed: ${message}`);
    }
    docOutStream.end(), await promises.finished(docOutStream), progressSpinner.set({ step: "Archiving files into a tarball...", update: !0 });
    try {
      await archiveDir(tmpOutDir, outFilePath, (processedBytes) => {
        progressSpinner.update({
          step: `Archiving files into a tarball, ${humanFileSize(processedBytes)} bytes written...`
        });
      });
    } catch (err) {
      throw progressSpinner.fail(), new Error(`Archiving backup failed: ${err.message}`);
    }
    progressSpinner.set({
      step: `Cleaning up temporary files at ${chalk.cyan(`${tmpOutDir}`)}`
    }), cleanupTmpDir(tmpOutDir), progressSpinner.set({
      step: `Backup download complete [${prettyMs__default.default(Date.now() - start)}]`
    }), progressSpinner.succeed();
  }
};
async function prepareBackupOptions(context, args) {
  const flags = await parseCliFlags$7(args), [dataset] = args.argsWithoutOptions, { prompt, workDir } = context, { projectId, datasetName, client } = await resolveApiClient(
    context,
    dataset,
    defaultApiVersion$1
  ), { token } = client.config();
  if (!isString__default.default(token) || token.length < 1)
    throw new Error("token is missing");
  if (!isString__default.default(datasetName) || datasetName.length < 1)
    throw new Error(`dataset ${datasetName} must be a valid dataset name`);
  const backupId = String(flags["backup-id"] || await chooseBackupIdPrompt(context, datasetName));
  if (backupId.length < 1)
    throw new Error(`backup-id ${flags["backup-id"]} should be a valid string`);
  if ("concurrency" in flags && (flags.concurrency < 1 || flags.concurrency > MAX_DOWNLOAD_CONCURRENCY))
    throw new Error(`concurrency should be in 1 to ${MAX_DOWNLOAD_CONCURRENCY} range`);
  const defaultOutFileName = `${datasetName}-backup-${backupId}.tar.gz`;
  let out = await (async () => flags.out !== void 0 ? fs$2.absolutify(flags.out) : await prompt.single({
    type: "input",
    message: "Output path:",
    default: path__default.default.join(workDir, defaultOutFileName),
    filter: fs$2.absolutify
  }))();
  return isPathDirName(out) && (out = path__default.default.join(out, defaultOutFileName)), !flags.overwrite && fs.existsSync(out) && (await prompt.single({
    type: "confirm",
    message: `File "${out}" already exists, would you like to overwrite it?`,
    default: !1
  }) || (out = "")), [
    client,
    {
      projectId,
      datasetName,
      backupId,
      token,
      outDir: path__default.default.dirname(out),
      outFileName: path__default.default.basename(out),
      overwrite: flags.overwrite,
      concurrency: flags.concurrency || DEFAULT_DOWNLOAD_CONCURRENCY
    }
  ];
}
const helpText$w = `
Examples
  sanity backup enable DATASET_NAME
`, enableDatasetBackupCommand = {
  name: "enable",
  group: "backup",
  signature: "[DATASET_NAME]",
  description: "Enable backup for a dataset.",
  helpText: helpText$w,
  action: async (args, context) => {
    const { output, chalk } = context, [dataset] = args.argsWithoutOptions, { projectId, datasetName, token, client } = await resolveApiClient(
      context,
      dataset,
      defaultApiVersion$1
    );
    try {
      await client.request({
        method: "PUT",
        headers: { Authorization: `Bearer ${token}` },
        uri: `/projects/${projectId}/datasets/${datasetName}/settings/backups`,
        body: {
          enabled: !0
        }
      }), output.print(
        `${chalk.green(
          `Enabled backups for dataset ${datasetName}.
Please note that it may take up to 24 hours before the first backup is created.
`
        )}`
      ), output.print(
        `${chalk.bold(`Retention policies may apply depending on your plan and agreement.
`)}`
      );
    } catch (error) {
      const { message } = parseApiErr(error);
      output.print(`${chalk.red(`Enabling dataset backup failed: ${message}`)}
`);
    }
  }
}, DEFAULT_LIST_BACKUP_LIMIT = 30, helpText$v = `
Options
  --limit <int>     Maximum number of backups returned. Default 30.
  --after <string>  Only return backups after this date (inclusive)
  --before <string> Only return backups before this date (exclusive). Cannot be younger than <after> if specified.

Examples
  sanity backup list DATASET_NAME
  sanity backup list DATASET_NAME --limit 50
  sanity backup list DATASET_NAME --after 2024-01-31 --limit 10
  sanity backup list DATASET_NAME --after 2024-01-31 --before 2024-01-10
`;
function parseCliFlags$6(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).options("after", { type: "string" }).options("before", { type: "string" }).options("limit", { type: "number", default: DEFAULT_LIST_BACKUP_LIMIT, alias: "l" }).argv;
}
const listDatasetBackupCommand = {
  name: "list",
  group: "backup",
  signature: "[DATASET_NAME]",
  description: "List available backups for a dataset.",
  helpText: helpText$v,
  action: async (args, context) => {
    const { output, chalk } = context, flags = await parseCliFlags$6(args), [dataset] = args.argsWithoutOptions, { projectId, datasetName, token, client } = await resolveApiClient(
      context,
      dataset,
      defaultApiVersion$1
    ), query = { limit: DEFAULT_LIST_BACKUP_LIMIT.toString() };
    if (flags.limit) {
      if (flags.limit < 1 || flags.limit > Number.MAX_SAFE_INTEGER)
        throw new Error(
          `Parsing --limit: must be an integer between 1 and ${Number.MAX_SAFE_INTEGER}`
        );
      query.limit = flags.limit.toString();
    }
    if (flags.before || flags.after)
      try {
        const parsedBefore = processDateFlags(flags.before), parsedAfter = processDateFlags(flags.after);
        if (parsedAfter && parsedBefore && dateFns.isAfter(parsedAfter, parsedBefore))
          throw new Error("--after date must be before --before");
        query.before = flags.before, query.after = flags.after;
      } catch (err) {
        throw new Error(`Parsing date flags: ${err}`);
      }
    let response;
    try {
      response = await client.request({
        headers: { Authorization: `Bearer ${token}` },
        uri: `/projects/${projectId}/datasets/${datasetName}/backups`,
        query: { ...query }
      });
    } catch (error) {
      const { message } = parseApiErr(error);
      output.error(`${chalk.red(`List dataset backup failed: ${message}`)}
`);
    }
    if (response && response.backups) {
      if (response.backups.length === 0) {
        output.print("No backups found.");
        return;
      }
      const table = new consoleTablePrinter.Table({
        columns: [
          { name: "resource", title: "RESOURCE", alignment: "left" },
          { name: "createdAt", title: "CREATED AT", alignment: "left" },
          { name: "backupId", title: "BACKUP ID", alignment: "left" }
        ]
      });
      response.backups.forEach((backup) => {
        const { id, createdAt } = backup;
        table.addRow({
          resource: "Dataset",
          createdAt: dateFns.lightFormat(Date.parse(createdAt), "yyyy-MM-dd HH:mm:ss"),
          backupId: id
        });
      }), table.printTable();
    }
  }
};
function processDateFlags(date) {
  if (!date) return;
  const parsedDate = dateFns.parse(date, "yyyy-MM-dd", /* @__PURE__ */ new Date());
  if (dateFns.isValid(parsedDate))
    return parsedDate;
  throw new Error(`Invalid ${date} date format. Use YYYY-MM-DD`);
}
const helpText$u = `
Options
  --source-maps Enable source maps for built bundles (increases size of bundle)
  --no-minify Skip minifying built JavaScript (speeds up build, increases size of bundle)
  -y, --yes Use unattended mode, accepting defaults and using only flags for choices

Examples
  sanity build
  sanity build --no-minify --source-maps
`, buildCommand = {
  name: "build",
  signature: "[OUTPUT_DIR]",
  description: "Builds the Sanity Studio configuration into a static bundle",
  action: async (args, context, overrides) => (await getBuildAction())(args, context, overrides),
  helpText: helpText$u
};
async function getBuildAction() {
  return (await Promise.resolve().then(function() {
    return require("./buildAction.js");
  })).default;
}
const checkCommand = {
  name: "check",
  signature: "",
  description: "[deprecated]",
  helpText: "",
  hideFromHelp: !0,
  action: (_args, context) => {
    const { output } = context;
    return output.print("`sanity check` is deprecated and no longer has any effect"), Promise.resolve();
  }
}, configCheckCommand = {
  name: "configcheck",
  signature: "",
  description: "Checks if the required configuration files for plugins exists and are up to date",
  helpText: "",
  hideFromHelp: !0,
  action: async (args, context) => (context.output.error("`sanity configcheck` is no longer required/used"), Promise.resolve())
}, wildcardReplacement = "a-wild-card-r3pl4c3m3n7-a", portReplacement = ":7777777";
async function addCorsOrigin(givenOrigin, flags, context) {
  const { apiClient, prompt, output } = context, origin = await (givenOrigin ? filterAndValidateOrigin(givenOrigin) : promptForOrigin$1(prompt)), hasWildcard = origin.includes("*");
  if (hasWildcard && !await promptForWildcardConfirmation(origin, context))
    return !1;
  const allowCredentials = typeof flags.credentials > "u" ? await promptForCredentials(hasWildcard, context) : !!flags.credentials;
  return givenOrigin !== origin && output.print(`Normalized origin to ${origin}`), await apiClient({
    requireUser: !0,
    requireProject: !0
  }).request({
    method: "POST",
    url: "/cors",
    body: { origin, allowCredentials },
    maxRedirects: 0
  }), !0;
}
function promptForCredentials(hasWildcard, context) {
  const { prompt, output, chalk } = context;
  return output.print(""), hasWildcard ? output.print(oneline__default.default`
      ${chalk.yellow(`${logSymbols__default.default.warning} Warning:`)}
      We ${chalk.red(chalk.underline("HIGHLY"))} recommend NOT allowing credentials
      on origins containing wildcards. If you are logged in to a studio, people will
      be able to send requests ${chalk.underline("on your behalf")} to read and modify
      data, from any matching origin. Please tread carefully!
    `) : output.print(oneline__default.default`
      ${chalk.yellow(`${logSymbols__default.default.warning} Warning:`)}
      Should this origin be allowed to send requests using authentication tokens or
      session cookies? Be aware that any script on this origin will be able to send
      requests ${chalk.underline("on your behalf")} to read and modify data if you
      are logged in to a Sanity studio. If this origin hosts a studio, you will need
      this, otherwise you should probably answer "No" (n).
    `), output.print(""), prompt.single({
    type: "confirm",
    message: oneline__default.default`
      Allow credentials to be sent from this origin? Please read the warning above.
    `,
    default: !1
  });
}
function promptForWildcardConfirmation(origin, context) {
  const { prompt, output, chalk } = context;
  return output.print(""), output.print(chalk.yellow(`${logSymbols__default.default.warning} Warning: Examples of allowed origins:`)), origin === "*" ? (output.print("- http://www.some-malicious.site"), output.print("- https://not.what-you-were-expecting.com"), output.print("- https://high-traffic-site.com"), output.print("- http://192.168.1.1:8080")) : (output.print(`- ${origin.replace(/:\*/, ":1234").replace(/\*/g, "foo")}`), output.print(`- ${origin.replace(/:\*/, ":3030").replace(/\*/g, "foo.bar")}`)), output.print(""), prompt.single({
    type: "confirm",
    message: oneline__default.default`
      Using wildcards can be ${chalk.red("risky")}.
      Are you ${chalk.underline("absolutely sure")} you want to allow this origin?`,
    default: !1
  });
}
function promptForOrigin$1(prompt) {
  return prompt.single({
    type: "input",
    message: "Origin (including protocol):",
    filter: filterOrigin,
    validate: (origin) => validateOrigin(origin, origin)
  });
}
function filterOrigin(origin) {
  if (origin === "*" || origin === "file:///*" || origin === "null")
    return origin;
  try {
    const example = origin.replace(/([^:])\*/g, `$1${wildcardReplacement}`).replace(/:\*/, portReplacement), parsed = url__default.default.parse(example);
    let host = parsed.host || "";
    return /^https?:$/.test(parsed.protocol || "") && (host = host.replace(/:(80|443)$/, "")), host = host.replace(portReplacement, ":*").replace(new RegExp(wildcardReplacement, "g"), "*"), `${parsed.protocol}//${host}`;
  } catch {
    return null;
  }
}
function validateOrigin(origin, givenOrigin) {
  if (origin === "*" || origin === "file:///*" || origin === "null")
    return !0;
  try {
    return url__default.default.parse(origin || 0), !0;
  } catch {
  }
  return /^file:\/\//.test(givenOrigin) ? "Only a local file wildcard is currently allowed: file:///*" : `Invalid origin "${givenOrigin}", must include protocol (https://some.host)`;
}
function filterAndValidateOrigin(givenOrigin) {
  const origin = filterOrigin(givenOrigin), result = validateOrigin(origin, givenOrigin);
  if (result !== !0)
    throw new Error(result);
  if (!origin)
    throw new Error("Invalid origin");
  return origin;
}
const helpText$t = `
Options
  --credentials Allow credentials (token/cookie) to be sent from this origin
  --no-credentials Disallow credentials (token/cookie) to be sent from this origin

Examples
  sanity cors add
  sanity cors add http://localhost:3000 --no-credentials
`, addCorsOriginCommand = {
  name: "add",
  group: "cors",
  signature: "[ORIGIN]",
  helpText: helpText$t,
  description: "Allow a new origin to use your project API through CORS",
  action: async (args, context) => {
    const { output } = context, [origin] = args.argsWithoutOptions;
    if (!origin)
      throw new Error("No origin specified, use `sanity cors add <origin-url>`");
    const flags = args.extOptions;
    fs__default.default.existsSync(path__default.default.join(process.cwd(), origin)) && output.warn(`Origin "${origin}?" Remember to quote values (sanity cors add "*")`), await addCorsOrigin(origin, flags, context) && output.print("CORS origin added successfully");
  }
}, corsGroup = {
  name: "cors",
  signature: "[COMMAND]",
  isGroupRoot: !0,
  description: "Configures CORS settings for Sanity projects"
}, helpText$s = `
Examples
  sanity cors delete
  sanity cors delete http://localhost:3000
`, deleteCorsOriginCommand = {
  name: "delete",
  group: "cors",
  signature: "[ORIGIN]",
  helpText: helpText$s,
  description: "Delete an existing CORS-origin from your project",
  action: async (args, context) => {
    const { output, apiClient } = context, [origin] = args.argsWithoutOptions, client = apiClient({ requireUser: !0, requireProject: !0 }), originId = await promptForOrigin(origin, context);
    try {
      await client.request({ method: "DELETE", uri: `/cors/${originId}` }), output.print("Origin deleted");
    } catch (err) {
      throw new Error(`Origin deletion failed:
${err.message}`);
    }
  }
};
async function promptForOrigin(specified, context) {
  const specifiedOrigin = specified && specified.toLowerCase(), { prompt, apiClient } = context, origins = await apiClient({ requireUser: !0, requireProject: !0 }).request({ url: "/cors" });
  if (specifiedOrigin) {
    const selected = origins.filter((origin) => origin.origin.toLowerCase() === specifiedOrigin)[0];
    if (!selected)
      throw new Error(`Origin "${specified} not found"`);
    return selected.id;
  }
  const choices = origins.map((origin) => ({ value: origin.id, name: origin.origin }));
  return prompt.single({
    message: "Select origin to delete",
    type: "list",
    choices
  });
}
const helpText$r = `
Examples
  sanity cors list
`, listCorsOriginsCommand = {
  name: "list",
  group: "cors",
  signature: "",
  helpText: helpText$r,
  description: "List all origins allowed to access the API for this project",
  action: async (args, context) => {
    const { output } = context, { apiClient } = context, origins = await apiClient({ requireUser: !0, requireProject: !0 }).request({ url: "/cors" });
    output.print(origins.map((origin) => origin.origin).join(`
`));
  }
};
function validateDatasetAliasName(datasetName) {
  if (!datasetName)
    return "Alias name is missing";
  const name = `${datasetName}`;
  return name.toLowerCase() !== name ? "Alias name must be all lowercase characters" : name.length < 2 ? "Alias name must be at least two characters long" : name.length > 64 ? "Alias name must be at most 64 characters" : /^[a-z0-9~]/.test(name) ? /^[a-z0-9~][-_a-z0-9]+$/.test(name) ? /[-_]$/.test(name) ? "Alias name must not end with a dash or an underscore" : !1 : "Alias name must only contain letters, numbers, dashes and underscores" : "Alias name must start with a letter or a number";
}
function promptForDatasetAliasName(prompt, options = {}) {
  return prompt.single({
    type: "input",
    message: "Alias name:",
    validate: (name) => validateDatasetAliasName(name) || !0,
    ...options
  });
}
const ALIAS_PREFIX = "~";
function listAliases(client) {
  return client.request({ uri: "/aliases" });
}
function createAlias(client, aliasName, datasetName) {
  return modify(client, "PUT", aliasName, datasetName ? { datasetName } : void 0);
}
function updateAlias(client, aliasName, datasetName) {
  return modify(client, "PATCH", aliasName, datasetName ? { datasetName } : void 0);
}
function unlinkAlias(client, aliasName) {
  return modify(client, "PATCH", `${aliasName}/unlink`, {});
}
function removeAlias(client, aliasName) {
  return modify(client, "DELETE", aliasName);
}
function modify(client, method, aliasName, body) {
  return client.request({ method, uri: `/aliases/${aliasName}`, body });
}
const createAliasHandler = async (args, context) => {
  const { apiClient, output, prompt } = context, [, alias, targetDataset] = args.argsWithoutOptions, client = apiClient(), nameError = alias && validateDatasetAliasName(alias);
  if (nameError)
    throw new Error(nameError);
  const [datasets, aliases, projectFeatures] = await Promise.all([
    client.datasets.list().then((sets) => sets.map((ds) => ds.name)),
    listAliases(client).then((sets) => sets.map((ds) => ds.name)),
    client.request({ uri: "/features" })
  ]);
  let aliasName = await (alias || promptForDatasetAliasName(prompt)), aliasOutputName = aliasName;
  if (aliasName.startsWith(ALIAS_PREFIX) ? aliasName = aliasName.slice(1) : aliasOutputName = `${ALIAS_PREFIX}${aliasName}`, aliases.includes(aliasName))
    throw new Error(`Dataset alias "${aliasOutputName}" already exists`);
  if (targetDataset) {
    const datasetErr = validateDatasetName(targetDataset);
    if (datasetErr)
      throw new Error(datasetErr);
  }
  const datasetName = await (targetDataset || promptForDatasetName(prompt));
  if (datasetName && !datasets.includes(datasetName))
    throw new Error(`Dataset "${datasetName}" does not exist `);
  if (!projectFeatures.includes("advancedDatasetManagement"))
    throw new Error("This project cannot create a dataset alias");
  try {
    await createAlias(client, aliasName, datasetName), output.print(
      `Dataset alias ${aliasOutputName} created ${datasetName && `and linked to ${datasetName}`} successfully`
    );
  } catch (err) {
    throw new Error(`Dataset alias creation failed:
${err.message}`);
  }
};
function parseCliFlags$5(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).option("force", { type: "boolean" }).argv;
}
const deleteAliasHandler = async (args, context) => {
  const { apiClient, prompt, output } = context, [, ds] = args.argsWithoutOptions, { force } = await parseCliFlags$5(args), client = apiClient();
  if (!ds)
    throw new Error("Dataset alias name must be provided");
  let aliasName = `${ds}`;
  const dsError = validateDatasetAliasName(aliasName);
  if (dsError)
    throw dsError;
  aliasName = aliasName.startsWith(ALIAS_PREFIX) ? aliasName.slice(1) : aliasName;
  const [fetchedAliases] = await Promise.all([listAliases(client)]), linkedAlias = fetchedAliases.find((elem) => elem.name === aliasName), message = linkedAlias && linkedAlias.datasetName ? `This dataset alias is linked to ${linkedAlias.datasetName}. ` : "";
  return force ? output.warn(`'--force' used: skipping confirmation, deleting alias "${aliasName}"`) : await prompt.single({
    type: "input",
    message: `${message}Are you ABSOLUTELY sure you want to delete this dataset alias?
  Type the name of the dataset alias to confirm delete: `,
    filter: (input) => `${input}`.trim(),
    validate: (input) => input === aliasName || "Incorrect dataset alias name. Ctrl + C to cancel delete."
  }), removeAlias(client, aliasName).then(() => {
    output.print("Dataset alias deleted successfully");
  });
}, linkAliasHandler = async (args, context) => {
  const { apiClient, output, prompt } = context, [, alias, targetDataset] = args.argsWithoutOptions, flags = args.extOptions, client = apiClient(), nameError = alias && validateDatasetAliasName(alias);
  if (nameError)
    throw new Error(nameError);
  const [datasets, fetchedAliases] = await Promise.all([
    client.datasets.list().then((sets) => sets.map((ds) => ds.name)),
    listAliases(client)
  ]), aliases = fetchedAliases.map((da) => da.name);
  let aliasName = await (alias || promptForDatasetAliasName(prompt)), aliasOutputName = aliasName;
  if (aliasName.startsWith(ALIAS_PREFIX) ? aliasName = aliasName.slice(1) : aliasOutputName = `${ALIAS_PREFIX}${aliasName}`, !aliases.includes(aliasName))
    throw new Error(`Dataset alias "${aliasOutputName}" does not exist `);
  const datasetName = await (targetDataset || promptForDatasetName(prompt)), datasetErr = validateDatasetName(datasetName);
  if (datasetErr)
    throw new Error(datasetErr);
  if (!datasets.includes(datasetName))
    throw new Error(`Dataset "${datasetName}" does not exist `);
  const linkedAlias = fetchedAliases.find((elem) => elem.name === aliasName);
  if (linkedAlias && linkedAlias.datasetName) {
    if (linkedAlias.datasetName === datasetName)
      throw new Error(`Dataset alias ${aliasOutputName} already linked to ${datasetName}`);
    flags.force || await prompt.single({
      type: "input",
      message: `This alias is linked to dataset <${linkedAlias.datasetName}>. Are you ABSOLUTELY sure you want to link this dataset alias to this dataset?
        
  Type YES/NO: `,
      filter: (input) => `${input}`.toLowerCase(),
      validate: (input) => input === "yes" || "Ctrl + C to cancel dataset alias link."
    });
  }
  try {
    await updateAlias(client, aliasName, datasetName), output.print(`Dataset alias ${aliasOutputName} linked to ${datasetName} successfully`);
  } catch (err) {
    throw new Error(`Dataset alias link failed:
${err.message}`);
  }
};
function parseCliFlags$4(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).option("force", { type: "boolean" }).argv;
}
const unlinkAliasHandler = async (args, context) => {
  const { apiClient, output, prompt } = context, [, alias] = args.argsWithoutOptions, { force } = await parseCliFlags$4(args), client = apiClient(), nameError = alias && validateDatasetAliasName(alias);
  if (nameError)
    throw new Error(nameError);
  const fetchedAliases = await listAliases(client);
  let aliasName = await (alias || promptForDatasetAliasName(prompt)), aliasOutputName = aliasName;
  aliasName.startsWith(ALIAS_PREFIX) ? aliasName = aliasName.slice(1) : aliasOutputName = `${ALIAS_PREFIX}${aliasName}`;
  const linkedAlias = fetchedAliases.find((elem) => elem.name === aliasName);
  if (!linkedAlias)
    throw new Error(`Dataset alias "${aliasOutputName}" does not exist`);
  if (!linkedAlias.datasetName)
    throw new Error(`Dataset alias "${aliasOutputName}" is not linked to a dataset`);
  force ? output.warn(`'--force' used: skipping confirmation, unlinking alias "${aliasOutputName}"`) : await prompt.single({
    type: "input",
    message: `Are you ABSOLUTELY sure you want to unlink this alias from the "${linkedAlias.datasetName}" dataset?
        
  Type YES/NO: `,
    filter: (input) => `${input}`.toLowerCase(),
    validate: (input) => input === "yes" || "Ctrl + C to cancel dataset alias unlink."
  });
  try {
    const result = await unlinkAlias(client, aliasName);
    output.print(
      `Dataset alias ${aliasOutputName} unlinked from ${result.datasetName} successfully`
    );
  } catch (err) {
    throw new Error(`Dataset alias unlink failed:
${err.message}`);
  }
}, helpText$q = `
Below are examples of the alias subcommand

Create Alias
  sanity dataset alias create
  sanity dataset alias create <alias-name>
  sanity dataset alias create <alias-name> <target-dataset>

Delete Alias
  Options
    --force Skips security prompt and forces link command

  Usage
    sanity dataset alias delete <alias-name>
    sanity dataset alias delete <alias-name> --force

Link Alias
  Options
    --force Skips security prompt and forces link command

  Usage
    sanity dataset alias link
    sanity dataset alias link <alias-name>
    sanity dataset alias link <alias-name> <target-dataset>
    sanity dataset alias link <alias-name> <target-dataset> --force

Un-link Alias
  Options
    --force Skips security prompt and forces link command

  Usage
    sanity dataset alias unlink
    sanity dataset alias unlink <alias-name>
    sanity dataset alias unlink <alias-name> --force
`, aliasCommand = {
  name: "alias",
  group: "dataset",
  signature: "SUBCOMMAND [ALIAS_NAME, TARGET_DATASET]",
  helpText: helpText$q,
  description: "You can manage your dataset alias using this command.",
  action: async (args, context) => {
    const [verb] = args.argsWithoutOptions;
    switch (verb) {
      case "create":
        await createAliasHandler(args, context);
        break;
      case "delete":
        await deleteAliasHandler(args, context);
        break;
      case "unlink":
        await unlinkAliasHandler(args, context);
        break;
      case "link":
        await linkAliasHandler(args, context);
        break;
      default:
        throw new Error(oneline__default.default`
          Invalid command provided. Available commands are: create, delete, link and unlink.
          For more guide run the help command 'sanity dataset alias --help'
        `);
    }
  }
};
async function listDatasetCopyJobs(flags, context) {
  const { apiClient, output, chalk } = context, client = apiClient(), projectId = client.config().projectId, query = {};
  let response;
  flags.offset && flags.offset >= 0 && (query.offset = `${flags.offset}`), flags.limit && flags.limit > 0 && (query.limit = `${flags.limit}`);
  try {
    response = await client.request({
      method: "GET",
      uri: `/projects/${projectId}/datasets/copy`,
      query
    });
  } catch (error) {
    error.statusCode ? output.error(`${chalk.red(`Dataset copy list failed:
${error.response.body.message}`)}
`) : output.error(`${chalk.red(`Dataset copy list failed:
${error.message}`)}
`);
  }
  if (response && response.length > 0) {
    const table = new consoleTablePrinter.Table({
      title: "Dataset copy jobs for this project in descending order",
      columns: [
        { name: "id", title: "Job ID", alignment: "left" },
        { name: "sourceDataset", title: "Source Dataset", alignment: "left" },
        { name: "targetDataset", title: "Target Dataset", alignment: "left" },
        { name: "state", title: "State", alignment: "left" },
        { name: "withHistory", title: "With history", alignment: "left" },
        { name: "timeStarted", title: "Time started", alignment: "left" },
        { name: "timeTaken", title: "Time taken", alignment: "left" }
      ]
    });
    response.forEach((job) => {
      const { id, state, createdAt, updatedAt, sourceDataset, targetDataset, withHistory } = job;
      let timeStarted = "";
      createdAt !== "" && (timeStarted = dateFns.formatDistanceToNow(dateFns.parseISO(createdAt)));
      let timeTaken = "";
      updatedAt !== "" && (timeTaken = dateFns.formatDistance(dateFns.parseISO(updatedAt), dateFns.parseISO(createdAt)));
      let color;
      switch (state) {
        case "completed":
          color = "green";
          break;
        case "failed":
          color = "red";
          break;
        case "pending":
          color = "yellow";
          break;
        default:
          color = "";
      }
      table.addRow(
        {
          id,
          state,
          withHistory,
          timeStarted: `${timeStarted} ago`,
          timeTaken,
          sourceDataset,
          targetDataset
        },
        { color }
      );
    }), table.printTable();
  } else
    output.print("This project doesn't have any dataset copy jobs");
}
const getClientUrl = (client, uri, useCdn = !1) => {
  const config = client.config();
  return `${useCdn ? config.cdnUrl : config.url}/${uri.replace(/^\//, "")}`;
}, helpText$p = `
Options
  --detach Start the copy without waiting for it to finish
  --attach <job-id> Attach to the running copy process to show progress
  --skip-history Don't preserve document history on copy
  --list Lists all dataset copy jobs corresponding to a certain criteria.
  --offset Start position in the list of jobs. Default 0. With --list.
  --limit Maximum number of jobs returned. Default 10. Maximum 1000. With --list.

Examples
  sanity dataset copy
  sanity dataset copy <source-dataset>
  sanity dataset copy <source-dataset> <target-dataset>
  sanity dataset copy --skip-history <source-dataset> <target-dataset>
  sanity dataset copy --detach <source-dataset> <target-dataset>
  sanity dataset copy --attach <job-id>
  sanity dataset copy --list
  sanity dataset copy --list --offset=2
  sanity dataset copy --list --offset=2 --limit=10
`;
function parseCliFlags$3(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).option("attach", { type: "string" }).option("list", { type: "boolean" }).option("limit", { type: "number" }).option("offset", { type: "number" }).option("skip-history", { type: "boolean" }).option("detach", { type: "boolean" }).argv;
}
const progress = (url2) => new rxjs.Observable((observer) => {
  let progressSource = new EventSource__default.default(url2), stopped = !1;
  function onError(error) {
    progressSource && progressSource.close(), debug$2(`Error received: ${error}`), !stopped && (observer.next({ type: "reconnect" }), progressSource = new EventSource__default.default(url2));
  }
  function onChannelError(error) {
    stopped = !0, progressSource.close(), observer.error(error);
  }
  function onMessage(event) {
    const data = JSON.parse(event.data);
    data.state === "failed" ? (debug$2("Job failed. Data: %o", event), observer.error(event)) : data.state === "completed" ? (debug$2("Job succeeded. Data: %o", event), onComplete()) : (debug$2("Job progressed. Data: %o", event), observer.next(data));
  }
  function onComplete() {
    progressSource.removeEventListener("error", onError), progressSource.removeEventListener("channel_error", onChannelError), progressSource.removeEventListener("job", onMessage), progressSource.removeEventListener("done", onComplete), progressSource.close(), observer.complete();
  }
  progressSource.addEventListener("error", onError), progressSource.addEventListener("channel_error", onChannelError), progressSource.addEventListener("job", onMessage), progressSource.addEventListener("done", onComplete);
}), followProgress = (jobId, client, output) => {
  let currentProgress = 0;
  const spinner = output.spinner({}).start(), listenUrl = getClientUrl(client, `jobs/${jobId}/listen`);
  return debug$2(`Listening to ${listenUrl}`), new Promise((resolve, reject) => {
    progress(listenUrl).subscribe({
      next: (event) => {
        typeof event.progress == "number" && (currentProgress = event.progress), spinner.text = `Copy in progress: ${currentProgress}%`;
      },
      error: (err) => {
        spinner.fail(), reject(new Error(`${err.data}`));
      },
      complete: () => {
        spinner.succeed("Copy finished."), resolve();
      }
    });
  });
}, copyDatasetCommand = {
  name: "copy",
  group: "dataset",
  signature: "[SOURCE_DATASET] [TARGET_DATASET]",
  helpText: helpText$p,
  description: "Manages dataset copying, including starting a new copy job, listing copy jobs and following the progress of a running copy job",
  action: async (args, context) => {
    const { apiClient, output, prompt, chalk } = context, flags = await parseCliFlags$3(args), client = apiClient();
    if (flags.list) {
      await listDatasetCopyJobs(flags, context);
      return;
    }
    if (flags.attach) {
      const jobId = flags.attach;
      if (!jobId)
        throw new Error("Please supply a jobId");
      await followProgress(jobId, client, output);
      return;
    }
    const [sourceDataset, targetDataset] = args.argsWithoutOptions, shouldSkipHistory = !!flags["skip-history"], nameError = sourceDataset && validateDatasetName(sourceDataset);
    if (nameError)
      throw new Error(nameError);
    const existingDatasets = await client.datasets.list().then((datasets) => datasets.map((ds) => ds.name)), sourceDatasetName = await (sourceDataset || promptForDatasetName(prompt, { message: "Source dataset name:" }));
    if (!existingDatasets.includes(sourceDatasetName))
      throw new Error(`Source dataset "${sourceDatasetName}" doesn't exist`);
    const targetDatasetName = await (targetDataset || promptForDatasetName(prompt, { message: "Target dataset name:" }));
    if (existingDatasets.includes(targetDatasetName))
      throw new Error(`Target dataset "${targetDatasetName}" already exists`);
    const err = validateDatasetName(targetDatasetName);
    if (err)
      throw new Error(err);
    try {
      const response = await client.request({
        method: "PUT",
        uri: `/datasets/${sourceDatasetName}/copy`,
        body: {
          targetDataset: targetDatasetName,
          skipHistory: shouldSkipHistory
        }
      });
      if (output.print(
        `Copying dataset ${chalk.green(sourceDatasetName)} to ${chalk.green(targetDatasetName)}...`
      ), shouldSkipHistory || output.print(
        "Note: You can run this command with flag '--skip-history'. The flag will reduce copy time in larger datasets."
      ), output.print(`Job ${chalk.green(response.jobId)} started`), flags.detach)
        return;
      await followProgress(response.jobId, client, output), output.print(`Job ${chalk.green(response.jobId)} completed`);
    } catch (error) {
      error.statusCode ? output.print(`${chalk.red(`Dataset copying failed:
${error.response.body.message}`)}
`) : output.print(`${chalk.red(`Dataset copying failed:
${error.message}`)}
`);
    }
  }
}, helpText$o = `
Options
  --visibility <mode> Set visibility for this dataset (public/private)

Examples
  sanity dataset create
  sanity dataset create <name>
  sanity dataset create <name> --visibility private
`, allowedModes = ["private", "public", "custom"], createDatasetCommand = {
  name: "create",
  group: "dataset",
  signature: "[NAME]",
  helpText: helpText$o,
  description: "Create a new dataset within your project",
  action: async (args, context) => {
    const { apiClient, output, prompt } = context, flags = args.extOptions, [dataset] = args.argsWithoutOptions, client = apiClient(), nameError = dataset && validateDatasetName(dataset);
    if (nameError)
      throw new Error(nameError);
    const [datasets, projectFeatures] = await Promise.all([
      client.datasets.list().then((sets) => sets.map((ds) => ds.name)),
      client.request({ uri: "/features" })
    ]);
    if (flags.visibility && !allowedModes.includes(flags.visibility))
      throw new Error(`Visibility mode "${flags.visibility}" not allowed`);
    const datasetName = await (dataset || promptForDatasetName(prompt));
    if (datasets.includes(datasetName))
      throw new Error(`Dataset "${datasetName}" already exists`);
    const canCreatePrivate = projectFeatures.includes("privateDataset");
    debug$2("%s create private datasets", canCreatePrivate ? "Can" : "Cannot");
    const aclMode = await ((canCreatePrivate ? flags.visibility : "public") || promptForDatasetVisibility(prompt, output));
    try {
      await client.datasets.create(datasetName, { aclMode }), output.print("Dataset created successfully");
    } catch (err) {
      throw new Error(`Dataset creation failed:
${err.message}`);
    }
  }
};
async function promptForDatasetVisibility(prompt, output) {
  const mode = await prompt.single({
    type: "list",
    message: "Dataset visibility",
    choices: [
      {
        value: "public",
        name: "Public (world readable)"
      },
      {
        value: "private",
        name: "Private (Authenticated user or token needed)"
      }
    ]
  });
  return mode === "private" && output.print(
    `Please note that while documents are private, assets (files and images) are still public
`
  ), mode;
}
var datasetGroup = {
  name: "dataset",
  signature: "[COMMAND]",
  isGroupRoot: !0,
  description: "Manages datasets, like create or delete, within projects"
};
const datasetVisibilityCommand = {
  name: "visibility",
  group: "dataset",
  helpText: "",
  signature: "get/set [dataset] [mode]",
  description: "Set visibility of a dataset",
  action: async (args, context) => {
    const { apiClient, output } = context, [action, ds, aclMode] = args.argsWithoutOptions, client = apiClient();
    if (!client.datasets.edit)
      throw new Error(`@sanity/cli must be upgraded first:
  npm install -g @sanity/cli`);
    if (!action)
      throw new Error("Action must be provided (get/set)");
    if (!["set", "get"].includes(action))
      throw new Error("Invalid action (only get/set allowed)");
    if (!ds)
      throw new Error("Dataset name must be provided");
    if (action === "set" && !aclMode)
      throw new Error("Please provide a visibility mode (public/private)");
    const dataset = `${ds}`, dsError = validateDatasetName(dataset);
    if (dsError)
      throw new Error(dsError);
    const current = (await client.datasets.list()).find((curr) => curr.name === dataset);
    if (!current)
      throw new Error("Dataset not found");
    if (action === "get") {
      output.print(current.aclMode);
      return;
    }
    if (current.aclMode === aclMode) {
      output.print(`Dataset already in "${aclMode}"-mode`);
      return;
    }
    aclMode === "private" && output.print(
      `Please note that while documents are private, assets (files and images) are still public
`
    ), await client.datasets.edit(dataset, { aclMode }), output.print("Dataset visibility changed");
  }
}, helpText$n = `
Options
  --force Do not prompt for delete confirmation - forcefully delete

Examples
  sanity dataset delete
  sanity dataset delete my-dataset
  sanity dataset delete my-dataset --force
`;
function parseCliFlags$2(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).option("force", { type: "boolean" }).argv;
}
const deleteDatasetCommand = {
  name: "delete",
  group: "dataset",
  helpText: helpText$n,
  signature: "[datasetName]",
  description: "Delete a dataset within your project",
  action: async (args, context) => {
    const { apiClient, prompt, output } = context, { force } = await parseCliFlags$2(args), [ds] = args.argsWithoutOptions;
    if (!ds)
      throw new Error("Dataset name must be provided");
    const dataset = `${ds}`, dsError = validateDatasetName(dataset);
    if (dsError)
      throw dsError;
    force ? output.warn(`'--force' used: skipping confirmation, deleting dataset "${dataset}"`) : await prompt.single({
      type: "input",
      message: `Are you ABSOLUTELY sure you want to delete this dataset?
  Type the name of the dataset to confirm delete:`,
      filter: (input) => `${input}`.trim(),
      validate: (input) => input === dataset || "Incorrect dataset name. Ctrl + C to cancel delete."
    }), await apiClient().datasets.delete(dataset), output.print("Dataset deleted successfully");
  }
}, noop = () => null, helpText$m = `
Options
  --raw                     Extract only documents, without rewriting asset references
  --no-assets               Export only non-asset documents and remove references to image assets
  --no-drafts               Export only published versions of documents
  --no-compress             Skips compressing tarball entries (still generates a gzip file)
  --types                   Defines which document types to export
  --overwrite               Overwrite any file with the same name
  --asset-concurrency <num> Concurrent number of asset downloads

Examples
  sanity dataset export moviedb localPath.tar.gz
  sanity dataset export moviedb assetless.tar.gz --no-assets
  sanity dataset export staging staging.tar.gz --raw
  sanity dataset export staging staging.tar.gz --types products,shops
`;
function parseFlags$1(rawFlags) {
  const flags = {};
  return rawFlags.types && (flags.types = `${rawFlags.types}`.split(",")), rawFlags["asset-concurrency"] && (flags.assetConcurrency = parseInt(rawFlags["asset-concurrency"], 10)), typeof rawFlags.raw < "u" && (flags.raw = !!rawFlags.raw), typeof rawFlags.assets < "u" && (flags.assets = !!rawFlags.assets), typeof rawFlags.drafts < "u" && (flags.drafts = !!rawFlags.drafts), typeof rawFlags.compress < "u" && (flags.compress = !!rawFlags.compress), typeof rawFlags.overwrite < "u" && (flags.overwrite = !!rawFlags.overwrite), flags;
}
const exportDatasetCommand = {
  name: "export",
  group: "dataset",
  signature: "[NAME] [DESTINATION]",
  description: "Export dataset to local filesystem as a gzipped tarball",
  helpText: helpText$m,
  action: async (args, context) => {
    const { apiClient, output, chalk, workDir, prompt } = context, client = apiClient(), [targetDataset, targetDestination] = args.argsWithoutOptions, flags = parseFlags$1(args.extOptions);
    let dataset = targetDataset ? `${targetDataset}` : null;
    dataset || (dataset = await chooseDatasetPrompt(context, { message: "Select dataset to export" }));
    const dsError = validateDatasetName(dataset);
    if (dsError)
      throw dsError;
    if (!(await client.datasets.list()).find((set) => set.name === dataset))
      throw new Error(`Dataset with name "${dataset}" not found`);
    const { projectId } = client.config();
    output.print("\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E"), output.print("\u2502                                               \u2502"), output.print("\u2502 Exporting from:                               \u2502"), output.print(`\u2502 ${chalk.bold("projectId")}: ${chalk.cyan(projectId).padEnd(44)} \u2502`), output.print(`\u2502 ${chalk.bold("dataset")}: ${chalk.cyan(dataset).padEnd(46)} \u2502`), output.print("\u2502                                               \u2502"), output.print("\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F"), output.print("");
    let destinationPath = targetDestination;
    destinationPath || (destinationPath = await prompt.single({
      type: "input",
      message: "Output path:",
      default: path__default.default.join(workDir, `${dataset}.tar.gz`),
      filter: fs$2.absolutify
    }));
    const outputPath = await getOutputPath(destinationPath, dataset, prompt, flags);
    if (!outputPath) {
      output.print("Cancelled");
      return;
    }
    outputPath !== "-" && output.print(`Exporting dataset "${chalk.cyan(dataset)}" to "${chalk.cyan(outputPath)}"`);
    let currentStep = "Exporting documents...", spinner = output.spinner(currentStep).start();
    const onProgress = (progress2) => {
      progress2.step !== currentStep ? (spinner.succeed(), spinner = output.spinner(progress2.step).start()) : progress2.step === currentStep && progress2.update && (spinner.text = `${progress2.step} (${progress2.current}/${progress2.total})`), currentStep = progress2.step;
    }, start = Date.now();
    try {
      await exportDataset__default.default({
        client,
        dataset,
        outputPath,
        onProgress,
        ...flags
      }), spinner.succeed();
    } catch (err) {
      throw spinner.fail(), err;
    }
    output.print(`Export finished (${prettyMs__default.default(Date.now() - start)})`);
  }
};
async function getOutputPath(destination, dataset, prompt, flags) {
  if (destination === "-")
    return "-";
  const dstPath = path__default.default.isAbsolute(destination) ? destination : path__default.default.resolve(process.cwd(), destination);
  let dstStats = await fs__default$1.default.stat(dstPath).catch(noop);
  const looksLikeFile = dstStats ? dstStats.isFile() : path__default.default.basename(dstPath).indexOf(".") !== -1;
  if (!dstStats) {
    const createPath = looksLikeFile ? path__default.default.dirname(dstPath) : dstPath;
    await fs__default$1.default.mkdir(createPath, { recursive: !0 });
  }
  const finalPath = looksLikeFile ? dstPath : path__default.default.join(dstPath, `${dataset}.tar.gz`);
  return dstStats = await fs__default$1.default.stat(finalPath).catch(noop), !flags.overwrite && dstStats && dstStats.isFile() && !await prompt.single({
    type: "confirm",
    message: `File "${finalPath}" already exists, would you like to overwrite it?`,
    default: !1
  }) ? !1 : finalPath;
}
const yellow = (str) => `\x1B[33m${str}\x1B[39m`, helpText$l = `
Options
  --missing On duplicate document IDs, skip importing document in question
  --replace On duplicate document IDs, replace existing document with imported document
  --allow-failing-assets Skip assets that cannot be fetched/uploaded
  --replace-assets Skip reuse of existing assets
  --skip-cross-dataset-references Skips references to other datasets

Rarely used options (should generally not be used)
  --allow-assets-in-different-dataset Allow asset documents to reference different project/dataset
  --allow-system-documents Allow system documents like dataset permissions and custom retention to be imported

Examples
  # Import "moviedb.ndjson" from the current directory to the dataset called "moviedb"
  sanity dataset import moviedb.ndjson moviedb

  # Import "moviedb.tar.gz" from the current directory to the dataset called "moviedb",
  # replacing any documents encountered that have the same document IDs
  sanity dataset import moviedb.tar.gz moviedb --replace

  # Import from a folder containing an ndjson file, such as an extracted tarball
  # retrieved through "sanity dataset export".
  sanity dataset import ~/some/folder moviedb

  # Import from a remote URL. Will download and extract the tarball to a temporary
  # location before importing it.
  sanity dataset import https://some.url/moviedb.tar.gz moviedb --replace
`;
function toBoolIfSet(flag) {
  return typeof flag > "u" ? void 0 : !!flag;
}
function parseFlags(rawFlags) {
  const allowAssetsInDifferentDataset = toBoolIfSet(rawFlags["allow-assets-in-different-dataset"]), allowFailingAssets = toBoolIfSet(rawFlags["allow-failing-assets"]), assetConcurrency = toBoolIfSet(rawFlags["asset-concurrency"]), replaceAssets = toBoolIfSet(rawFlags["replace-assets"]), skipCrossDatasetReferences = toBoolIfSet(rawFlags["skip-cross-dataset-references"]), allowSystemDocuments = toBoolIfSet(rawFlags["allow-system-documents"]), replace = toBoolIfSet(rawFlags.replace), missing = toBoolIfSet(rawFlags.missing);
  return {
    allowAssetsInDifferentDataset,
    allowFailingAssets,
    assetConcurrency,
    skipCrossDatasetReferences,
    allowSystemDocuments,
    replaceAssets,
    replace,
    missing
  };
}
const importDatasetCommand = {
  name: "import",
  group: "dataset",
  signature: "[FILE | FOLDER | URL] [TARGET_DATASET]",
  description: "Import documents to given dataset from either an ndjson file or a gzipped tarball",
  helpText: helpText$l,
  // eslint-disable-next-line max-statements
  action: async (args, context) => {
    const { apiClient, output, chalk, fromInitCommand } = context, flags = parseFlags(args.extOptions), {
      allowAssetsInDifferentDataset,
      allowFailingAssets,
      assetConcurrency,
      skipCrossDatasetReferences,
      allowSystemDocuments,
      replaceAssets
    } = flags, operation = getMutationOperation(args.extOptions), client = apiClient(), [file, target] = args.argsWithoutOptions;
    if (!file)
      throw new Error(
        `Source file name and target dataset must be specified ("sanity dataset import ${chalk.bold(
          "[file]"
        )} [dataset]")`
      );
    const targetDataset = await determineTargetDataset(target, context);
    debug$2(`Target dataset has been set to "${targetDataset}"`);
    const isUrl = /^https?:\/\//i.test(file);
    let inputStream, assetsBase, sourceIsFolder = !1;
    if (isUrl)
      debug$2("Input is a URL, streaming from source URL"), inputStream = await getUrlStream(file);
    else {
      const sourceFile = path__default.default.resolve(process.cwd(), file), fileStats = await fs__default$1.default.stat(sourceFile).catch(() => null);
      if (!fileStats)
        throw new Error(`${sourceFile} does not exist or is not readable`);
      sourceIsFolder = fileStats.isDirectory(), sourceIsFolder ? inputStream = sourceFile : (assetsBase = path__default.default.dirname(sourceFile), inputStream = await fs.createReadStream(sourceFile));
    }
    const importClient = client.clone().config({ dataset: targetDataset }), { projectId, dataset } = importClient.config();
    output.print("\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E"), output.print("\u2502                                               \u2502"), output.print("\u2502 Importing to:                                 \u2502"), output.print(`\u2502 ${chalk.bold("projectId")}: ${chalk.cyan(projectId).padEnd(44)} \u2502`), output.print(`\u2502 ${chalk.bold("dataset")}: ${chalk.cyan(dataset).padEnd(46)} \u2502`), output.print("\u2502                                               \u2502"), output.print("\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F"), output.print("");
    let currentStep, currentProgress, stepStart, spinInterval = null, percent;
    function onProgress(opts) {
      const lengthComputable = opts.total, sameStep = opts.step == currentStep;
      if (percent = getPercentage(opts), lengthComputable && opts.total === opts.current && (spinInterval && clearInterval(spinInterval), spinInterval = null), sameStep)
        return;
      const prevStep = currentStep, prevStepStart = stepStart || Date.now();
      if (stepStart = Date.now(), currentStep = opts.step, currentProgress && currentProgress.succeed) {
        const timeSpent = prettyMs__default.default(Date.now() - prevStepStart, {
          secondsDecimalDigits: 2
        });
        currentProgress.text = `[100%] ${prevStep} (${timeSpent})`, currentProgress.succeed();
      }
      currentProgress = output.spinner(`[0%] ${opts.step} (0.00s)`).start(), spinInterval && (clearInterval(spinInterval), spinInterval = null), spinInterval = setInterval(() => {
        const timeSpent = prettyMs__default.default(Date.now() - prevStepStart, {
          secondsDecimalDigits: 2
        });
        currentProgress && (currentProgress.text = `${percent}${opts.step} (${timeSpent})`);
      }, 60);
    }
    function endTask({ success }) {
      if (spinInterval && clearInterval(spinInterval), spinInterval = null, success && stepStart && currentProgress) {
        const timeSpent = prettyMs__default.default(Date.now() - stepStart, {
          secondsDecimalDigits: 2
        });
        currentProgress.text = `[100%] ${currentStep} (${timeSpent})`, currentProgress.succeed();
      } else currentProgress && currentProgress.fail();
    }
    try {
      const { numDocs, warnings } = await sanityImport__default.default(inputStream, {
        client: importClient,
        assetsBase,
        operation,
        onProgress,
        allowFailingAssets,
        allowAssetsInDifferentDataset,
        skipCrossDatasetReferences,
        allowSystemDocuments,
        assetConcurrency,
        replaceAssets
      });
      endTask({ success: !0 }), output.print(`Done! Imported %d documents to dataset "%s"
`, numDocs, targetDataset), printWarnings(warnings, output);
    } catch (err) {
      if (endTask({ success: !1 }), !(!fromInitCommand && err.response && err.response.statusCode === 409 && err.step !== "strengthen-references"))
        throw err;
      const message = [
        err.message,
        "",
        "You probably want either:",
        " --replace (replace existing documents with same IDs)",
        " --missing (only import documents that do not already exist)",
        ""
      ].join(`
`), error = new Error(message);
      throw error.details = err.details, error.response = err.response, error.responseBody = err.responseBody, error;
    }
  }
};
async function determineTargetDataset(target, context) {
  const { apiClient, output, prompt } = context, client = apiClient();
  if (target) {
    const dsError = validateDatasetName(target);
    if (dsError)
      throw new Error(dsError);
  }
  debug$2("Fetching available datasets");
  const spinner = output.spinner("Fetching available datasets").start(), datasets = await client.datasets.list();
  spinner.succeed("[100%] Fetching available datasets");
  let targetDataset = target ? `${target}` : null;
  if (!targetDataset)
    targetDataset = await chooseDatasetPrompt(context, {
      message: "Select target dataset",
      allowCreation: !0
    });
  else if (!datasets.find((dataset) => dataset.name === targetDataset)) {
    if (debug$2("Target dataset does not exist, prompting for creation"), !await prompt.single({
      type: "confirm",
      message: `Dataset "${targetDataset}" does not exist, would you like to create it?`,
      default: !0
    }))
      throw new Error(`Dataset "${targetDataset}" does not exist`);
    await client.datasets.create(targetDataset);
  }
  return targetDataset;
}
function getMutationOperation(flags) {
  const { replace, missing } = flags;
  if (replace && missing)
    throw new Error("Cannot use both --replace and --missing");
  return flags.replace ? "createOrReplace" : flags.missing ? "createIfNotExists" : "create";
}
function getPercentage(opts) {
  if (!opts.total || typeof opts.current > "u")
    return "";
  const percent = Math.floor(opts.current / opts.total * 100);
  return `[${padStart__default.default(`${percent}`, 3, " ")}%] `;
}
function getUrlStream(url2) {
  return getIt.getIt([middleware.promise({ onlyBody: !0 })])({ url: url2, stream: !0 });
}
function printWarnings(warnings, output) {
  const assetFails = warnings.filter((warn2) => warn2.type === "asset");
  if (!assetFails.length)
    return;
  const warn = (output.warn || output.print).bind(output);
  warn(yellow("\u26A0 Failed to import the following %s:"), assetFails.length > 1 ? "assets" : "asset"), warnings.forEach((warning) => {
    warn(`  ${warning.url}`);
  });
}
const listAliasesHandler = async (args, context) => {
  const { apiClient, output } = context, client = apiClient(), aliases = await listAliases(client);
  output.print(
    aliases.map((set) => `${ALIAS_PREFIX}${set.name} -> ${set.datasetName || "<unlinked>"}`).join(`
`)
  );
}, listDatasetsCommand = {
  name: "list",
  group: "dataset",
  helpText: "",
  signature: "",
  description: "List datasets of your project",
  action: async (args, context) => {
    const { apiClient, output } = context, datasets = await apiClient().datasets.list();
    output.print(datasets.map((set) => set.name).join(`
`)), await listAliasesHandler(args, context);
  }
}, helpText$k = `
Options
  --source-maps Enable source maps for built bundles (increases size of bundle)
  --no-minify Skip minifying built JavaScript (speeds up build, increases size of bundle)
  --no-build Don't build the studio prior to deploy, instead deploying the version currently in \`dist/\`

Examples
  sanity deploy
  sanity deploy --no-minify --source-maps
`, deployCommand = {
  name: "deploy",
  signature: "[SOURCE_DIR] [--no-build]  [--source-maps] [--no-minify]",
  description: "Builds and deploys Sanity Studio to Sanity hosting",
  action: async (args, context) => (await Promise.resolve().then(function() {
    return require("./deployAction.js");
  })).default(args, context),
  helpText: helpText$k
}, helpText$j = `
Examples
  sanity undeploy
`, undeployCommand = {
  name: "undeploy",
  signature: "",
  description: "Removes the deployed Sanity Studio from Sanity hosting",
  action: async (args, context) => (await Promise.resolve().then(function() {
    return require("./undeployAction.js");
  })).default(args, context),
  helpText: helpText$j
}, helpText$i = `
Notes
  Changing the hostname or port number might require a new entry to the CORS-origins allow list.

Options
  --port <port> TCP port to start server on. [default: 3333]
  --host <host> The local network interface at which to listen. [default: "127.0.0.1"]

Examples
  sanity dev --host=0.0.0.0
  sanity dev --port=1942
`, devCommand = {
  name: "dev",
  signature: "[--port <port>] [--host <host>]",
  description: "Starts a local dev server for Sanity Studio with live reloading",
  action: async (args, context) => (await getDevAction())(args, context),
  helpText: helpText$i
};
async function getDevAction() {
  return (await Promise.resolve().then(function() {
    return require("./devAction.js");
  })).default;
}
const helpText$h = `
Options
  --replace On duplicate document IDs, replace existing document with specified document(s)
  --missing On duplicate document IDs, don't modify the target document(s)
  --watch   Write the documents whenever the target file or buffer changes
  --json5   Use JSON5 file type to allow a "simplified" version of JSON
  --id <id> Specify a document ID to use. Will fetch remote document ID and populate editor.
  --dataset NAME to override dataset

Examples
  # Create the document specified in "myDocument.json".
  sanity documents create myDocument.json

  # Open configured $EDITOR and create the specified document(s)
  sanity documents create

  # Fetch document with the ID "myDocId" and open configured $EDITOR with the
  # current document content (if any). Replace document with the edited version
  # when the editor closes
  sanity documents create --id myDocId --replace

  # Open configured $EDITOR and replace the document with the given content
  # on each save. Use JSON5 file extension and parser for simplified syntax.
  sanity documents create --id myDocId --watch --replace --json5
`, createDocumentsCommand = {
  name: "create",
  group: "documents",
  signature: "[FILE]",
  helpText: helpText$h,
  description: "Create one or more documents",
  // eslint-disable-next-line complexity
  action: async (args, context) => {
    const { apiClient, output } = context, { replace, missing, watch, id, dataset } = args.extOptions, [file] = args.argsWithoutOptions, useJson5 = args.extOptions.json5, client = dataset ? apiClient().clone().config({ dataset }) : apiClient();
    if (replace && missing)
      throw new Error("Cannot use both --replace and --missing");
    if (id && file)
      throw new Error("Cannot use --id when specifying a file path");
    let operation = "create";
    if ((replace || missing) && (operation = replace ? "createOrReplace" : "createIfNotExists"), file) {
      const contentPath = path__default.default.resolve(process.cwd(), file), content = json5__default.default.parse(await fs__default$1.default.readFile(contentPath, "utf8")), result = await writeDocuments(content, operation, client);
      output.print(getResultMessage(result, operation));
      return;
    }
    const docId = id || uuid.uuid(), ext = useJson5 ? "json5" : "json", tmpFile = path__default.default.join(os__default.default.tmpdir(), "sanity-cli", `${docId}.${ext}`), stringify = useJson5 ? json5__default.default.stringify : JSON.stringify, defaultValue = id && await client.getDocument(id) || { _id: docId, _type: "specify-me" };
    await fs__default$1.default.mkdir(path__default.default.join(os__default.default.tmpdir(), "sanity-cli"), { recursive: !0 }), await fs__default$1.default.writeFile(tmpFile, stringify(defaultValue, null, 2), "utf8");
    const editor = getEditor();
    watch ? (registerUnlinkOnSigInt(tmpFile), output.print(`Watch mode: ${tmpFile}`), output.print("Watch mode: Will write documents on each save."), output.print("Watch mode: Press Ctrl + C to cancel watch mode."), chokidar__default.default.watch(tmpFile).on("change", () => (output.print(""), readAndPerformCreatesFromFile(tmpFile))), execa__default.default(editor.bin, editor.args.concat(tmpFile), { stdio: "inherit" })) : (execa__default.default.sync(editor.bin, editor.args.concat(tmpFile), { stdio: "inherit" }), await readAndPerformCreatesFromFile(tmpFile), await fs__default$1.default.unlink(tmpFile).catch(noop__default.default));
    async function readAndPerformCreatesFromFile(filePath) {
      let content;
      try {
        content = json5__default.default.parse(await fs__default$1.default.readFile(filePath, "utf8"));
      } catch (err) {
        output.error(`Failed to read input: ${err.message}`);
        return;
      }
      if (isEqual__default.default(content, defaultValue)) {
        output.print("Value not modified, doing nothing."), output.print("Modify document to trigger creation.");
        return;
      }
      try {
        const writeResult = await writeDocuments(content, operation, client);
        output.print(getResultMessage(writeResult, operation));
      } catch (err) {
        output.error(`Failed to write documents: ${err.message}`), err.message.includes("already exists") && output.error("Perhaps you want to use `--replace` or `--missing`?");
      }
    }
  }
};
function registerUnlinkOnSigInt(tmpFile) {
  process.on("SIGINT", async () => {
    await fs__default$1.default.unlink(tmpFile).catch(noop__default.default), process.exit(130);
  });
}
function writeDocuments(documents, operation, client) {
  const docs = Array.isArray(documents) ? documents : [documents];
  if (docs.length === 0)
    throw new Error("No documents provided");
  const mutations = docs.map((doc, index) => {
    if (validateDocument(doc, index, docs), operation === "create")
      return { create: doc };
    if (operation === "createIfNotExists") {
      if (isIdentifiedSanityDocument(doc))
        return { createIfNotExists: doc };
      throw new Error(`Missing required _id attribute for ${operation}`);
    }
    if (operation === "createOrReplace") {
      if (isIdentifiedSanityDocument(doc))
        return { createOrReplace: doc };
      throw new Error(`Missing required _id attribute for ${operation}`);
    }
    throw new Error(`Unsupported operation ${operation}`);
  });
  return client.transaction(mutations).commit();
}
function validateDocument(doc, index, arr) {
  const isSingle = arr.length === 1;
  if (!isPlainObject__default.default(doc))
    throw new Error(getErrorMessage("must be an object", index, isSingle));
  if (!isSanityDocumentish(doc))
    throw new Error(getErrorMessage("must have a `_type` property of type string", index, isSingle));
}
function isSanityDocumentish(doc) {
  return doc !== null && typeof doc == "object" && "_type" in doc && typeof doc._type == "string";
}
function isIdentifiedSanityDocument(doc) {
  return isSanityDocumentish(doc) && "_id" in doc;
}
function getErrorMessage(message, index, isSingle) {
  return isSingle ? `Document ${message}` : `Document at index ${index} ${message}`;
}
function getResultMessage(result, operation) {
  const joiner = `
  - `;
  if (operation === "createOrReplace")
    return `Upserted:
  - ${result.results.map((res) => res.id).join(joiner)}`;
  if (operation === "create")
    return `Created:
  - ${result.results.map((res) => res.id).join(joiner)}`;
  const created = [], skipped = [];
  for (const res of result.results)
    res.operation === "update" ? skipped.push(res.id) : created.push(res.id);
  return created.length > 0 && skipped.length > 0 ? [
    `Created:
  - ${created.join(joiner)}`,
    `Skipped (already exists):${joiner}${skipped.join(joiner)}`
  ].join(`

`) : created.length > 0 ? `Created:
  - ${created.join(joiner)}` : `Skipped (already exists):
  - ${skipped.join(joiner)}`;
}
function getEditor() {
  const defaultEditor = /^win/.test(process.platform) ? "notepad" : "vim", args = (process.env.VISUAL || process.env.EDITOR || defaultEditor).split(/\s+/);
  return { bin: args.shift() || "", args };
}
const helpText$g = `
Delete a document from the projects configured dataset

Options
  --dataset NAME to override dataset

Example
  # Delete the document with the ID "myDocId"
  sanity documents delete myDocId

  # ID wrapped in double or single quote works equally well
  sanity documents delete 'myDocId'

  # Delete document with ID "someDocId" from dataset "blog"
  sanity documents delete --dataset=blog someDocId

  # Delete the document with ID "doc1" and "doc2"
  sanity documents delete doc1 doc2
`, deleteDocumentsCommand = {
  name: "delete",
  group: "documents",
  signature: "[ID] [...IDS]",
  helpText: helpText$g,
  description: "Delete a document by ID",
  action: async (args, context) => {
    const { apiClient, output, chalk } = context, { dataset } = args.extOptions, ids = args.argsWithoutOptions.map((str) => `${str}`);
    if (!ids.length)
      throw new Error("Document ID must be specified");
    const client = dataset ? apiClient().clone().config({ dataset }) : apiClient(), transaction = ids.reduce((trx, id) => trx.delete(id), client.transaction());
    try {
      const { results } = await transaction.commit(), deleted = results.filter((res) => res.operation === "delete").map((res) => res.id), notFound = ids.filter((id) => !deleted.includes(id));
      deleted.length > 0 && output.print(`Deleted ${deleted.length} ${pluralize__default.default("document", deleted.length)}`), notFound.length > 0 && output.error(
        chalk.red(`${pluralize__default.default("Document", notFound.length)} not found: ${notFound.join(", ")}`)
      );
    } catch (err) {
      throw new Error(`Failed to delete ${pluralize__default.default("document", ids.length)}:
${err.message}`);
    }
  }
}, documentsGroup = {
  name: "documents",
  signature: "[COMMAND]",
  isGroupRoot: !0,
  description: "Manages documents in your Sanity Content Lake datasets"
}, identity = (inp) => inp;
function colorizeJson(input, chalk) {
  const formatters = {
    punctuator: chalk.white,
    key: chalk.white,
    string: chalk.green,
    number: chalk.yellow,
    literal: chalk.bold,
    whitespace: identity
  }, json = JSON.stringify(input, null, 2);
  return tokenize__default.default(json).map((token, i, arr) => {
    const prevToken = i === 0 ? token : arr[i - 1];
    return token.type === "string" && prevToken.type === "whitespace" && /^\n\s+$/.test(prevToken.value) ? { ...token, type: "key" } : token;
  }).map((token) => (formatters[token.type] || identity)(token.raw)).join("");
}
const helpText$f = `
Get and print a document from the projects configured dataset

Options
  --pretty colorized JSON output
  --dataset NAME to override dataset

Examples
  # Get the document with the ID "myDocId"
  sanity documents get myDocId

  # ID wrapped in double or single quote works equally well
  sanity documents get 'myDocId'
`, getDocumentsCommand = {
  name: "get",
  group: "documents",
  signature: "[DOCUMENT_ID]",
  helpText: helpText$f,
  description: "Get and print a document by ID",
  action: async (args, context) => {
    const { apiClient, output, chalk } = context, { pretty, dataset } = args.extOptions, [docId] = args.argsWithoutOptions.map((str) => `${str}`);
    if (!docId)
      throw new Error("Document ID must be specified");
    const client = dataset ? apiClient().clone().config({ dataset }) : apiClient();
    try {
      const doc = await client.getDocument(docId);
      if (!doc)
        throw new Error(`Document ${docId} not found`);
      output.print(pretty ? colorizeJson(doc, chalk) : JSON.stringify(doc, null, 2));
    } catch (err) {
      throw new Error(`Failed to fetch document:
${err.message}`);
    }
  }
}, defaultApiVersion = "v2022-06-01", helpText$e = `
Run a query against the projects configured dataset

Options
  --pretty colorized JSON output
  --dataset NAME to override dataset
  --project PROJECT to override project ID
  --anonymous Send the query without any authorization token
  --api-version API version to use (defaults to \`${defaultApiVersion}\`)

Environment variables
  \`SANITY_CLI_QUERY_API_VERSION\` - will use the defined API version,
  unless \`--api-version\` is specified.

Examples
  # Fetch 5 documents of type "movie"
  sanity documents query '*[_type == "movie"][0..4]'

  # Fetch title of the oldest movie in the dataset named "staging"
  sanity documents query '*[_type == "movie"]|order(releaseDate asc)[0]{title}' --dataset staging

  # Use API version v2021-06-07 and do a query
  sanity documents query --api-version v2021-06-07 '*[_id == "header"] { "headerText": pt::text(body) }'
`;
var queryDocumentsCommand = {
  name: "query",
  group: "documents",
  signature: "[QUERY]",
  helpText: helpText$e,
  description: "Query for documents",
  action: async (args, context) => {
    var _a, _b;
    const {
      pretty,
      dataset,
      project,
      anonymous,
      "api-version": apiVersion
    } = await parseCliFlags$1(args), { apiClient, output, chalk, cliConfig } = context, [query] = args.argsWithoutOptions;
    if (!query)
      throw new Error("Query must be specified");
    apiVersion || output.warn(chalk.yellow(`--api-version not specified, using \`${defaultApiVersion}\``));
    const requireDataset = !dataset, requireProject = !project, requireUser = !anonymous;
    if (requireProject && !((_a = cliConfig == null ? void 0 : cliConfig.api) != null && _a.projectId))
      throw new Error(
        "No project configured in CLI config - either configure one, or use `--project` flag"
      );
    if (requireDataset && !((_b = cliConfig == null ? void 0 : cliConfig.api) != null && _b.dataset))
      throw new Error(
        "No dataset configured in CLI config - either configure one, or use `--dataset` flag"
      );
    const baseClient = apiClient({ requireProject, requireUser }).clone(), { dataset: originalDataset, projectId: originalProjectId } = baseClient.config(), client = baseClient.config({
      projectId: project || originalProjectId,
      dataset: dataset || originalDataset,
      apiVersion: apiVersion || defaultApiVersion
    });
    try {
      const docs = await client.fetch(query);
      if (!docs)
        throw new Error("Query returned no results");
      output.print(pretty ? colorizeJson(docs, chalk) : JSON.stringify(docs, null, 2));
    } catch (err) {
      throw new Error(`Failed to run query:
${err.message}`);
    }
  }
};
function parseCliFlags$1(args) {
  const fallbackApiVersion = process.env.SANITY_CLI_QUERY_API_VERSION;
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).option("pretty", { type: "boolean", default: !1 }).option("dataset", { type: "string" }).option("project", { type: "string" }).option("anonymous", { type: "boolean", default: !1 }).option("api-version", { type: "string", default: fallbackApiVersion }).argv;
}
const description$2 = "Downloads and validates all document specified in a workspace", helpText$d = `
Options
  -y, --yes Skips the first confirmation prompt.
  --workspace <name> The name of the workspace to use when downloading and validating all documents.
  --dataset <name> Override the dataset used. By default, this is derived from the given workspace.
  --file <filepath> Provide a path to either an .ndjson file or a tarball containing an .ndjson file.
  --format <pretty|ndjson|json> The output format used to print the found validation markers and report progress.
  --level <error|warning|info> The minimum level reported out. Defaults to warning.
  --max-custom-validation-concurrency <number> Specify how many custom validators can run concurrently. Defaults to 5.

Examples
  # Validates all documents in a Sanity project with more than one workspace
  sanity documents validate --workspace default

  # Override the dataset specified in the workspace
  sanity documents validate --workspace default --dataset staging

  # Save the results of the report into a file
  sanity documents validate > report.txt

  # Report out info level validation markers too
  sanity documents validate --level info
`, validateDocumentsCommand$1 = {
  name: "validate",
  group: "documents",
  signature: "",
  description: description$2,
  helpText: helpText$d,
  action: async (args, context) => (await Promise.resolve().then(function() {
    return require("./validateAction2.js");
  })).default(args, context)
}, helpText$c = `
Options
  --with-user-token Prime access token from CLI config into getCliClient()
  --mock-browser-env Mocks a browser-like environment using jsdom

Examples
  # Run the script at some/script.js in Sanity context
  sanity exec some/script.js

  # Run the script at migrations/fullname.ts and configure \`getCliClient()\`
  # from \`sanity/cli\`to include the current user's token
  sanity exec migrations/fullname.ts --with-user-token

  # Run the script at scripts/browserScript.js in a mock browser environment
  sanity exec scripts/browserScript.js --mock-browser-env

  # Pass arbitrary arguments to scripts by separating them with a \`--\`.
  # Arguments are available in \`process.argv\` as they would in regular node scripts
  # eg the following command would yield a \`process.argv\` of:
  # ['/path/to/node', '/path/to/myscript.js', '--dry-run', 'positional-argument']
  sanity exec --mock-browser-env myscript.js -- --dry-run positional-argument
`, execCommand = {
  name: "exec",
  signature: "SCRIPT",
  description: "Executes a script within the Sanity Studio context",
  helpText: helpText$c,
  action: async (args, context) => (await Promise.resolve().then(function() {
    return require("./execScript.js");
  })).default(args, context)
}, helpText$b = `
Options
  --api <api-id> Undeploy API with this ID (project, dataset and tag flags takes preference)
  --project <projectId> Project ID to delete GraphQL API for
  --dataset <dataset> Delete GraphQL API for the given dataset
  --tag <tag> Delete GraphQL API for the given tag (defaults to 'default')
  --force Skip confirmation prompt, forcefully undeploying the GraphQL API

Examples
  sanity graphql undeploy
  sanity graphql undeploy --api ios
  sanity graphql undeploy --dataset staging
  sanity graphql undeploy --dataset staging --tag next
`, deleteGraphQLAPICommand = {
  name: "undeploy",
  group: "graphql",
  signature: "",
  description: "Remove a deployed GraphQL API",
  action: async (args, context) => (await Promise.resolve().then(function() {
    return require("./deleteApiAction.js");
  })).default(args, context),
  helpText: helpText$b
}, helpText$a = `
Options
  --dry-run Validate defined APIs, exiting with an error on breaking changes
  --force Deploy API without confirming breaking changes
  --api <api-id> Only deploy API with this ID. Can be specified multiple times.

The following options will override any setting from the CLI configuration file
(sanity.cli.js/sanity.cli.ts) - and applies to ALL defined APIs defined in that
configuration file. Tread with caution!

  --tag Deploy API(s) to given tag (defaults to 'default')
  --dataset <name> Deploy API for the given dataset
  --generation <gen1|gen2|gen3> API generation to deploy (defaults to 'gen3')
  --non-null-document-fields Use non-null document fields (_id, _type etc)
  --playground Enable GraphQL playground for easier debugging
  --no-playground Disable GraphQL playground
  --with-union-cache *Experimental:* Enable union cache that optimizes schema generation for schemas with many self referencing types

Examples
  # Deploy all defined GraphQL APIs
  sanity graphql deploy

  # Validate defined GraphQL APIs, check for breaking changes, skip deploy
  sanity graphql deploy --dry-run

  # Deploy only the GraphQL APIs with the IDs "staging" and "ios"
  sanity graphql deploy --api staging --api ios

  # Deploy all defined GraphQL APIs, overriding any playground setting
  sanity graphql deploy --playground
`, deployGraphQLAPICommand = {
  name: "deploy",
  signature: "",
  group: "graphql",
  description: "Deploy a GraphQL API from the current Sanity schema",
  action: async (args, context) => (await Promise.resolve().then(function() {
    return require("./deployApiAction.js");
  })).default(args, context),
  helpText: helpText$a
}, graphqlGroup = {
  name: "graphql",
  signature: "[COMMAND]",
  isGroupRoot: !0,
  description: "Deploys changes to your project's GraphQL API(s)"
}, helpText$9 = `
Examples
  sanity graphql list
`, listGraphQLAPIsCommand = {
  name: "list",
  signature: "",
  group: "graphql",
  description: "Lists all the GraphQL endpoints deployed for this project",
  action: async (args, context) => (await Promise.resolve().then(function() {
    return require("./listApisAction.js");
  })).default(args, context),
  helpText: helpText$9
}, createHookCommand = {
  name: "create",
  group: "hook",
  signature: "",
  helpText: "",
  description: "Create a new hook for the given dataset",
  action: async (args, context) => {
    const { apiClient, output } = context, client = apiClient(), { projectId } = client.config();
    if (!projectId)
      throw new Error("No project ID found");
    const manageUrl = `https://www.sanity.io/organizations/${(await client.projects.getById(projectId) || {}).organizationId || "personal"}/project/${projectId}/api/webhooks/new`;
    output.print(`Opening ${manageUrl}`), open__default.default(manageUrl);
  }
}, deleteHookCommand = {
  name: "delete",
  group: "hook",
  signature: "[NAME]",
  helpText: "",
  description: "Delete a hook within your project",
  action: async (args, context) => {
    const { apiClient } = context, [name] = args.argsWithoutOptions, client = apiClient(), hookId = await promptForHook$1(name, context);
    try {
      await client.clone().config({ apiVersion: "2021-10-04" }).request({ method: "DELETE", uri: `/hooks/${hookId}` });
    } catch (err) {
      throw new Error(`Hook deletion failed:
${err.message}`);
    }
  }
};
async function promptForHook$1(specified, context) {
  const specifiedName = specified && specified.toLowerCase(), { prompt, apiClient } = context, hooks = await apiClient().clone().config({ apiVersion: "2021-10-04" }).request({ uri: "/hooks", json: !0 });
  if (specifiedName) {
    const selected = hooks.filter((hook) => hook.name.toLowerCase() === specifiedName)[0];
    if (!selected)
      throw new Error(`Hook with name "${specified} not found"`);
    return selected.id;
  }
  const choices = hooks.map((hook) => ({ value: hook.id, name: hook.name }));
  return prompt.single({
    message: "Select hook to delete",
    type: "list",
    choices
  });
}
const hookGroup = {
  name: "hook",
  signature: "[COMMAND]",
  isGroupRoot: !0,
  description: "Sets up and manages webhooks within your Sanity project"
}, printHookAttemptCommand = {
  name: "attempt",
  group: "hook",
  signature: "ATTEMPT_ID",
  helpText: "",
  description: "Print details of a given webhook delivery attempt",
  action: async (args, context) => {
    const { apiClient, output } = context, [attemptId] = args.argsWithoutOptions, client = apiClient();
    let attempt;
    try {
      attempt = await client.request({ uri: `/hooks/attempts/${attemptId}` });
    } catch (err) {
      throw new Error(`Hook attempt retrieval failed:
${err.message}`);
    }
    const { createdAt, resultCode, resultBody, failureReason, inProgress } = attempt;
    if (output.print(`Date: ${createdAt}`), output.print(`Status: ${getStatus(attempt)}`), output.print(`Status code: ${resultCode}`), attempt.isFailure && output.print(`Failure: ${formatFailure(attempt)}`), !inProgress && (!failureReason || failureReason === "http")) {
      const body = resultBody ? `
---
${resultBody}
---
` : "<empty>";
      output.print(`Response body: ${body}`);
    }
  }
};
function formatFailure(attempt, options = {}) {
  const { includeHelp } = options, { id, failureReason, resultCode } = attempt, help = includeHelp ? `(run \`sanity hook attempt ${id}\` for details)` : "";
  switch (failureReason) {
    case "http":
      return `HTTP ${resultCode} ${help}`;
    case "timeout":
      return "Request timed out";
    case "network":
      return "Network error";
  }
  return "Unknown error";
}
function getStatus(attempt) {
  return attempt.isFailure ? "Failed" : attempt.inProgress ? "In progress" : "Delivered";
}
const listHookLogsCommand = {
  name: "logs",
  group: "hook",
  signature: "[NAME]",
  helpText: "",
  description: "List latest log entries for a given hook",
  action: async (args, context) => {
    const { apiClient } = context, flags = args.extOptions, [name] = args.argsWithoutOptions, client = apiClient(), hookId = await promptForHook(name, context);
    let messages, attempts;
    try {
      messages = await client.request({ uri: `/hooks/${hookId}/messages` }), attempts = await client.request({ uri: `/hooks/${hookId}/attempts` });
    } catch (err) {
      throw new Error(`Hook logs retrieval failed:
${err.message}`);
    }
    const groupedAttempts = groupBy__default.default(attempts, "messageId"), populated = messages.map((msg) => ({
      ...msg,
      attempts: groupedAttempts[msg.id]
    })), totalMessages = messages.length - 1;
    populated.forEach((message, i) => {
      printMessage(message, context, { detailed: flags.detailed }), printSeparator(context, totalMessages === i);
    });
  }
};
async function promptForHook(specified, context) {
  const specifiedName = specified && specified.toLowerCase(), { prompt, apiClient } = context, hooks = await apiClient().clone().config({ apiVersion: "2021-10-04" }).request({ uri: "/hooks", json: !0 });
  if (specifiedName) {
    const selected = hooks.filter((hook) => hook.name.toLowerCase() === specifiedName)[0];
    if (!selected)
      throw new Error(`Hook with name "${specified} not found"`);
    return selected.id;
  }
  if (hooks.length === 0)
    throw new Error("No hooks currently registered");
  if (hooks.length === 1)
    return hooks[0].id;
  const choices = hooks.map((hook) => ({ value: hook.id, name: hook.name }));
  return prompt.single({
    message: "Select hook to list logs for",
    type: "list",
    choices
  });
}
function printSeparator(context, skip) {
  skip || context.output.print(`---
`);
}
function printMessage(message, context, options) {
  const { detailed } = options, { output, chalk } = context;
  output.print(`Date: ${message.createdAt}`), output.print(`Status: ${message.status}`), output.print(`Result code: ${message.resultCode}`), message.failureCount > 0 && output.print(`Failures: ${message.failureCount}`), detailed && (output.print("Payload:"), output.print(node_util.inspect(JSON.parse(message.payload), { colors: !0 }))), detailed && message.attempts && (output.print("Attempts:"), message.attempts.forEach((attempt) => {
    const prefix = `  [${attempt.createdAt.replace(/\.\d+Z$/, "Z")}]`;
    if (attempt.inProgress)
      output.print(`${prefix} ${chalk.yellow("Pending")}`);
    else if (attempt.isFailure) {
      const failure = formatFailure(attempt, { includeHelp: !0 });
      output.print(`${prefix} ${chalk.yellow(`Failure: ${failure}`)}`);
    } else
      output.print(`${prefix} Success: HTTP ${attempt.resultCode} (${attempt.duration}ms)`);
  })), output.print("");
}
const listHooksCommand = {
  name: "list",
  group: "hook",
  signature: "",
  helpText: "",
  description: "List hooks for a given project",
  action: async (args, context) => {
    const { apiClient, output } = context, client = apiClient();
    let hooks;
    try {
      hooks = await client.clone().config({ apiVersion: "2021-10-04" }).request({ uri: "/hooks" });
    } catch (err) {
      throw new Error(`Hook list retrieval failed:
${err.message}`);
    }
    hooks.forEach((hook) => {
      output.print(`Name: ${hook.name}`), output.print(`Dataset: ${hook.dataset}`), output.print(`URL: ${hook.url}`), hook.type === "document" && (output.print(`HTTP method: ${hook.httpMethod}`), hook.description && output.print(`Description: ${hook.description}`)), output.print("");
    });
  }
}, MIGRATIONS_DIRECTORY = "migrations", MIGRATION_SCRIPT_EXTENSIONS = ["mjs", "js", "ts", "cjs"], minimalAdvanced = ({
  migrationName,
  documentTypes
}) => `import {defineMigration, patch, at, setIfMissing} from 'sanity/migrate'

/**
 * this migration will set \`Default title\` on all documents that are missing a title
 * and make \`true\` the default value for the \`enabled\` field
 */
export default defineMigration({
  title: '${migrationName}',
${documentTypes.length > 0 ? `  documentTypes: [${documentTypes.map((t) => JSON.stringify(t)).join(", ")}],
` : ""}
  async *migrate(documents, context) {
    for await (const document of documents()) {
      yield patch(document._id, [
        at('title', setIfMissing('Default title')),
        at('enabled', setIfMissing(true)),
      ])
    }
  }
})
`, minimalSimple = ({
  migrationName,
  documentTypes
}) => `import {at, defineMigration, setIfMissing, unset} from 'sanity/migrate'

export default defineMigration({
  title: '${migrationName}',
${documentTypes.length > 0 ? `  documentTypes: [${documentTypes.map((t) => JSON.stringify(t)).join(", ")}],
` : ""}
  migrate: {
    document(doc, context) {
      // this will be called for every document of the matching type
      // any patch returned will be applied to the document
      // you can also return mutations that touches other documents

      return at('title', setIfMissing('Default title'))
    },
    node(node, path, context) {
      // this will be called for every node in every document of the matching type
      // any patch returned will be applied to the document
      // you can also return mutations that touches other documents

      if (typeof node === 'string' && node === 'deleteme') {
        return unset()
      }
    },
    object(node, path, context) {
      // this will be called for every object node in every document of the matching type
      // any patch returned will be applied to the document
      // you can also return mutations that touches other documents
      if (node._type === 'author') {
        // make sure all authors objects have a books array
        return at('books', setIfMissing([]))
      }
    },
    array(node, path, context) {
      // this will be called for every array node in every document of the matching type
      // any patch returned will be applied to the document
      // you can also return mutations that touches other documents
    },
    string(node, path, context) {
      // this will be called for every string node in every document of the matching type
      // any patch returned will be applied to the document
      // you can also return mutations that touches other documents
    },
    number(node, path, context) {
      // this will be called for every number node in every document of the matching type
      // any patch returned will be applied to the document
      // you can also return mutations that touches other documents
    },
    boolean(node, path, context) {
      // this will be called for every boolean node in every document of the matching type
      // any patch returned will be applied to the document
      // you can also return mutations that touches other documents
    },
    null(node, path, context) {
      // this will be called for every null node in every document of the matching type
      // any patch returned will be applied to the document
      // you can also return mutations that touches other documents
    },
  },
})
`, renameField = ({
  migrationName,
  documentTypes
}) => `import {defineMigration, at, setIfMissing, unset} from 'sanity/migrate'

const from = 'oldFieldName'
const to = 'newFieldName'

export default defineMigration({
  title: '${migrationName}',
${documentTypes.length > 0 ? `  documentTypes: [${documentTypes.map((t) => JSON.stringify(t)).join(", ")}],
` : ""}
  migrate: {
    document(doc, context) {
      return [
        at(to, setIfMissing(doc[from])),
        at(from, unset())
      ]
    }
  }
})
`, renameType = ({
  migrationName,
  documentTypes
}) => `import {defineMigration, at, set} from 'sanity/migrate'

const oldType = 'old'
const newType = 'new'

export default defineMigration({
  title: '${migrationName}',
${documentTypes.length > 0 ? `  documentTypes: [${documentTypes.map((t) => JSON.stringify(t)).join(", ")}],
` : ""}
  migrate: {
    object(object, path, context) {
      if (object._type === oldType) {
        return at('_type', set(newType))
      }
    }
  }
})
`, stringToPTE = ({
  migrationName,
  documentTypes
}) => `import {pathsAreEqual, stringToPath} from 'sanity'
import {defineMigration, set} from 'sanity/migrate'

const targetPath = stringToPath('some.path')

export default defineMigration({
  title: '${migrationName}',
${documentTypes.length > 0 ? `  documentTypes: [${documentTypes.map((t) => JSON.stringify(t)).join(", ")}],
` : ""}
  migrate: {
    string(node, path, ctx) {
      if (pathsAreEqual(path, targetPath)) {
        return set([
          {
            style: 'normal',
            _type: 'block',
            children: [
              {
                _type: 'span',
                marks: [],
                text: node,
              },
            ],
            markDefs: [],
          },
        ])
      }
    },
  },
})
`, helpText$8 = `
Examples:
  # Create a new migration, prompting for title and options
  sanity migration create

  # Create a new migration with the provided title, prompting for options
  sanity migration create "Rename field from location to address"
`, TEMPLATES = [
  { name: "Minimalistic migration to get you started", template: minimalSimple },
  { name: "Rename an object type", template: renameType },
  { name: "Rename a field", template: renameField },
  { name: "Convert string field to Portable Text", template: stringToPTE },
  {
    name: "Advanced template using async iterators providing more fine grained control",
    template: minimalAdvanced
  }
], createMigrationCommand = {
  name: "create",
  group: "migration",
  signature: "[TITLE]",
  helpText: helpText$8,
  description: "Create a new migration within your project",
  action: async (args, context) => {
    const { output, prompt, workDir, chalk } = context;
    let [title] = args.argsWithoutOptions;
    for (; !(title != null && title.trim()); )
      title = await prompt.single({
        type: "input",
        suffix: ' (e.g. "Rename field from location to address")',
        message: "Title of migration"
      }), title.trim() || output.error(chalk.red("Name cannot be empty"));
    const types2 = await prompt.single({
      type: "input",
      suffix: " (optional)",
      message: "Type of documents to migrate. You can add multiple types separated by comma"
    }), templatesByName = Object.fromEntries(TEMPLATES.map((t) => [t.name, t])), template = await prompt.single({
      type: "list",
      message: "Select a template",
      choices: TEMPLATES.map((definedTemplate) => ({
        name: definedTemplate.name,
        value: definedTemplate.name
      }))
    }), sluggedName = deburr__default.default(title.toLowerCase()).replace(/\s+/g, "-").replace(/[^a-z0-9-]/g, ""), destDir = path__default.default.join(workDir, MIGRATIONS_DIRECTORY, sluggedName);
    if (fs.existsSync(destDir) && !await prompt.single({
      type: "confirm",
      message: `Migration directory ${chalk.cyan(destDir)} already exists. Overwrite?`,
      default: !1
    }))
      return;
    fs.mkdirSync(destDir, { recursive: !0 });
    const renderedTemplate = (templatesByName[template].template || minimalSimple)({
      migrationName: title,
      documentTypes: types2.split(",").map((t) => t.trim()).filter(Boolean)
    }), definitionFile = path__default.default.join(destDir, "index.ts");
    await fs$1.writeFile(definitionFile, renderedTemplate), output.print(), output.print(`${chalk.green("\u2713")} Migration created!`), output.print(), output.print("Next steps:"), output.print(
      `Open ${chalk.bold(
        definitionFile
      )} in your code editor and write the code for your migration.`
    ), output.print(
      `Dry run the migration with:
\`${chalk.bold(
        `sanity migration run ${sluggedName} --project=<projectId> --dataset <dataset> `
      )}\``
    ), output.print(
      `Run the migration against a dataset with:
 \`${chalk.bold(
        `sanity migration run ${sluggedName} --project=<projectId> --dataset <dataset> --no-dry-run`
      )}\``
    ), output.print(), output.print(
      `\u{1F449} Learn more about schema and content migrations at ${chalk.bold(
        "https://www.sanity.io/docs/schema-and-content-migrations"
      )}`
    );
  }
};
function resolveMigrationScript(workDir, migrationName) {
  return [migrationName, path__default.default.join(migrationName, "index")].flatMap(
    (location) => MIGRATION_SCRIPT_EXTENSIONS.map((ext) => {
      const relativePath = path__default.default.join(MIGRATIONS_DIRECTORY, `${location}.${ext}`), absolutePath = path__default.default.resolve(workDir, relativePath);
      let mod;
      try {
        mod = require(absolutePath);
      } catch (err) {
        if (err.code !== "MODULE_NOT_FOUND")
          throw new Error(`Error: ${err.message}"`);
      }
      return { relativePath, absolutePath, mod };
    })
  );
}
function isLoadableMigrationScript(script) {
  if (typeof script.mod > "u" || !isPlainObject__default.default(script.mod.default))
    return !1;
  const mod = script.mod.default;
  return typeof mod.title == "string" && mod.migrate !== void 0;
}
const helpText$7 = "", listMigrationCommand = {
  name: "list",
  group: "migration",
  signature: "",
  helpText: helpText$7,
  description: "List available migrations",
  action: async (_, context) => {
    const { workDir, output, chalk } = context;
    try {
      const migrations = await resolveMigrations(workDir);
      if (migrations.length === 0) {
        output.print("No migrations found in migrations folder of the project"), output.print(
          `
Run ${chalk.green("`sanity migration create <NAME>`")} to create a new migration`
        );
        return;
      }
      const table = new consoleTablePrinter.Table({
        title: `Found ${migrations.length} migrations in project`,
        columns: [
          { name: "id", title: "ID", alignment: "left" },
          { name: "title", title: "Title", alignment: "left" }
        ]
      });
      migrations.forEach((definedMigration) => {
        table.addRow({ id: definedMigration.id, title: definedMigration.migration.title });
      }), table.printTable(), output.print("\nRun `sanity migration run <ID>` to run a migration");
    } catch (error) {
      if (error.code === "ENOENT") {
        output.print("No migrations folder found in the project"), output.print(
          `
Run ${chalk.green("`sanity migration create <NAME>`")} to create a new migration`
        );
        return;
      }
      throw new Error(`An error occurred while listing migrations: ${error.message}`);
    }
  }
};
async function resolveMigrations(workDir) {
  let unregister;
  unregister = node.register({
    target: `node${process.version.slice(1)}`
  }).unregister;
  const migrationsDir = path__default.default.join(workDir, MIGRATIONS_DIRECTORY), migrationEntries = await fs$1.readdir(migrationsDir, { withFileTypes: !0 }), migrations = [];
  for (const entry of migrationEntries) {
    const entryName = entry.isDirectory() ? entry.name : removeMigrationScriptExtension(entry.name), candidates = resolveMigrationScript(workDir, entryName).filter(isLoadableMigrationScript);
    for (const candidate of candidates)
      migrations.push({
        id: entryName,
        migration: candidate.mod.default
      });
  }
  return unregister && unregister(), migrations;
}
function removeMigrationScriptExtension(fileName) {
  return MIGRATION_SCRIPT_EXTENSIONS.reduce(
    (name, ext) => name.endsWith(`.${ext}`) ? path__default.default.basename(name, `.${ext}`) : name,
    fileName
  );
}
var migrationGroup = {
  name: "migration",
  signature: "[COMMAND]",
  isGroupRoot: !0,
  description: "Manages content migrations for Content Lake datasets"
};
function pathToString(path2) {
  if (!Array.isArray(path2))
    throw new Error("Path is not an array");
  return path2.reduce((target, segment, i) => {
    if (types.isIndexSegment(segment))
      return `${target}[${segment}]`;
    if (types.isKeySegment(segment) && segment._key)
      return `${target}[_key=="${segment._key}"]`;
    if (types.isIndexTuple(segment)) {
      const [from, to] = segment;
      return `${target}[${from}:${to}]`;
    }
    if (typeof segment == "string")
      return `${target}${i === 0 ? "" : "."}${segment}`;
    throw new Error(`Unsupported path segment \`${JSON.stringify(segment)}\``);
  }, "");
}
const maxKeyLength = (children = {}, depth = 0) => Object.entries(children).map(
  ([key, child]) => Math.max(key.length + depth * 2, maxKeyLength(child.children, depth + 1))
).reduce((max, next) => next > max ? next : max, 0), formatTree = ({
  node: node2 = {},
  paddingLength,
  indent: indent2 = "",
  getNodes: getLeaves = ({ nodes }) => nodes,
  getMessage
}) => {
  const entries = Object.entries(node2);
  return entries.map(([key, child], index) => {
    const isLast = index === entries.length - 1, nextIndent = `${indent2}${isLast ? "  " : "\u2502 "}`, leaves = getLeaves(child), nested = formatTree({
      node: child.children,
      paddingLength,
      indent: nextIndent,
      getNodes: getLeaves,
      getMessage
    });
    if (!(leaves != null && leaves.length))
      return [`${indent2}${isLast ? "\u2514" : "\u251C"}\u2500 ${key}`, nested].filter(Boolean).join(`
`);
    const [first, ...rest] = leaves, firstPadding = ".".repeat(paddingLength - indent2.length - key.length), elbow = isLast ? "\u2514" : "\u251C", subsequentPadding = " ".repeat(paddingLength - indent2.length + 2), firstMessage = `${indent2}${elbow}\u2500 ${key} ${firstPadding} ${getMessage(first)}`, subsequentMessages = rest.map((marker) => `${nextIndent}${subsequentPadding} ${getMessage(marker)}`).join(`
`);
    return [[firstMessage, subsequentMessages].filter(Boolean).join(`
`), nested].filter(Boolean).join(`
`);
  }).join(`
`);
};
function convertToTree(nodes) {
  const root = {};
  function addNode(node2, tree = root) {
    if (!node2.path.length) {
      tree.nodes || (tree.nodes = []), tree.nodes.push(node2);
      return;
    }
    const [current, ...rest] = node2.path, key = pathToString([current]);
    tree.children || (tree.children = {}), key in tree.children || (tree.children[key] = {}), addNode({ ...node2, path: rest }, tree.children[key]);
  }
  for (const node2 of nodes) addNode(node2);
  return root;
}
const isTty = node_tty.isatty(1);
function prettyFormat({
  chalk,
  subject,
  migration,
  indentSize = 0
}) {
  return (Array.isArray(subject) ? subject : [subject]).map((subjectEntry) => subjectEntry.type === "transaction" ? [
    [
      badge("transaction", "info", chalk),
      typeof subjectEntry.id > "u" ? null : chalk.underline(subjectEntry.id)
    ].filter(Boolean).join(" "),
    indent(
      prettyFormat({
        chalk,
        subject: subjectEntry.mutations,
        migration,
        indentSize
      })
    )
  ].join(`

`) : prettyFormatMutation({
    chalk,
    subject: subjectEntry,
    migration,
    indentSize
  })).join(`

`);
}
function encodeItemRef(ref) {
  return typeof ref == "number" ? ref : ref._key;
}
function badgeStyle(chalk, variant) {
  return {
    info: chalk.bgWhite.black,
    incremental: chalk.bgGreen.black.bold,
    maybeDestructive: chalk.bgYellow.black.bold,
    destructive: chalk.bgRed.black.bold
  }[variant];
}
function badge(label, variant, chalk) {
  return isTty ? badgeStyle(chalk, variant)(` ${label} `) : `[${label}]`;
}
const mutationImpact = {
  create: "incremental",
  createIfNotExists: "incremental",
  createOrReplace: "maybeDestructive",
  delete: "destructive",
  patch: "maybeDestructive"
};
function documentId(mutation) {
  if ("id" in mutation)
    return mutation.id;
  if ("document" in mutation)
    return mutation.document._id;
}
const listFormatter = new Intl.ListFormat("en-US", {
  type: "disjunction"
});
function mutationHeader(chalk, mutation, migration) {
  var _a;
  const mutationType = badge(mutation.type, mutationImpact[mutation.type], chalk), documentType = "document" in mutation || migration.documentTypes ? badge(
    "document" in mutation ? mutation.document._type : listFormatter.format((_a = migration.documentTypes) != null ? _a : []),
    "info",
    chalk
  ) : null;
  return [mutationType, documentType, chalk.underline(documentId(mutation))].filter(Boolean).join(" ");
}
function prettyFormatMutation({
  chalk,
  subject,
  migration,
  indentSize = 0
}) {
  var _a;
  const lock = "options" in subject ? chalk.cyan(`(if revision==${(_a = subject.options) == null ? void 0 : _a.ifRevision})`) : "", header = [mutationHeader(chalk, subject, migration), lock].join(" "), padding = " ".repeat(indentSize);
  if (subject.type === "create" || subject.type === "createIfNotExists" || subject.type === "createOrReplace")
    return [header, `
`, indent(JSON.stringify(subject.document, null, 2), indentSize)].join("");
  if (subject.type === "patch") {
    const tree = convertToTree(subject.patches.flat()), paddingLength = Math.max(maxKeyLength(tree.children) + 2, 30);
    return [
      header,
      `
`,
      formatTree({
        node: tree.children,
        paddingLength,
        indent: padding,
        getMessage: (patch) => formatPatchMutation(chalk, patch)
      })
    ].join("");
  }
  return header;
}
function formatPatchMutation(chalk, patch) {
  const { op } = patch, formattedType = chalk.bold(op.type);
  if (op.type === "unset")
    return `${chalk.red(formattedType)}()`;
  if (op.type === "diffMatchPatch")
    return `${chalk.yellow(formattedType)}(${op.value})`;
  if (op.type === "inc" || op.type === "dec")
    return `${chalk.yellow(formattedType)}(${op.amount})`;
  if (op.type === "set")
    return `${chalk.yellow(formattedType)}(${JSON.stringify(op.value)})`;
  if (op.type === "setIfMissing")
    return `${chalk.green(formattedType)}(${JSON.stringify(op.value)})`;
  if (op.type === "insert")
    return `${chalk.green(formattedType)}(${op.position}, ${encodeItemRef(
      op.referenceItem
    )}, ${JSON.stringify(op.items)})`;
  if (op.type === "replace")
    return `${chalk.yellow(formattedType)}(${encodeItemRef(op.referenceItem)}, ${JSON.stringify(
      op.items
    )})`;
  if (op.type === "truncate")
    return `${chalk.red(formattedType)}(${op.startIndex}, ${op.endIndex})`;
  throw new Error(`Invalid operation type: ${op.type}`);
}
function indent(subject, size2 = 2) {
  const padding = " ".repeat(size2);
  return subject.split(`
`).map((line) => padding + line).join(`
`);
}
const helpText$6 = `
Options
  --no-dry-run By default the migration runs in dry mode. Pass this option to migrate dataset.
  --concurrency <concurrent> How many mutation requests to run in parallel. Must be between 1 and ${migrate.MAX_MUTATION_CONCURRENCY}. Default: ${migrate.DEFAULT_MUTATION_CONCURRENCY}.
  --no-progress Don't output progress. Useful if you want debug your migration script and see the output of console.log() statements.
  --dataset <dataset> Dataset to migrate. Defaults to the dataset configured in your Sanity CLI config.
  --project <project id> Project ID of the dataset to migrate. Defaults to the projectId configured in your Sanity CLI config.
  --no-confirm Skip the confirmation prompt before running the migration. Make sure you know what you're doing before using this flag.
  --from-export <export.tar.gz> Use a local dataset export as source for migration instead of calling the Sanity API. Note: this is only supported for dry runs.


Examples
  # dry run the migration
  sanity migration run <id>

  # execute the migration against a dataset
  sanity migration run <id> --no-dry-run --project xyz --dataset staging

  # execute the migration using a dataset export as the source
  sanity migration run <id>  --from-export=production.tar.gz --no-dry-run --projectId xyz --dataset staging
`;
function parseCliFlags(args) {
  return yargs__default.default(helpers.hideBin(args.argv || process.argv).slice(2)).options("dry-run", { type: "boolean", default: !0 }).options("concurrency", { type: "number", default: migrate.DEFAULT_MUTATION_CONCURRENCY }).options("progress", { type: "boolean", default: !0 }).options("dataset", { type: "string" }).options("from-export", { type: "string" }).options("project", { type: "string" }).options("confirm", { type: "boolean", default: !0 }).argv;
}
const runMigrationCommand = {
  name: "run",
  group: "migration",
  signature: "ID",
  helpText: helpText$6,
  description: "Run a migration against a dataset",
  // eslint-disable-next-line max-statements
  action: async (args, context) => {
    const { apiClient, output, prompt, chalk, workDir } = context, [id] = args.argsWithoutOptions, migrationsDirectoryPath = path__default.default.join(workDir, MIGRATIONS_DIRECTORY), flags = await parseCliFlags(args), fromExport = flags.fromExport, dry = flags.dryRun, dataset = flags.dataset, project = flags.project;
    if (dataset && !project || project && !dataset)
      throw new Error("If either --dataset or --project is provided, both must be provided");
    if (!id) {
      output.error(chalk.red("Error: Migration ID must be provided"));
      const migrations = await resolveMigrations(workDir), table = new consoleTablePrinter.Table({
        title: "Migrations found in project",
        columns: [
          { name: "id", title: "ID", alignment: "left" },
          { name: "title", title: "Title", alignment: "left" }
        ]
      });
      migrations.forEach((definedMigration) => {
        table.addRow({ id: definedMigration.id, title: definedMigration.migration.title });
      }), table.printTable(), output.print("\nRun `sanity migration run <ID>` to run a migration");
      return;
    }
    node.register({
      target: `node${process.version.slice(1)}`
    });
    const candidates = resolveMigrationScript(workDir, id), resolvedScripts = candidates.filter(isLoadableMigrationScript);
    if (resolvedScripts.length > 1)
      throw new Error(
        `Found multiple migrations for "${id}" in ${chalk.cyan(migrationsDirectoryPath)}: 
 - ${candidates.map((candidate) => path__default.default.relative(migrationsDirectoryPath, candidate.absolutePath)).join(`
 - `)}`
      );
    const script = resolvedScripts[0];
    if (!script)
      throw new Error(
        `No migration found for "${id}" in ${chalk.cyan(chalk.cyan(migrationsDirectoryPath))}. Make sure that the migration file exists and exports a valid migration as its default export.

 Tried the following files:
 - ${candidates.map((candidate) => path__default.default.relative(migrationsDirectoryPath, candidate.absolutePath)).join(`
 - `)}`
      );
    const mod = script.mod;
    if ("up" in mod || "down" in mod)
      throw new Error(
        'Only "up" migrations are supported at this time, please use a default export'
      );
    const migration = mod.default;
    if (fromExport && !dry)
      throw new Error("Can only dry run migrations from a dataset export file");
    const concurrency = flags.concurrency;
    if (concurrency !== void 0) {
      if (concurrency > migrate.MAX_MUTATION_CONCURRENCY)
        throw new Error(
          `Concurrency exceeds the maximum allowed value of ${migrate.MAX_MUTATION_CONCURRENCY}`
        );
      if (concurrency === 0)
        throw new Error(`Concurrency must be a positive number, got ${concurrency}`);
    }
    const projectConfig = apiClient({
      requireUser: !0,
      requireProject: !0
    }).config(), apiConfig = {
      dataset: dataset != null ? dataset : projectConfig.dataset,
      projectId: project != null ? project : projectConfig.projectId,
      apiHost: projectConfig.apiHost,
      token: projectConfig.token,
      apiVersion: "v2024-01-29"
    };
    if (dry) {
      dryRunHandler();
      return;
    }
    if (output.print(
      `
${chalk.yellow(chalk.bold("Note: During migrations, your webhooks stay active."))}`
    ), output.print(
      `To adjust them, launch the management interface with ${chalk.cyan("sanity manage")}, navigate to the API settings, and toggle the webhooks before and after the migration as needed.
`
    ), (flags.confirm && await prompt.single({
      message: `This migration will run on the ${chalk.yellow(
        chalk.bold(apiConfig.dataset)
      )} dataset in ${chalk.yellow(chalk.bold(apiConfig.projectId))} project. Are you sure?`,
      type: "confirm"
    })) === !1) {
      debug$2("User aborted migration");
      return;
    }
    const spinner = output.spinner(`Running migration "${id}"`).start();
    await migrate.run({ api: apiConfig, concurrency, onProgress: createProgress(spinner) }, migration), spinner.stop();
    function createProgress(progressSpinner) {
      return function(progress2) {
        if (!flags.progress) {
          progressSpinner.stop();
          return;
        }
        if (progress2.done) {
          progressSpinner.text = `Migration "${id}" completed.

  Project id:  ${chalk.bold(apiConfig.projectId)}
  Dataset:     ${chalk.bold(apiConfig.dataset)}

  ${progress2.documents} documents processed.
  ${progress2.mutations} mutations generated.
  ${chalk.green(progress2.completedTransactions.length)} transactions committed.`, progressSpinner.stopAndPersist({ symbol: chalk.green("\u2714") });
          return;
        }
        [null, ...progress2.currentTransactions].forEach((transaction) => {
          var _a;
          progressSpinner.text = `Running migration "${id}" ${dry ? "in dry mode..." : "..."}

  Project id:     ${chalk.bold(apiConfig.projectId)}
  Dataset:        ${chalk.bold(apiConfig.dataset)}
  Document type:  ${chalk.bold((_a = migration.documentTypes) == null ? void 0 : _a.join(","))}

  ${progress2.documents} documents processed\u2026
  ${progress2.mutations} mutations generated\u2026
  ${chalk.blue(progress2.pending)} requests pending\u2026
  ${chalk.green(progress2.completedTransactions.length)} transactions committed.

  ${transaction && !progress2.done ? `\xBB ${prettyFormat({ chalk, subject: transaction, migration, indentSize: 2 })}` : ""}`;
        });
      };
    }
    async function dryRunHandler() {
      output.print(`Running migration "${id}" in dry mode`), fromExport && output.print(`Using export ${chalk.cyan(fromExport)}`), output.print(), output.print(`Project id:  ${chalk.bold(apiConfig.projectId)}`), output.print(`Dataset:     ${chalk.bold(apiConfig.dataset)}`);
      for await (const mutation of migrate.dryRun({ api: apiConfig, exportPath: fromExport }, migration))
        mutation && (output.print(), output.print(
          prettyFormat({
            chalk,
            subject: mutation,
            migration
          })
        ));
    }
  }
}, helpText$5 = `
Notes
  Changing the hostname or port number might require a new entry to the CORS-origins allow list.

Options
  --port <port> TCP port to start server on. [default: 3333]
  --host <host> The local network interface at which to listen. [default: "127.0.0.1"]

Examples
  sanity preview --host=0.0.0.0
  sanity preview --port=1942
  sanity preview some/build-output-dir
`, previewCommand = {
  name: "preview",
  signature: "[BUILD_OUTPUT_DIR] [--port <port>] [--host <host>]",
  description: "Starts a server to preview a production build of Sanity Studio",
  action: async (args, context) => (await getPreviewAction$1())(args, context),
  helpText: helpText$5
};
async function getPreviewAction$1() {
  return (await Promise.resolve().then(function() {
    return require("./previewAction.js");
  })).default;
}
const description$1 = "Extracts a JSON representation of a Sanity schema within a Studio context.", helpText$4 = `
**Note**: This command is experimental and subject to change.

Options
  --workspace <name> The name of the workspace to generate a schema for
  --path Optional path to specify destination of the schema file
  --enforce-required-fields Makes the schema generated treat fields marked as required as non-optional. Defaults to false.
  --format=[groq-type-nodes] Format the schema as GROQ type nodes. Only available format at the moment.

Examples
  # Extracts schema types in a Sanity project with more than one workspace
  sanity schema extract --workspace default
`, extractSchemaCommand = {
  name: "extract",
  group: "schema",
  signature: "",
  description: description$1,
  helpText: helpText$4,
  action: async (args, context) => (await Promise.resolve().then(function() {
    return require("./extractAction.js");
  })).default(args, context)
};
var schemaGroup = {
  name: "schema",
  signature: "[COMMAND]",
  isGroupRoot: !0,
  description: "Interacts with Sanity Studio schema configurations"
};
const description = "Validates all schema types specified in a workspace.", helpText$3 = `
Options
  --workspace <name> The name of the workspace to use when validating all schema types.
  --format <pretty|ndjson|json> The output format used to print schema errors and warnings.
  --level <error|warning> The minimum level reported out. Defaults to warning.

Examples
  # Validates all schema types in a Sanity project with more than one workspace
  sanity schema validate --workspace default

  # Save the results of the report into a file
  sanity schema validate > report.txt

  # Report out only errors
  sanity schema validate --level error
`, validateDocumentsCommand = {
  name: "validate",
  group: "schema",
  signature: "",
  description,
  helpText: helpText$3,
  action: async (args, context) => (await Promise.resolve().then(function() {
    return require("./validateAction.js");
  })).default(args, context)
}, isInteractive = process.stdout.isTTY && process.env.TERM !== "dumb" && !("CI" in process.env), helpText$2 = `
Notes
  Changing the hostname or port number might require a new CORS-entry to be added.

Options
  --port <port> TCP port to start server on. [default: 3333]
  --host <host> The local network interface at which to listen. [default: "127.0.0.1"]

Examples
  sanity start --host=0.0.0.0
  sanity start --port=1942
  sanity start some/build-output-dir
`, startCommand = {
  name: "start",
  signature: "[BUILD_OUTPUT_DIR] [--port <port>] [--host <host>]",
  description: "Alias for `sanity preview`",
  action: async (args, context) => {
    const { output, chalk, prompt } = context, previewAction = await getPreviewAction(), warn = (msg) => output.warn(chalk.yellow.bgBlack(msg)), error = (msg) => output.warn(chalk.red.bgBlack(msg));
    warn("\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E"), warn("\u2502                                                           \u2502"), warn("\u2502  You're running Sanity Studio v3. In this version the     \u2502"), warn("\u2502  [start] command is used to preview static builds.        |"), warn("\u2502                                                           \u2502"), warn("\u2502  To run a development server, use the [npm run dev] or    |"), warn("\u2502  [npx sanity dev] command instead. For more information,  \u2502"), warn("\u2502  see https://www.sanity.io/help/studio-v2-vs-v3           \u2502"), warn("\u2502                                                           \u2502"), warn("\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F"), warn("");
    try {
      await previewAction(args, context);
    } catch (err) {
      if (err.name !== "BUILD_NOT_FOUND")
        throw err;
      error(err.message), error(`
`), isInteractive && await prompt.single({
        message: "Do you want to start a development server instead?",
        type: "confirm"
      }) ? await (await getDevAction())(args, context) : process.exit(1);
    }
  },
  helpText: helpText$2
};
async function getPreviewAction() {
  return (await Promise.resolve().then(function() {
    return require("./previewAction.js");
  })).default;
}
const uninstallCommand = {
  name: "uninstall",
  signature: "[plugin]",
  helpText: "",
  description: "Removes a Sanity plugin from the current Sanity configuration",
  hideFromHelp: !0,
  action: async (args, context) => {
    await context.output.error("`sanity uninstall` is no longer supported - use npm/yarn");
  }
};
function prettifyQuotaError(message) {
  return (err) => {
    throw err.statusCode === 402 && (err.message = message), err;
  };
}
const helpText$1 = `
Options
  --role Role to invite the user as

Examples
  # Invite a new user to the project (prompt for details)
  sanity users invite

  # Send a new user invite to the email "pippi@sanity.io", prompt for role
  sanity users invite pippi@sanity.io

  # Send a new user invite to the email "pippi@sanity.io", as administrator
  sanity users invite pippi@sanity.io --role administrator
`, inviteUserCommand = {
  name: "invite",
  group: "users",
  signature: "[EMAIL]",
  helpText: helpText$1,
  description: "Invite a new user to the project",
  action: async (args, context) => {
    const { apiClient, output, prompt } = context, [selectedEmail] = args.argsWithoutOptions, flags = args.extOptions, client = apiClient().clone().config({ useProjectHostname: !1, apiVersion: "2021-06-07" }), { projectId } = client.config(), roles = (await client.request({ uri: `/projects/${projectId}/roles` })).filter(
      (role2) => role2.appliesToUsers
    ), email = selectedEmail || await promptForEmail(prompt), selectedRole = flags.role || await promptForRole(prompt, roles), role = roles.find(({ name }) => name.toLowerCase() === selectedRole.toLowerCase());
    if (!role)
      throw new Error(`Role name "${selectedRole}" not found`);
    await client.clone().request({
      method: "POST",
      uri: `/invitations/project/${projectId}`,
      body: { email, role: role.name },
      useGlobalApi: !0,
      maxRedirects: 0
    }).catch(
      prettifyQuotaError(
        "Project is already at user quota, add billing details to the project in order to allow overage charges."
      )
    ), output.print(`Invitation sent to ${email}`);
  }
};
function promptForEmail(prompt) {
  return prompt.single({
    type: "input",
    message: "Email to invite:",
    filter: (val) => val.trim(),
    validate: (name) => !name || !name.includes("@") ? "Invalid email" : !0
  });
}
function promptForRole(prompt, roles) {
  return prompt.single({
    type: "list",
    message: "Which role should the user have?",
    choices: roles.map((role) => ({
      value: role.name,
      name: `${role.title} (${role.description})`
    }))
  });
}
const sortFields = ["id", "name", "role", "date"], helpText = `
Options
  --no-invitations Don't include pending invitations
  --no-robots Don't include robots (token users)
  --sort <field> Sort users by specified column: ${sortFields.join(", ")}
  --order <asc/desc> Sort output ascending/descending

Examples
  # List all users of the project
  sanity users list

  # List all users of the project, but exclude pending invitations and robots
  sanity users list --no-invitations --no-robots

  # List all users, sorted by role
  sanity users list --sort role
`, listUsersCommand = {
  name: "list",
  group: "users",
  signature: "",
  helpText,
  description: "List all users of the project",
  action: async (args, context) => {
    const { apiClient, output, chalk } = context, { sort, order, robots, invitations } = {
      sort: "date",
      order: "asc",
      robots: !0,
      invitations: !0,
      ...args.extOptions
    };
    if (!sortFields.includes(sort))
      throw new Error(`Can't sort by field "${sort}". Must be one of ${sortFields.join(", ")}`);
    if (order !== "asc" && order !== "desc")
      throw new Error(`Unknown sort order "${order}", must be either "asc" or "desc"`);
    const client = apiClient(), globalClient = client.clone().config({ useProjectHostname: !1 }), { projectId } = client.config(), useGlobalApi = !0, [pendingInvitations, project] = await Promise.all([
      invitations ? globalClient.request({ uri: `/invitations/project/${projectId}`, useGlobalApi }).then(getPendingInvitations) : [],
      globalClient.request({ uri: `/projects/${projectId}`, useGlobalApi })
    ]), memberIds = project.members.map((member) => member.id), users = await globalClient.request({ uri: `/users/${memberIds.join(",")}`, useGlobalApi }).then((user) => Array.isArray(user) ? user : [user]), members = [...project.members.map((member) => ({
      ...member,
      ...getUserProps(users.find((candidate) => candidate.id === member.id))
    })).filter((member) => !member.isRobot || robots), ...pendingInvitations], ordered = sortBy__default.default(
      members.map(({ id, name, role, date }) => [id, name, role, date]),
      [sortFields.indexOf(sort)]
    ), rows = order === "asc" ? ordered : ordered.reverse(), maxWidths = rows.reduce(
      (max, row) => row.map((current, index) => Math.max(size__default.default(current), max[index])),
      sortFields.map((str) => size__default.default(str))
    ), printRow = (row) => {
      const isInvite = row[0] === "<pending>", textRow = row.map((col, i) => `${col}`.padEnd(maxWidths[i])).join("   ");
      return isInvite ? chalk.dim(textRow) : textRow;
    };
    output.print(chalk.cyan(printRow(sortFields))), rows.forEach((row) => output.print(printRow(row)));
  }
};
function getUserProps(user) {
  const { displayName: name, createdAt: date } = user || {};
  return { name: name || "", date: date || "" };
}
function getPendingInvitations(invitations) {
  return invitations.filter((invite) => !invite.isAccepted && !invite.isRevoked && !invite.acceptedByUserId).map((invite) => ({
    id: "<pending>",
    name: invite.email,
    role: invite.role,
    date: invite.createdAt
  }));
}
const usersGroup = {
  name: "users",
  signature: "[COMMAND]",
  isGroupRoot: !0,
  description: "Manages users of your Sanity project"
}, commands = [
  buildCommand,
  checkCommand,
  configCheckCommand,
  datasetGroup,
  deployCommand,
  undeployCommand,
  listDatasetsCommand,
  createDatasetCommand,
  datasetVisibilityCommand,
  exportDatasetCommand,
  importDatasetCommand,
  deleteDatasetCommand,
  copyDatasetCommand,
  aliasCommand,
  datasetBackupGroup,
  listDatasetBackupCommand,
  downloadBackupCommand,
  disableDatasetBackupCommand,
  enableDatasetBackupCommand,
  corsGroup,
  listCorsOriginsCommand,
  addCorsOriginCommand,
  deleteCorsOriginCommand,
  usersGroup,
  inviteUserCommand,
  listUsersCommand,
  hookGroup,
  listHooksCommand,
  createHookCommand,
  migrationGroup,
  createMigrationCommand,
  runMigrationCommand,
  listMigrationCommand,
  deleteHookCommand,
  listHookLogsCommand,
  printHookAttemptCommand,
  documentsGroup,
  getDocumentsCommand,
  queryDocumentsCommand,
  deleteDocumentsCommand,
  createDocumentsCommand,
  validateDocumentsCommand$1,
  graphqlGroup,
  listGraphQLAPIsCommand,
  deployGraphQLAPICommand,
  deleteGraphQLAPICommand,
  devCommand,
  startCommand,
  schemaGroup,
  validateDocumentsCommand,
  extractSchemaCommand,
  previewCommand,
  uninstallCommand,
  execCommand
], cliProjectCommands = {
  requiredCliVersionRange: "^3.0.0",
  commands
};
exports.cliProjectCommands = cliProjectCommands;
exports.convertToTree = convertToTree;
exports.debug = debug$2;
exports.formatTree = formatTree;
exports.getClientUrl = getClientUrl;
exports.maxKeyLength = maxKeyLength;
//# sourceMappingURL=_internal.js.map
